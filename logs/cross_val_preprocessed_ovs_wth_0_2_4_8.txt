INFO:root:the deviced being used is cuda:0
INFO:root:Start validation
INFO:root:statistics used: {'mean': tensor([0.0132, 0.0048, 0.0149, 0.0123, 0.0097, 0.0121, 0.0050, 0.0077]), 'std': tensor([0.0271, 0.0020, 0.0204, 0.0125, 0.0076, 0.0052, 0.0021, 0.0031])}
INFO:root:train dataset: 131886, test dataset: 6256
INFO:root:used only channels: [1, 3, 5, 6, 7, 9, 10, 11]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.03400157
INFO:root:[1,   100] training loss: 0.03376625
INFO:root:[1,   150] training loss: 0.05848642
INFO:root:[1,   200] training loss: 0.05680014
INFO:root:[1,   250] training loss: 0.06345035
INFO:root:[1,   300] training loss: 0.05903744
INFO:root:[1,   350] training loss: 0.05540152
INFO:root:[1,   400] training loss: 0.06347328
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.02041644
INFO:root:[2,   100] training loss: 0.02218201
INFO:root:[2,   150] training loss: 0.04747955
INFO:root:[2,   200] training loss: 0.04822907
INFO:root:[2,   250] training loss: 0.05410854
INFO:root:[2,   300] training loss: 0.05675129
INFO:root:[2,   350] training loss: 0.05456565
INFO:root:[2,   400] training loss: 0.05809132
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.00836481
INFO:root:[3,   100] training loss: 0.01955086
INFO:root:[3,   150] training loss: 0.04165183
INFO:root:[3,   200] training loss: 0.04296927
INFO:root:[3,   250] training loss: 0.04456516
INFO:root:[3,   300] training loss: 0.05338138
INFO:root:[3,   350] training loss: 0.05171903
INFO:root:[3,   400] training loss: 0.05398997
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.00586458
INFO:root:[4,   100] training loss: 0.01842805
INFO:root:[4,   150] training loss: 0.03710589
INFO:root:[4,   200] training loss: 0.03668523
INFO:root:[4,   250] training loss: 0.03620638
INFO:root:[4,   300] training loss: 0.04986024
INFO:root:[4,   350] training loss: 0.04909400
INFO:root:[4,   400] training loss: 0.04465246
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.00479337
INFO:root:[5,   100] training loss: 0.01724304
INFO:root:[5,   150] training loss: 0.03146721
INFO:root:[5,   200] training loss: 0.03022381
INFO:root:[5,   250] training loss: 0.02872390
INFO:root:[5,   300] training loss: 0.04622738
INFO:root:[5,   350] training loss: 0.04349096
INFO:root:[5,   400] training loss: 0.03335043
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.00383938
INFO:root:[6,   100] training loss: 0.01457176
INFO:root:[6,   150] training loss: 0.02469594
INFO:root:[6,   200] training loss: 0.02548999
INFO:root:[6,   250] training loss: 0.02157666
INFO:root:[6,   300] training loss: 0.04208796
INFO:root:[6,   350] training loss: 0.03526071
INFO:root:[6,   400] training loss: 0.02642851
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.00348928
INFO:root:[7,   100] training loss: 0.01222545
INFO:root:[7,   150] training loss: 0.02043016
INFO:root:[7,   200] training loss: 0.01981200
INFO:root:[7,   250] training loss: 0.01630619
INFO:root:[7,   300] training loss: 0.03659437
INFO:root:[7,   350] training loss: 0.02786327
INFO:root:[7,   400] training loss: 0.02005359
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.00326859
INFO:root:[8,   100] training loss: 0.01337989
INFO:root:[8,   150] training loss: 0.03557020
INFO:root:[8,   200] training loss: 0.03583262
INFO:root:[8,   250] training loss: 0.01949618
INFO:root:[8,   300] training loss: 0.04138584
INFO:root:[8,   350] training loss: 0.02161539
INFO:root:[8,   400] training loss: 0.01726449
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.00313239
INFO:root:[9,   100] training loss: 0.00878830
INFO:root:[9,   150] training loss: 0.02091275
INFO:root:[9,   200] training loss: 0.02172503
INFO:root:[9,   250] training loss: 0.01513469
INFO:root:[9,   300] training loss: 0.03682247
INFO:root:[9,   350] training loss: 0.02135031
INFO:root:[9,   400] training loss: 0.01780583
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.00308054
INFO:root:[10,   100] training loss: 0.00806396
INFO:root:[10,   150] training loss: 0.01837460
INFO:root:[10,   200] training loss: 0.01802868
INFO:root:[10,   250] training loss: 0.01369392
INFO:root:[10,   300] training loss: 0.03326506
INFO:root:[10,   350] training loss: 0.02063542
INFO:root:[10,   400] training loss: 0.01652194
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.00295460
INFO:root:[11,   100] training loss: 0.00743104
INFO:root:[11,   150] training loss: 0.01695421
INFO:root:[11,   200] training loss: 0.01613808
INFO:root:[11,   250] training loss: 0.01312677
INFO:root:[11,   300] training loss: 0.03096146
INFO:root:[11,   350] training loss: 0.02005694
INFO:root:[11,   400] training loss: 0.01535601
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.00294466
INFO:root:[12,   100] training loss: 0.00730974
INFO:root:[12,   150] training loss: 0.01578606
INFO:root:[12,   200] training loss: 0.01510311
INFO:root:[12,   250] training loss: 0.01291139
INFO:root:[12,   300] training loss: 0.02944722
INFO:root:[12,   350] training loss: 0.02004336
INFO:root:[12,   400] training loss: 0.01501772
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.00286119
INFO:root:[13,   100] training loss: 0.00693787
INFO:root:[13,   150] training loss: 0.01478452
INFO:root:[13,   200] training loss: 0.01399095
INFO:root:[13,   250] training loss: 0.01190899
INFO:root:[13,   300] training loss: 0.02800230
INFO:root:[13,   350] training loss: 0.02007640
INFO:root:[13,   400] training loss: 0.01370856
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.00285222
INFO:root:[14,   100] training loss: 0.00667114
INFO:root:[14,   150] training loss: 0.01428367
INFO:root:[14,   200] training loss: 0.01361101
INFO:root:[14,   250] training loss: 0.01146613
INFO:root:[14,   300] training loss: 0.02645242
INFO:root:[14,   350] training loss: 0.01908065
INFO:root:[14,   400] training loss: 0.01341426
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.00282362
INFO:root:[15,   100] training loss: 0.00655068
INFO:root:[15,   150] training loss: 0.01397260
INFO:root:[15,   200] training loss: 0.01407318
INFO:root:[15,   250] training loss: 0.01158306
INFO:root:[15,   300] training loss: 0.02618288
INFO:root:[15,   350] training loss: 0.01809209
INFO:root:[15,   400] training loss: 0.01234538
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.00274930
INFO:root:[16,   100] training loss: 0.00648893
INFO:root:[16,   150] training loss: 0.01360400
INFO:root:[16,   200] training loss: 0.01372225
INFO:root:[16,   250] training loss: 0.01151770
INFO:root:[16,   300] training loss: 0.02588924
INFO:root:[16,   350] training loss: 0.01808079
INFO:root:[16,   400] training loss: 0.01213889
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.00280688
INFO:root:[17,   100] training loss: 0.00636708
INFO:root:[17,   150] training loss: 0.01355062
INFO:root:[17,   200] training loss: 0.01335245
INFO:root:[17,   250] training loss: 0.01189625
INFO:root:[17,   300] training loss: 0.02600229
INFO:root:[17,   350] training loss: 0.01803926
INFO:root:[17,   400] training loss: 0.01235535
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.00281472
INFO:root:[18,   100] training loss: 0.00632987
INFO:root:[18,   150] training loss: 0.01339099
INFO:root:[18,   200] training loss: 0.01322794
INFO:root:[18,   250] training loss: 0.01164122
INFO:root:[18,   300] training loss: 0.02530460
INFO:root:[18,   350] training loss: 0.01801640
INFO:root:[18,   400] training loss: 0.01142169
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.00282762
INFO:root:[19,   100] training loss: 0.00634790
INFO:root:[19,   150] training loss: 0.01341194
INFO:root:[19,   200] training loss: 0.01305696
INFO:root:[19,   250] training loss: 0.01131847
INFO:root:[19,   300] training loss: 0.02489689
INFO:root:[19,   350] training loss: 0.01810190
INFO:root:[19,   400] training loss: 0.01160389
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.00277696
INFO:root:[20,   100] training loss: 0.00635012
INFO:root:[20,   150] training loss: 0.01348031
INFO:root:[20,   200] training loss: 0.01298280
INFO:root:[20,   250] training loss: 0.01101050
INFO:root:[20,   300] training loss: 0.02470616
INFO:root:[20,   350] training loss: 0.01806879
INFO:root:[20,   400] training loss: 0.01236202
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.00276397
INFO:root:[21,   100] training loss: 0.00623050
INFO:root:[21,   150] training loss: 0.01331255
INFO:root:[21,   200] training loss: 0.01320897
INFO:root:[21,   250] training loss: 0.01110223
INFO:root:[21,   300] training loss: 0.02450909
INFO:root:[21,   350] training loss: 0.01811771
INFO:root:[21,   400] training loss: 0.01172331
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.00278223
INFO:root:[22,   100] training loss: 0.00623059
INFO:root:[22,   150] training loss: 0.01340841
INFO:root:[22,   200] training loss: 0.01302403
INFO:root:[22,   250] training loss: 0.01110844
INFO:root:[22,   300] training loss: 0.02429967
INFO:root:[22,   350] training loss: 0.01791130
INFO:root:[22,   400] training loss: 0.01195837
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.00277274
INFO:root:[23,   100] training loss: 0.00620797
INFO:root:[23,   150] training loss: 0.01358892
INFO:root:[23,   200] training loss: 0.01257799
INFO:root:[23,   250] training loss: 0.01084100
INFO:root:[23,   300] training loss: 0.02452421
INFO:root:[23,   350] training loss: 0.01818249
INFO:root:[23,   400] training loss: 0.01203906
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.00275974
INFO:root:[24,   100] training loss: 0.00629725
INFO:root:[24,   150] training loss: 0.01315914
INFO:root:[24,   200] training loss: 0.01315302
INFO:root:[24,   250] training loss: 0.01080221
INFO:root:[24,   300] training loss: 0.02400154
INFO:root:[24,   350] training loss: 0.01814869
INFO:root:[24,   400] training loss: 0.01214013
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.00277389
INFO:root:[25,   100] training loss: 0.00627643
INFO:root:[25,   150] training loss: 0.01327708
INFO:root:[25,   200] training loss: 0.01300345
INFO:root:[25,   250] training loss: 0.01122412
INFO:root:[25,   300] training loss: 0.02453748
INFO:root:[25,   350] training loss: 0.01776222
INFO:root:[25,   400] training loss: 0.01190980
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.00277163
INFO:root:[26,   100] training loss: 0.00616956
INFO:root:[26,   150] training loss: 0.01316039
INFO:root:[26,   200] training loss: 0.01312648
INFO:root:[26,   250] training loss: 0.01112938
INFO:root:[26,   300] training loss: 0.02454437
INFO:root:[26,   350] training loss: 0.01835027
INFO:root:[26,   400] training loss: 0.01200865
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.00276620
INFO:root:[27,   100] training loss: 0.00618740
INFO:root:[27,   150] training loss: 0.01305079
INFO:root:[27,   200] training loss: 0.01299028
INFO:root:[27,   250] training loss: 0.01174883
INFO:root:[27,   300] training loss: 0.02419244
INFO:root:[27,   350] training loss: 0.01820448
INFO:root:[27,   400] training loss: 0.01170782
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.00275194
INFO:root:[28,   100] training loss: 0.00613607
INFO:root:[28,   150] training loss: 0.01324256
INFO:root:[28,   200] training loss: 0.01282729
INFO:root:[28,   250] training loss: 0.01138705
INFO:root:[28,   300] training loss: 0.02453318
INFO:root:[28,   350] training loss: 0.01785789
INFO:root:[28,   400] training loss: 0.01172712
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.00277852
INFO:root:[29,   100] training loss: 0.00618521
INFO:root:[29,   150] training loss: 0.01311553
INFO:root:[29,   200] training loss: 0.01253354
INFO:root:[29,   250] training loss: 0.01097428
INFO:root:[29,   300] training loss: 0.02414474
INFO:root:[29,   350] training loss: 0.01761471
INFO:root:[29,   400] training loss: 0.01215517
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.00276389
INFO:root:[30,   100] training loss: 0.00627068
INFO:root:[30,   150] training loss: 0.01319038
INFO:root:[30,   200] training loss: 0.01300408
INFO:root:[30,   250] training loss: 0.01111761
INFO:root:[30,   300] training loss: 0.02421842
INFO:root:[30,   350] training loss: 0.01816116
INFO:root:[30,   400] training loss: 0.01254657
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.00274960
INFO:root:[31,   100] training loss: 0.00631228
INFO:root:[31,   150] training loss: 0.01294766
INFO:root:[31,   200] training loss: 0.01319923
INFO:root:[31,   250] training loss: 0.01148901
INFO:root:[31,   300] training loss: 0.02463827
INFO:root:[31,   350] training loss: 0.01811983
INFO:root:[31,   400] training loss: 0.01203255
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.00275979
INFO:root:[32,   100] training loss: 0.00626898
INFO:root:[32,   150] training loss: 0.01323853
INFO:root:[32,   200] training loss: 0.01334527
INFO:root:[32,   250] training loss: 0.01123794
INFO:root:[32,   300] training loss: 0.02442157
INFO:root:[32,   350] training loss: 0.01794987
INFO:root:[32,   400] training loss: 0.01207692
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.00277657
INFO:root:[33,   100] training loss: 0.00624275
INFO:root:[33,   150] training loss: 0.01322848
INFO:root:[33,   200] training loss: 0.01298663
INFO:root:[33,   250] training loss: 0.01120028
INFO:root:[33,   300] training loss: 0.02447829
INFO:root:[33,   350] training loss: 0.01795123
INFO:root:[33,   400] training loss: 0.01217584
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.00277272
INFO:root:[34,   100] training loss: 0.00620660
INFO:root:[34,   150] training loss: 0.01327518
INFO:root:[34,   200] training loss: 0.01293210
INFO:root:[34,   250] training loss: 0.01100143
INFO:root:[34,   300] training loss: 0.02425267
INFO:root:[34,   350] training loss: 0.01804617
INFO:root:[34,   400] training loss: 0.01180067
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.00273182
INFO:root:[35,   100] training loss: 0.00622571
INFO:root:[35,   150] training loss: 0.01331046
INFO:root:[35,   200] training loss: 0.01282271
INFO:root:[35,   250] training loss: 0.01091408
INFO:root:[35,   300] training loss: 0.02473832
INFO:root:[35,   350] training loss: 0.01802158
INFO:root:[35,   400] training loss: 0.01229837
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.00278777
INFO:root:[36,   100] training loss: 0.00614100
INFO:root:[36,   150] training loss: 0.01321413
INFO:root:[36,   200] training loss: 0.01307820
INFO:root:[36,   250] training loss: 0.01163624
INFO:root:[36,   300] training loss: 0.02456100
INFO:root:[36,   350] training loss: 0.01784753
INFO:root:[36,   400] training loss: 0.01164688
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.00277695
INFO:root:[37,   100] training loss: 0.00627018
INFO:root:[37,   150] training loss: 0.01320288
INFO:root:[37,   200] training loss: 0.01273839
INFO:root:[37,   250] training loss: 0.01118316
INFO:root:[37,   300] training loss: 0.02435906
INFO:root:[37,   350] training loss: 0.01779274
INFO:root:[37,   400] training loss: 0.01184425
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.00278394
INFO:root:[38,   100] training loss: 0.00618128
INFO:root:[38,   150] training loss: 0.01324390
INFO:root:[38,   200] training loss: 0.01284620
INFO:root:[38,   250] training loss: 0.01136932
INFO:root:[38,   300] training loss: 0.02439917
INFO:root:[38,   350] training loss: 0.01771100
INFO:root:[38,   400] training loss: 0.01154170
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.00275524
INFO:root:[39,   100] training loss: 0.00624063
INFO:root:[39,   150] training loss: 0.01332129
INFO:root:[39,   200] training loss: 0.01328662
INFO:root:[39,   250] training loss: 0.01110008
INFO:root:[39,   300] training loss: 0.02464215
INFO:root:[39,   350] training loss: 0.01802330
INFO:root:[39,   400] training loss: 0.01214716
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.00277610
INFO:root:[40,   100] training loss: 0.00636281
INFO:root:[40,   150] training loss: 0.01321659
INFO:root:[40,   200] training loss: 0.01270307
INFO:root:[40,   250] training loss: 0.01141338
INFO:root:[40,   300] training loss: 0.02438868
INFO:root:[40,   350] training loss: 0.01816593
INFO:root:[40,   400] training loss: 0.01093660
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.00278762
INFO:root:[41,   100] training loss: 0.00627517
INFO:root:[41,   150] training loss: 0.01349333
INFO:root:[41,   200] training loss: 0.01270953
INFO:root:[41,   250] training loss: 0.01108484
INFO:root:[41,   300] training loss: 0.02411682
INFO:root:[41,   350] training loss: 0.01791285
INFO:root:[41,   400] training loss: 0.01168850
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.00277954
INFO:root:[42,   100] training loss: 0.00619226
INFO:root:[42,   150] training loss: 0.01329588
INFO:root:[42,   200] training loss: 0.01287092
INFO:root:[42,   250] training loss: 0.01100601
INFO:root:[42,   300] training loss: 0.02418132
INFO:root:[42,   350] training loss: 0.01812155
INFO:root:[42,   400] training loss: 0.01208600
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.00276566
INFO:root:[43,   100] training loss: 0.00621710
INFO:root:[43,   150] training loss: 0.01301749
INFO:root:[43,   200] training loss: 0.01285830
INFO:root:[43,   250] training loss: 0.01135690
INFO:root:[43,   300] training loss: 0.02461078
INFO:root:[43,   350] training loss: 0.01827260
INFO:root:[43,   400] training loss: 0.01149640
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.00276557
INFO:root:[44,   100] training loss: 0.00623383
INFO:root:[44,   150] training loss: 0.01330711
INFO:root:[44,   200] training loss: 0.01297508
INFO:root:[44,   250] training loss: 0.01129991
INFO:root:[44,   300] training loss: 0.02447516
INFO:root:[44,   350] training loss: 0.01806293
INFO:root:[44,   400] training loss: 0.01181685
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.00276447
INFO:root:[45,   100] training loss: 0.00613664
INFO:root:[45,   150] training loss: 0.01318959
INFO:root:[45,   200] training loss: 0.01283258
INFO:root:[45,   250] training loss: 0.01144824
INFO:root:[45,   300] training loss: 0.02483144
INFO:root:[45,   350] training loss: 0.01798660
INFO:root:[45,   400] training loss: 0.01224372
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.00276830
INFO:root:[46,   100] training loss: 0.00614484
INFO:root:[46,   150] training loss: 0.01294099
INFO:root:[46,   200] training loss: 0.01328134
INFO:root:[46,   250] training loss: 0.01086859
INFO:root:[46,   300] training loss: 0.02483786
INFO:root:[46,   350] training loss: 0.01801797
INFO:root:[46,   400] training loss: 0.01175460
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.00279230
INFO:root:[47,   100] training loss: 0.00616709
INFO:root:[47,   150] training loss: 0.01308066
INFO:root:[47,   200] training loss: 0.01353844
INFO:root:[47,   250] training loss: 0.01112863
INFO:root:[47,   300] training loss: 0.02436107
INFO:root:[47,   350] training loss: 0.01776616
INFO:root:[47,   400] training loss: 0.01152681
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.00278759
INFO:root:[48,   100] training loss: 0.00631192
INFO:root:[48,   150] training loss: 0.01327009
INFO:root:[48,   200] training loss: 0.01271194
INFO:root:[48,   250] training loss: 0.01144029
INFO:root:[48,   300] training loss: 0.02451074
INFO:root:[48,   350] training loss: 0.01794546
INFO:root:[48,   400] training loss: 0.01214691
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.00277031
INFO:root:[49,   100] training loss: 0.00615277
INFO:root:[49,   150] training loss: 0.01308338
INFO:root:[49,   200] training loss: 0.01306558
INFO:root:[49,   250] training loss: 0.01091107
INFO:root:[49,   300] training loss: 0.02426363
INFO:root:[49,   350] training loss: 0.01782934
INFO:root:[49,   400] training loss: 0.01162422
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.00276174
INFO:root:[50,   100] training loss: 0.00612964
INFO:root:[50,   150] training loss: 0.01326374
INFO:root:[50,   200] training loss: 0.01290883
INFO:root:[50,   250] training loss: 0.01088476
INFO:root:[50,   300] training loss: 0.02460092
INFO:root:[50,   350] training loss: 0.01818068
INFO:root:[50,   400] training loss: 0.01178578
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 96 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.7739    0.5789    0.6624       266
           CD4+ T     0.9787    0.9954    0.9870       876
           CD8+ T     0.8779    0.9602    0.9172       352
 CD15+ neutrophil     0.9967    0.9989    0.9978      3671
   CD14+ monocyte     0.9692    1.0000    0.9844       252
          CD19+ B     0.8009    0.9389    0.8645       180
         CD56+ NK     0.9918    0.9167    0.9528       132
              NKT     0.7795    0.6909    0.7325       220
       eosinophil     0.9745    0.9967    0.9855       307

         accuracy                         0.9640      6256
        macro avg     0.9048    0.8974    0.8982      6256
     weighted avg     0.9625    0.9640    0.9622      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK      NKT   eosinophil
0  0.662366  0.986984  0.917232           0.997823         0.984375   0.86445   0.952756  0.73253     0.985507
INFO:root:statistics used: {'mean': tensor([0.0132, 0.0048, 0.0149, 0.0123, 0.0097, 0.0121, 0.0050, 0.0077]), 'std': tensor([0.0272, 0.0020, 0.0203, 0.0126, 0.0075, 0.0052, 0.0021, 0.0031])}
INFO:root:train dataset: 131886, test dataset: 6256
INFO:root:used only channels: [1, 3, 5, 6, 7, 9, 10, 11]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.03827520
INFO:root:[1,   100] training loss: 0.02925521
INFO:root:[1,   150] training loss: 0.05692897
INFO:root:[1,   200] training loss: 0.06036258
INFO:root:[1,   250] training loss: 0.04739315
INFO:root:[1,   300] training loss: 0.05541390
INFO:root:[1,   350] training loss: 0.05750543
INFO:root:[1,   400] training loss: 0.06614187
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.02098108
INFO:root:[2,   100] training loss: 0.02251553
INFO:root:[2,   150] training loss: 0.04490204
INFO:root:[2,   200] training loss: 0.05208320
INFO:root:[2,   250] training loss: 0.04432348
INFO:root:[2,   300] training loss: 0.04734858
INFO:root:[2,   350] training loss: 0.04780139
INFO:root:[2,   400] training loss: 0.05040550
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.01167629
INFO:root:[3,   100] training loss: 0.01949310
INFO:root:[3,   150] training loss: 0.03786739
INFO:root:[3,   200] training loss: 0.04464351
INFO:root:[3,   250] training loss: 0.03690565
INFO:root:[3,   300] training loss: 0.04453905
INFO:root:[3,   350] training loss: 0.04108392
INFO:root:[3,   400] training loss: 0.03948638
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.00663275
INFO:root:[4,   100] training loss: 0.01679089
INFO:root:[4,   150] training loss: 0.03037696
INFO:root:[4,   200] training loss: 0.03541606
INFO:root:[4,   250] training loss: 0.02383462
INFO:root:[4,   300] training loss: 0.03537463
INFO:root:[4,   350] training loss: 0.03284576
INFO:root:[4,   400] training loss: 0.03082342
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.00489464
INFO:root:[5,   100] training loss: 0.01356198
INFO:root:[5,   150] training loss: 0.02276826
INFO:root:[5,   200] training loss: 0.02793829
INFO:root:[5,   250] training loss: 0.01513506
INFO:root:[5,   300] training loss: 0.02981416
INFO:root:[5,   350] training loss: 0.02614541
INFO:root:[5,   400] training loss: 0.02544492
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.00423833
INFO:root:[6,   100] training loss: 0.01114110
INFO:root:[6,   150] training loss: 0.01777124
INFO:root:[6,   200] training loss: 0.02435826
INFO:root:[6,   250] training loss: 0.01074580
INFO:root:[6,   300] training loss: 0.02667268
INFO:root:[6,   350] training loss: 0.02102519
INFO:root:[6,   400] training loss: 0.02124628
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.00379046
INFO:root:[7,   100] training loss: 0.01013611
INFO:root:[7,   150] training loss: 0.01461490
INFO:root:[7,   200] training loss: 0.02017079
INFO:root:[7,   250] training loss: 0.00843294
INFO:root:[7,   300] training loss: 0.02290194
INFO:root:[7,   350] training loss: 0.01744670
INFO:root:[7,   400] training loss: 0.01907581
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.00366672
INFO:root:[8,   100] training loss: 0.01499229
INFO:root:[8,   150] training loss: 0.02530299
INFO:root:[8,   200] training loss: 0.04081382
INFO:root:[8,   250] training loss: 0.01295543
INFO:root:[8,   300] training loss: 0.03687265
INFO:root:[8,   350] training loss: 0.01491606
INFO:root:[8,   400] training loss: 0.01492763
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.00349766
INFO:root:[9,   100] training loss: 0.00807864
INFO:root:[9,   150] training loss: 0.01666750
INFO:root:[9,   200] training loss: 0.02786390
INFO:root:[9,   250] training loss: 0.00984165
INFO:root:[9,   300] training loss: 0.02477928
INFO:root:[9,   350] training loss: 0.01320862
INFO:root:[9,   400] training loss: 0.01348316
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.00346060
INFO:root:[10,   100] training loss: 0.00721951
INFO:root:[10,   150] training loss: 0.01342175
INFO:root:[10,   200] training loss: 0.01909084
INFO:root:[10,   250] training loss: 0.00829091
INFO:root:[10,   300] training loss: 0.01685415
INFO:root:[10,   350] training loss: 0.01235519
INFO:root:[10,   400] training loss: 0.01331832
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.00339297
INFO:root:[11,   100] training loss: 0.00637956
INFO:root:[11,   150] training loss: 0.01198721
INFO:root:[11,   200] training loss: 0.01621794
INFO:root:[11,   250] training loss: 0.00767331
INFO:root:[11,   300] training loss: 0.01400841
INFO:root:[11,   350] training loss: 0.01162837
INFO:root:[11,   400] training loss: 0.01282170
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.00332783
INFO:root:[12,   100] training loss: 0.00579810
INFO:root:[12,   150] training loss: 0.01068858
INFO:root:[12,   200] training loss: 0.01447601
INFO:root:[12,   250] training loss: 0.00697937
INFO:root:[12,   300] training loss: 0.01289283
INFO:root:[12,   350] training loss: 0.01131497
INFO:root:[12,   400] training loss: 0.01231896
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.00330123
INFO:root:[13,   100] training loss: 0.00542553
INFO:root:[13,   150] training loss: 0.01003834
INFO:root:[13,   200] training loss: 0.01330468
INFO:root:[13,   250] training loss: 0.00677025
INFO:root:[13,   300] training loss: 0.01182275
INFO:root:[13,   350] training loss: 0.01088860
INFO:root:[13,   400] training loss: 0.01139251
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.00329656
INFO:root:[14,   100] training loss: 0.00517376
INFO:root:[14,   150] training loss: 0.00950495
INFO:root:[14,   200] training loss: 0.01200388
INFO:root:[14,   250] training loss: 0.00680168
INFO:root:[14,   300] training loss: 0.01116642
INFO:root:[14,   350] training loss: 0.01061378
INFO:root:[14,   400] training loss: 0.01032809
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.00324324
INFO:root:[15,   100] training loss: 0.00508946
INFO:root:[15,   150] training loss: 0.00948675
INFO:root:[15,   200] training loss: 0.01302005
INFO:root:[15,   250] training loss: 0.00665093
INFO:root:[15,   300] training loss: 0.01101753
INFO:root:[15,   350] training loss: 0.00986751
INFO:root:[15,   400] training loss: 0.00898133
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.00324992
INFO:root:[16,   100] training loss: 0.00498355
INFO:root:[16,   150] training loss: 0.00921848
INFO:root:[16,   200] training loss: 0.01182453
INFO:root:[16,   250] training loss: 0.00660891
INFO:root:[16,   300] training loss: 0.01074869
INFO:root:[16,   350] training loss: 0.00983976
INFO:root:[16,   400] training loss: 0.00916292
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.00323388
INFO:root:[17,   100] training loss: 0.00500014
INFO:root:[17,   150] training loss: 0.00924169
INFO:root:[17,   200] training loss: 0.01168099
INFO:root:[17,   250] training loss: 0.00646905
INFO:root:[17,   300] training loss: 0.01080195
INFO:root:[17,   350] training loss: 0.00972000
INFO:root:[17,   400] training loss: 0.00940927
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.00320676
INFO:root:[18,   100] training loss: 0.00493296
INFO:root:[18,   150] training loss: 0.00908653
INFO:root:[18,   200] training loss: 0.01141935
INFO:root:[18,   250] training loss: 0.00633654
INFO:root:[18,   300] training loss: 0.01048634
INFO:root:[18,   350] training loss: 0.00979069
INFO:root:[18,   400] training loss: 0.00939244
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.00320923
INFO:root:[19,   100] training loss: 0.00488512
INFO:root:[19,   150] training loss: 0.00901396
INFO:root:[19,   200] training loss: 0.01193157
INFO:root:[19,   250] training loss: 0.00653516
INFO:root:[19,   300] training loss: 0.01056730
INFO:root:[19,   350] training loss: 0.00976586
INFO:root:[19,   400] training loss: 0.00916642
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.00320676
INFO:root:[20,   100] training loss: 0.00482277
INFO:root:[20,   150] training loss: 0.00869383
INFO:root:[20,   200] training loss: 0.01099486
INFO:root:[20,   250] training loss: 0.00608107
INFO:root:[20,   300] training loss: 0.01051417
INFO:root:[20,   350] training loss: 0.00960435
INFO:root:[20,   400] training loss: 0.00935268
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.00319466
INFO:root:[21,   100] training loss: 0.00487103
INFO:root:[21,   150] training loss: 0.00886256
INFO:root:[21,   200] training loss: 0.01052048
INFO:root:[21,   250] training loss: 0.00608416
INFO:root:[21,   300] training loss: 0.01050514
INFO:root:[21,   350] training loss: 0.00966591
INFO:root:[21,   400] training loss: 0.00962354
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.00318523
INFO:root:[22,   100] training loss: 0.00478496
INFO:root:[22,   150] training loss: 0.00891679
INFO:root:[22,   200] training loss: 0.01077395
INFO:root:[22,   250] training loss: 0.00637917
INFO:root:[22,   300] training loss: 0.01031709
INFO:root:[22,   350] training loss: 0.00963214
INFO:root:[22,   400] training loss: 0.00930801
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.00320780
INFO:root:[23,   100] training loss: 0.00484047
INFO:root:[23,   150] training loss: 0.00884878
INFO:root:[23,   200] training loss: 0.01162523
INFO:root:[23,   250] training loss: 0.00649369
INFO:root:[23,   300] training loss: 0.01036229
INFO:root:[23,   350] training loss: 0.00968861
INFO:root:[23,   400] training loss: 0.00909054
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.00324403
INFO:root:[24,   100] training loss: 0.00482480
INFO:root:[24,   150] training loss: 0.00891970
INFO:root:[24,   200] training loss: 0.01091666
INFO:root:[24,   250] training loss: 0.00607496
INFO:root:[24,   300] training loss: 0.01035239
INFO:root:[24,   350] training loss: 0.00957199
INFO:root:[24,   400] training loss: 0.00913172
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.00319579
INFO:root:[25,   100] training loss: 0.00479997
INFO:root:[25,   150] training loss: 0.00897671
INFO:root:[25,   200] training loss: 0.01088839
INFO:root:[25,   250] training loss: 0.00619450
INFO:root:[25,   300] training loss: 0.01046180
INFO:root:[25,   350] training loss: 0.00961637
INFO:root:[25,   400] training loss: 0.00931353
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.00319322
INFO:root:[26,   100] training loss: 0.00483181
INFO:root:[26,   150] training loss: 0.00894275
INFO:root:[26,   200] training loss: 0.01142764
INFO:root:[26,   250] training loss: 0.00627879
INFO:root:[26,   300] training loss: 0.01026188
INFO:root:[26,   350] training loss: 0.00967604
INFO:root:[26,   400] training loss: 0.00894876
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.00315940
INFO:root:[27,   100] training loss: 0.00483153
INFO:root:[27,   150] training loss: 0.00880323
INFO:root:[27,   200] training loss: 0.01119852
INFO:root:[27,   250] training loss: 0.00630641
INFO:root:[27,   300] training loss: 0.01036266
INFO:root:[27,   350] training loss: 0.00959239
INFO:root:[27,   400] training loss: 0.00911085
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.00316503
INFO:root:[28,   100] training loss: 0.00482472
INFO:root:[28,   150] training loss: 0.00891064
INFO:root:[28,   200] training loss: 0.01066542
INFO:root:[28,   250] training loss: 0.00612870
INFO:root:[28,   300] training loss: 0.01033976
INFO:root:[28,   350] training loss: 0.00964629
INFO:root:[28,   400] training loss: 0.00925944
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.00321165
INFO:root:[29,   100] training loss: 0.00481768
INFO:root:[29,   150] training loss: 0.00865829
INFO:root:[29,   200] training loss: 0.01066298
INFO:root:[29,   250] training loss: 0.00613643
INFO:root:[29,   300] training loss: 0.01023910
INFO:root:[29,   350] training loss: 0.00968948
INFO:root:[29,   400] training loss: 0.00917260
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.00313297
INFO:root:[30,   100] training loss: 0.00480031
INFO:root:[30,   150] training loss: 0.00874920
INFO:root:[30,   200] training loss: 0.01079553
INFO:root:[30,   250] training loss: 0.00621627
INFO:root:[30,   300] training loss: 0.01067572
INFO:root:[30,   350] training loss: 0.00960548
INFO:root:[30,   400] training loss: 0.00922612
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.00320115
INFO:root:[31,   100] training loss: 0.00480112
INFO:root:[31,   150] training loss: 0.00887784
INFO:root:[31,   200] training loss: 0.01106484
INFO:root:[31,   250] training loss: 0.00636134
INFO:root:[31,   300] training loss: 0.01039860
INFO:root:[31,   350] training loss: 0.00960146
INFO:root:[31,   400] training loss: 0.00905963
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.00322790
INFO:root:[32,   100] training loss: 0.00483828
INFO:root:[32,   150] training loss: 0.00878383
INFO:root:[32,   200] training loss: 0.01111593
INFO:root:[32,   250] training loss: 0.00633590
INFO:root:[32,   300] training loss: 0.01050376
INFO:root:[32,   350] training loss: 0.00965735
INFO:root:[32,   400] training loss: 0.00912539
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.00320449
INFO:root:[33,   100] training loss: 0.00478644
INFO:root:[33,   150] training loss: 0.00889552
INFO:root:[33,   200] training loss: 0.01115697
INFO:root:[33,   250] training loss: 0.00641612
INFO:root:[33,   300] training loss: 0.01021894
INFO:root:[33,   350] training loss: 0.00963319
INFO:root:[33,   400] training loss: 0.00926266
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.00322332
INFO:root:[34,   100] training loss: 0.00479585
INFO:root:[34,   150] training loss: 0.00882973
INFO:root:[34,   200] training loss: 0.01111327
INFO:root:[34,   250] training loss: 0.00615216
INFO:root:[34,   300] training loss: 0.01034989
INFO:root:[34,   350] training loss: 0.00962114
INFO:root:[34,   400] training loss: 0.00911307
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.00322037
INFO:root:[35,   100] training loss: 0.00481317
INFO:root:[35,   150] training loss: 0.00885620
INFO:root:[35,   200] training loss: 0.01059800
INFO:root:[35,   250] training loss: 0.00603925
INFO:root:[35,   300] training loss: 0.01048293
INFO:root:[35,   350] training loss: 0.00960233
INFO:root:[35,   400] training loss: 0.00951530
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.00319626
INFO:root:[36,   100] training loss: 0.00483915
INFO:root:[36,   150] training loss: 0.00875463
INFO:root:[36,   200] training loss: 0.01068453
INFO:root:[36,   250] training loss: 0.00627027
INFO:root:[36,   300] training loss: 0.01034246
INFO:root:[36,   350] training loss: 0.00967023
INFO:root:[36,   400] training loss: 0.00916010
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.00320026
INFO:root:[37,   100] training loss: 0.00477860
INFO:root:[37,   150] training loss: 0.00877240
INFO:root:[37,   200] training loss: 0.01103370
INFO:root:[37,   250] training loss: 0.00625420
INFO:root:[37,   300] training loss: 0.01025476
INFO:root:[37,   350] training loss: 0.00970611
INFO:root:[37,   400] training loss: 0.00906820
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.00322745
INFO:root:[38,   100] training loss: 0.00478040
INFO:root:[38,   150] training loss: 0.00885491
INFO:root:[38,   200] training loss: 0.01111054
INFO:root:[38,   250] training loss: 0.00619791
INFO:root:[38,   300] training loss: 0.01030855
INFO:root:[38,   350] training loss: 0.00958506
INFO:root:[38,   400] training loss: 0.00923069
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.00319029
INFO:root:[39,   100] training loss: 0.00479612
INFO:root:[39,   150] training loss: 0.00896731
INFO:root:[39,   200] training loss: 0.01135887
INFO:root:[39,   250] training loss: 0.00642617
INFO:root:[39,   300] training loss: 0.01035327
INFO:root:[39,   350] training loss: 0.00957437
INFO:root:[39,   400] training loss: 0.00899704
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.00315574
INFO:root:[40,   100] training loss: 0.00479639
INFO:root:[40,   150] training loss: 0.00875815
INFO:root:[40,   200] training loss: 0.01089567
INFO:root:[40,   250] training loss: 0.00613491
INFO:root:[40,   300] training loss: 0.01031833
INFO:root:[40,   350] training loss: 0.00966304
INFO:root:[40,   400] training loss: 0.00925387
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.00318712
INFO:root:[41,   100] training loss: 0.00484093
INFO:root:[41,   150] training loss: 0.00883383
INFO:root:[41,   200] training loss: 0.01108042
INFO:root:[41,   250] training loss: 0.00625272
INFO:root:[41,   300] training loss: 0.01031122
INFO:root:[41,   350] training loss: 0.00955675
INFO:root:[41,   400] training loss: 0.00919434
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.00317622
INFO:root:[42,   100] training loss: 0.00480180
INFO:root:[42,   150] training loss: 0.00883820
INFO:root:[42,   200] training loss: 0.01043766
INFO:root:[42,   250] training loss: 0.00625345
INFO:root:[42,   300] training loss: 0.01025918
INFO:root:[42,   350] training loss: 0.00958474
INFO:root:[42,   400] training loss: 0.00901342
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.00318429
INFO:root:[43,   100] training loss: 0.00485502
INFO:root:[43,   150] training loss: 0.00885367
INFO:root:[43,   200] training loss: 0.01103300
INFO:root:[43,   250] training loss: 0.00622278
INFO:root:[43,   300] training loss: 0.01035609
INFO:root:[43,   350] training loss: 0.00954044
INFO:root:[43,   400] training loss: 0.00897583
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.00318607
INFO:root:[44,   100] training loss: 0.00480039
INFO:root:[44,   150] training loss: 0.00874224
INFO:root:[44,   200] training loss: 0.01109147
INFO:root:[44,   250] training loss: 0.00608669
INFO:root:[44,   300] training loss: 0.01019656
INFO:root:[44,   350] training loss: 0.00963195
INFO:root:[44,   400] training loss: 0.00909736
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.00315602
INFO:root:[45,   100] training loss: 0.00480821
INFO:root:[45,   150] training loss: 0.00892284
INFO:root:[45,   200] training loss: 0.01078264
INFO:root:[45,   250] training loss: 0.00633566
INFO:root:[45,   300] training loss: 0.01033049
INFO:root:[45,   350] training loss: 0.00961282
INFO:root:[45,   400] training loss: 0.00885114
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.00321205
INFO:root:[46,   100] training loss: 0.00482668
INFO:root:[46,   150] training loss: 0.00889099
INFO:root:[46,   200] training loss: 0.01154171
INFO:root:[46,   250] training loss: 0.00643789
INFO:root:[46,   300] training loss: 0.01036832
INFO:root:[46,   350] training loss: 0.00957730
INFO:root:[46,   400] training loss: 0.00924305
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.00319507
INFO:root:[47,   100] training loss: 0.00481463
INFO:root:[47,   150] training loss: 0.00889277
INFO:root:[47,   200] training loss: 0.01050297
INFO:root:[47,   250] training loss: 0.00626195
INFO:root:[47,   300] training loss: 0.01031982
INFO:root:[47,   350] training loss: 0.00960452
INFO:root:[47,   400] training loss: 0.00891429
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.00320558
INFO:root:[48,   100] training loss: 0.00473886
INFO:root:[48,   150] training loss: 0.00886252
INFO:root:[48,   200] training loss: 0.01076628
INFO:root:[48,   250] training loss: 0.00637457
INFO:root:[48,   300] training loss: 0.01033841
INFO:root:[48,   350] training loss: 0.00964805
INFO:root:[48,   400] training loss: 0.00960512
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.00318318
INFO:root:[49,   100] training loss: 0.00477293
INFO:root:[49,   150] training loss: 0.00895283
INFO:root:[49,   200] training loss: 0.01080825
INFO:root:[49,   250] training loss: 0.00641920
INFO:root:[49,   300] training loss: 0.01030904
INFO:root:[49,   350] training loss: 0.00954066
INFO:root:[49,   400] training loss: 0.00936195
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.00319755
INFO:root:[50,   100] training loss: 0.00479642
INFO:root:[50,   150] training loss: 0.00882571
INFO:root:[50,   200] training loss: 0.01093265
INFO:root:[50,   250] training loss: 0.00631070
INFO:root:[50,   300] training loss: 0.01023539
INFO:root:[50,   350] training loss: 0.00957707
INFO:root:[50,   400] training loss: 0.00881572
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 96 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.7625    0.5520    0.6404       221
           CD4+ T     0.9744    1.0000    0.9870       874
           CD8+ T     0.8729    0.9455    0.9077       385
 CD15+ neutrophil     0.9976    0.9997    0.9986      3671
   CD14+ monocyte     0.9679    0.9963    0.9819       272
          CD19+ B     0.7191    0.9826    0.8305       172
         CD56+ NK     1.0000    0.8467    0.9170       137
              NKT     0.7379    0.5404    0.6239       198
       eosinophil     0.9817    0.9847    0.9832       326

         accuracy                         0.9613      6256
        macro avg     0.8904    0.8720    0.8745      6256
     weighted avg     0.9604    0.9613    0.9590      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0   0.64042  0.987013  0.907731           0.998639         0.981884  0.830467   0.916996  0.623907     0.983155
INFO:root:statistics used: {'mean': tensor([0.0132, 0.0048, 0.0149, 0.0123, 0.0097, 0.0121, 0.0050, 0.0077]), 'std': tensor([0.0272, 0.0020, 0.0204, 0.0125, 0.0076, 0.0052, 0.0021, 0.0031])}
INFO:root:train dataset: 132012, test dataset: 6256
INFO:root:used only channels: [1, 3, 5, 6, 7, 9, 10, 11]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.03641846
INFO:root:[1,   100] training loss: 0.02726581
INFO:root:[1,   150] training loss: 0.05066691
INFO:root:[1,   200] training loss: 0.07052956
INFO:root:[1,   250] training loss: 0.06332789
INFO:root:[1,   300] training loss: 0.05062867
INFO:root:[1,   350] training loss: 0.05279096
INFO:root:[1,   400] training loss: 0.05987846
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.02756505
INFO:root:[2,   100] training loss: 0.02096596
INFO:root:[2,   150] training loss: 0.04190180
INFO:root:[2,   200] training loss: 0.05372344
INFO:root:[2,   250] training loss: 0.04867742
INFO:root:[2,   300] training loss: 0.05028678
INFO:root:[2,   350] training loss: 0.05119551
INFO:root:[2,   400] training loss: 0.05524453
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.01110514
INFO:root:[3,   100] training loss: 0.01924955
INFO:root:[3,   150] training loss: 0.04051539
INFO:root:[3,   200] training loss: 0.04717029
INFO:root:[3,   250] training loss: 0.04335651
INFO:root:[3,   300] training loss: 0.05024490
INFO:root:[3,   350] training loss: 0.04764459
INFO:root:[3,   400] training loss: 0.04861434
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.00619166
INFO:root:[4,   100] training loss: 0.01798642
INFO:root:[4,   150] training loss: 0.03457534
INFO:root:[4,   200] training loss: 0.03364581
INFO:root:[4,   250] training loss: 0.03470354
INFO:root:[4,   300] training loss: 0.04479792
INFO:root:[4,   350] training loss: 0.04265891
INFO:root:[4,   400] training loss: 0.04022833
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.00481600
INFO:root:[5,   100] training loss: 0.01529346
INFO:root:[5,   150] training loss: 0.02828041
INFO:root:[5,   200] training loss: 0.02479118
INFO:root:[5,   250] training loss: 0.02627269
INFO:root:[5,   300] training loss: 0.03773619
INFO:root:[5,   350] training loss: 0.03325788
INFO:root:[5,   400] training loss: 0.03182655
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.00398203
INFO:root:[6,   100] training loss: 0.01261580
INFO:root:[6,   150] training loss: 0.02125286
INFO:root:[6,   200] training loss: 0.01854804
INFO:root:[6,   250] training loss: 0.01929470
INFO:root:[6,   300] training loss: 0.03015115
INFO:root:[6,   350] training loss: 0.02529371
INFO:root:[6,   400] training loss: 0.02400757
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.00346238
INFO:root:[7,   100] training loss: 0.00985428
INFO:root:[7,   150] training loss: 0.01632981
INFO:root:[7,   200] training loss: 0.01485189
INFO:root:[7,   250] training loss: 0.01559583
INFO:root:[7,   300] training loss: 0.02349547
INFO:root:[7,   350] training loss: 0.01908121
INFO:root:[7,   400] training loss: 0.01897391
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.00366584
INFO:root:[8,   100] training loss: 0.01174822
INFO:root:[8,   150] training loss: 0.02635373
INFO:root:[8,   200] training loss: 0.02804935
INFO:root:[8,   250] training loss: 0.02200130
INFO:root:[8,   300] training loss: 0.03812042
INFO:root:[8,   350] training loss: 0.01819537
INFO:root:[8,   400] training loss: 0.01624831
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.00315643
INFO:root:[9,   100] training loss: 0.00960801
INFO:root:[9,   150] training loss: 0.01715756
INFO:root:[9,   200] training loss: 0.01464605
INFO:root:[9,   250] training loss: 0.01424712
INFO:root:[9,   300] training loss: 0.02746056
INFO:root:[9,   350] training loss: 0.01436961
INFO:root:[9,   400] training loss: 0.01556863
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.00313724
INFO:root:[10,   100] training loss: 0.00789391
INFO:root:[10,   150] training loss: 0.01455581
INFO:root:[10,   200] training loss: 0.01207074
INFO:root:[10,   250] training loss: 0.01325513
INFO:root:[10,   300] training loss: 0.02164081
INFO:root:[10,   350] training loss: 0.01345366
INFO:root:[10,   400] training loss: 0.01410721
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.00307140
INFO:root:[11,   100] training loss: 0.00687563
INFO:root:[11,   150] training loss: 0.01329338
INFO:root:[11,   200] training loss: 0.01147362
INFO:root:[11,   250] training loss: 0.01232463
INFO:root:[11,   300] training loss: 0.01867927
INFO:root:[11,   350] training loss: 0.01294031
INFO:root:[11,   400] training loss: 0.01311851
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.00299136
INFO:root:[12,   100] training loss: 0.00631953
INFO:root:[12,   150] training loss: 0.01254953
INFO:root:[12,   200] training loss: 0.01062018
INFO:root:[12,   250] training loss: 0.01138064
INFO:root:[12,   300] training loss: 0.01694545
INFO:root:[12,   350] training loss: 0.01225060
INFO:root:[12,   400] training loss: 0.01183623
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.00297597
INFO:root:[13,   100] training loss: 0.00621348
INFO:root:[13,   150] training loss: 0.01154249
INFO:root:[13,   200] training loss: 0.01004783
INFO:root:[13,   250] training loss: 0.01073012
INFO:root:[13,   300] training loss: 0.01561332
INFO:root:[13,   350] training loss: 0.01182982
INFO:root:[13,   400] training loss: 0.01112984
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.00290582
INFO:root:[14,   100] training loss: 0.00585548
INFO:root:[14,   150] training loss: 0.01094652
INFO:root:[14,   200] training loss: 0.00977880
INFO:root:[14,   250] training loss: 0.01005401
INFO:root:[14,   300] training loss: 0.01468279
INFO:root:[14,   350] training loss: 0.01122785
INFO:root:[14,   400] training loss: 0.01086761
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.00286567
INFO:root:[15,   100] training loss: 0.00548753
INFO:root:[15,   150] training loss: 0.01077168
INFO:root:[15,   200] training loss: 0.00964096
INFO:root:[15,   250] training loss: 0.00999278
INFO:root:[15,   300] training loss: 0.01409005
INFO:root:[15,   350] training loss: 0.01053542
INFO:root:[15,   400] training loss: 0.00945607
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.00289564
INFO:root:[16,   100] training loss: 0.00556646
INFO:root:[16,   150] training loss: 0.01044644
INFO:root:[16,   200] training loss: 0.00952784
INFO:root:[16,   250] training loss: 0.01001274
INFO:root:[16,   300] training loss: 0.01418379
INFO:root:[16,   350] training loss: 0.01063708
INFO:root:[16,   400] training loss: 0.00961826
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.00288424
INFO:root:[17,   100] training loss: 0.00553814
INFO:root:[17,   150] training loss: 0.01065243
INFO:root:[17,   200] training loss: 0.00906269
INFO:root:[17,   250] training loss: 0.00975024
INFO:root:[17,   300] training loss: 0.01386204
INFO:root:[17,   350] training loss: 0.01060597
INFO:root:[17,   400] training loss: 0.00960636
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.00287266
INFO:root:[18,   100] training loss: 0.00547701
INFO:root:[18,   150] training loss: 0.01040248
INFO:root:[18,   200] training loss: 0.00911963
INFO:root:[18,   250] training loss: 0.01006328
INFO:root:[18,   300] training loss: 0.01393286
INFO:root:[18,   350] training loss: 0.01051681
INFO:root:[18,   400] training loss: 0.00955840
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.00289548
INFO:root:[19,   100] training loss: 0.00543051
INFO:root:[19,   150] training loss: 0.01016396
INFO:root:[19,   200] training loss: 0.00904146
INFO:root:[19,   250] training loss: 0.00941205
INFO:root:[19,   300] training loss: 0.01376306
INFO:root:[19,   350] training loss: 0.01051497
INFO:root:[19,   400] training loss: 0.00977577
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.00284760
INFO:root:[20,   100] training loss: 0.00533891
INFO:root:[20,   150] training loss: 0.01013135
INFO:root:[20,   200] training loss: 0.00917592
INFO:root:[20,   250] training loss: 0.00966942
INFO:root:[20,   300] training loss: 0.01358674
INFO:root:[20,   350] training loss: 0.01043376
INFO:root:[20,   400] training loss: 0.00965546
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.00281996
INFO:root:[21,   100] training loss: 0.00538396
INFO:root:[21,   150] training loss: 0.01023185
INFO:root:[21,   200] training loss: 0.00908695
INFO:root:[21,   250] training loss: 0.00931690
INFO:root:[21,   300] training loss: 0.01366530
INFO:root:[21,   350] training loss: 0.01041216
INFO:root:[21,   400] training loss: 0.00967925
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.00287386
INFO:root:[22,   100] training loss: 0.00543804
INFO:root:[22,   150] training loss: 0.01027529
INFO:root:[22,   200] training loss: 0.00941318
INFO:root:[22,   250] training loss: 0.00928888
INFO:root:[22,   300] training loss: 0.01340884
INFO:root:[22,   350] training loss: 0.01033723
INFO:root:[22,   400] training loss: 0.00945638
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.00287409
INFO:root:[23,   100] training loss: 0.00527493
INFO:root:[23,   150] training loss: 0.01005471
INFO:root:[23,   200] training loss: 0.00884928
INFO:root:[23,   250] training loss: 0.00946378
INFO:root:[23,   300] training loss: 0.01346022
INFO:root:[23,   350] training loss: 0.01048608
INFO:root:[23,   400] training loss: 0.00990420
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.00284745
INFO:root:[24,   100] training loss: 0.00539594
INFO:root:[24,   150] training loss: 0.01003814
INFO:root:[24,   200] training loss: 0.00893898
INFO:root:[24,   250] training loss: 0.00936960
INFO:root:[24,   300] training loss: 0.01352590
INFO:root:[24,   350] training loss: 0.01026042
INFO:root:[24,   400] training loss: 0.00945914
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.00283162
INFO:root:[25,   100] training loss: 0.00526421
INFO:root:[25,   150] training loss: 0.01007803
INFO:root:[25,   200] training loss: 0.00890010
INFO:root:[25,   250] training loss: 0.00935357
INFO:root:[25,   300] training loss: 0.01348047
INFO:root:[25,   350] training loss: 0.01019452
INFO:root:[25,   400] training loss: 0.00973273
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.00287173
INFO:root:[26,   100] training loss: 0.00543911
INFO:root:[26,   150] training loss: 0.01016767
INFO:root:[26,   200] training loss: 0.00911340
INFO:root:[26,   250] training loss: 0.00952145
INFO:root:[26,   300] training loss: 0.01351380
INFO:root:[26,   350] training loss: 0.01054437
INFO:root:[26,   400] training loss: 0.00940426
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.00286624
INFO:root:[27,   100] training loss: 0.00537709
INFO:root:[27,   150] training loss: 0.01013797
INFO:root:[27,   200] training loss: 0.00877032
INFO:root:[27,   250] training loss: 0.00908728
INFO:root:[27,   300] training loss: 0.01357613
INFO:root:[27,   350] training loss: 0.01021748
INFO:root:[27,   400] training loss: 0.00950825
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.00287313
INFO:root:[28,   100] training loss: 0.00543151
INFO:root:[28,   150] training loss: 0.00982832
INFO:root:[28,   200] training loss: 0.00906445
INFO:root:[28,   250] training loss: 0.00956816
INFO:root:[28,   300] training loss: 0.01337885
INFO:root:[28,   350] training loss: 0.01032351
INFO:root:[28,   400] training loss: 0.00974003
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.00284260
INFO:root:[29,   100] training loss: 0.00535918
INFO:root:[29,   150] training loss: 0.01002600
INFO:root:[29,   200] training loss: 0.00893286
INFO:root:[29,   250] training loss: 0.00929583
INFO:root:[29,   300] training loss: 0.01333562
INFO:root:[29,   350] training loss: 0.01060038
INFO:root:[29,   400] training loss: 0.00987998
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.00284828
INFO:root:[30,   100] training loss: 0.00520002
INFO:root:[30,   150] training loss: 0.00981064
INFO:root:[30,   200] training loss: 0.00880794
INFO:root:[30,   250] training loss: 0.00941784
INFO:root:[30,   300] training loss: 0.01344529
INFO:root:[30,   350] training loss: 0.01039621
INFO:root:[30,   400] training loss: 0.00968259
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.00288006
INFO:root:[31,   100] training loss: 0.00535791
INFO:root:[31,   150] training loss: 0.00976970
INFO:root:[31,   200] training loss: 0.00887291
INFO:root:[31,   250] training loss: 0.00939229
INFO:root:[31,   300] training loss: 0.01340088
INFO:root:[31,   350] training loss: 0.01044114
INFO:root:[31,   400] training loss: 0.00949751
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.00289364
INFO:root:[32,   100] training loss: 0.00531497
INFO:root:[32,   150] training loss: 0.00998384
INFO:root:[32,   200] training loss: 0.00896044
INFO:root:[32,   250] training loss: 0.00945785
INFO:root:[32,   300] training loss: 0.01340494
INFO:root:[32,   350] training loss: 0.01026055
INFO:root:[32,   400] training loss: 0.00945731
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.00286194
INFO:root:[33,   100] training loss: 0.00548466
INFO:root:[33,   150] training loss: 0.00992095
INFO:root:[33,   200] training loss: 0.00930573
INFO:root:[33,   250] training loss: 0.00912620
INFO:root:[33,   300] training loss: 0.01353012
INFO:root:[33,   350] training loss: 0.01030432
INFO:root:[33,   400] training loss: 0.00949177
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.00285861
INFO:root:[34,   100] training loss: 0.00533323
INFO:root:[34,   150] training loss: 0.00985646
INFO:root:[34,   200] training loss: 0.00942434
INFO:root:[34,   250] training loss: 0.00929362
INFO:root:[34,   300] training loss: 0.01341491
INFO:root:[34,   350] training loss: 0.01035696
INFO:root:[34,   400] training loss: 0.00939985
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.00288647
INFO:root:[35,   100] training loss: 0.00536380
INFO:root:[35,   150] training loss: 0.01009735
INFO:root:[35,   200] training loss: 0.00908784
INFO:root:[35,   250] training loss: 0.00928138
INFO:root:[35,   300] training loss: 0.01346031
INFO:root:[35,   350] training loss: 0.01020393
INFO:root:[35,   400] training loss: 0.00953159
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.00283727
INFO:root:[36,   100] training loss: 0.00531420
INFO:root:[36,   150] training loss: 0.01031228
INFO:root:[36,   200] training loss: 0.00877201
INFO:root:[36,   250] training loss: 0.00935760
INFO:root:[36,   300] training loss: 0.01329933
INFO:root:[36,   350] training loss: 0.01024545
INFO:root:[36,   400] training loss: 0.00958520
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.00281289
INFO:root:[37,   100] training loss: 0.00527636
INFO:root:[37,   150] training loss: 0.01012493
INFO:root:[37,   200] training loss: 0.00920336
INFO:root:[37,   250] training loss: 0.00932159
INFO:root:[37,   300] training loss: 0.01355161
INFO:root:[37,   350] training loss: 0.01031026
INFO:root:[37,   400] training loss: 0.00949907
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.00283805
INFO:root:[38,   100] training loss: 0.00540357
INFO:root:[38,   150] training loss: 0.01003755
INFO:root:[38,   200] training loss: 0.00913401
INFO:root:[38,   250] training loss: 0.00933025
INFO:root:[38,   300] training loss: 0.01338152
INFO:root:[38,   350] training loss: 0.01037028
INFO:root:[38,   400] training loss: 0.00960581
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.00288300
INFO:root:[39,   100] training loss: 0.00528194
INFO:root:[39,   150] training loss: 0.01007669
INFO:root:[39,   200] training loss: 0.00916361
INFO:root:[39,   250] training loss: 0.00970400
INFO:root:[39,   300] training loss: 0.01349306
INFO:root:[39,   350] training loss: 0.01018169
INFO:root:[39,   400] training loss: 0.00942897
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.00285966
INFO:root:[40,   100] training loss: 0.00529067
INFO:root:[40,   150] training loss: 0.00993441
INFO:root:[40,   200] training loss: 0.00874285
INFO:root:[40,   250] training loss: 0.00962802
INFO:root:[40,   300] training loss: 0.01346043
INFO:root:[40,   350] training loss: 0.01027111
INFO:root:[40,   400] training loss: 0.00963465
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.00285823
INFO:root:[41,   100] training loss: 0.00536415
INFO:root:[41,   150] training loss: 0.01024212
INFO:root:[41,   200] training loss: 0.00913594
INFO:root:[41,   250] training loss: 0.00912844
INFO:root:[41,   300] training loss: 0.01345132
INFO:root:[41,   350] training loss: 0.01021658
INFO:root:[41,   400] training loss: 0.00961167
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.00286074
INFO:root:[42,   100] training loss: 0.00531474
INFO:root:[42,   150] training loss: 0.01010357
INFO:root:[42,   200] training loss: 0.00880303
INFO:root:[42,   250] training loss: 0.00933283
INFO:root:[42,   300] training loss: 0.01337849
INFO:root:[42,   350] training loss: 0.01044433
INFO:root:[42,   400] training loss: 0.00972361
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.00286919
INFO:root:[43,   100] training loss: 0.00529608
INFO:root:[43,   150] training loss: 0.01070542
INFO:root:[43,   200] training loss: 0.00902397
INFO:root:[43,   250] training loss: 0.00946704
INFO:root:[43,   300] training loss: 0.01349033
INFO:root:[43,   350] training loss: 0.01031424
INFO:root:[43,   400] training loss: 0.00946088
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.00286451
INFO:root:[44,   100] training loss: 0.00529141
INFO:root:[44,   150] training loss: 0.00989058
INFO:root:[44,   200] training loss: 0.00876722
INFO:root:[44,   250] training loss: 0.00965249
INFO:root:[44,   300] training loss: 0.01349944
INFO:root:[44,   350] training loss: 0.01049916
INFO:root:[44,   400] training loss: 0.00965478
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.00280227
INFO:root:[45,   100] training loss: 0.00542593
INFO:root:[45,   150] training loss: 0.01023688
INFO:root:[45,   200] training loss: 0.00870947
INFO:root:[45,   250] training loss: 0.00967479
INFO:root:[45,   300] training loss: 0.01347508
INFO:root:[45,   350] training loss: 0.01025349
INFO:root:[45,   400] training loss: 0.00957818
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.00284758
INFO:root:[46,   100] training loss: 0.00534461
INFO:root:[46,   150] training loss: 0.01009253
INFO:root:[46,   200] training loss: 0.00896351
INFO:root:[46,   250] training loss: 0.00950526
INFO:root:[46,   300] training loss: 0.01338050
INFO:root:[46,   350] training loss: 0.01029051
INFO:root:[46,   400] training loss: 0.00961409
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.00286520
INFO:root:[47,   100] training loss: 0.00536001
INFO:root:[47,   150] training loss: 0.00996748
INFO:root:[47,   200] training loss: 0.00936924
INFO:root:[47,   250] training loss: 0.00943781
INFO:root:[47,   300] training loss: 0.01345336
INFO:root:[47,   350] training loss: 0.01032659
INFO:root:[47,   400] training loss: 0.00939273
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.00289426
INFO:root:[48,   100] training loss: 0.00538130
INFO:root:[48,   150] training loss: 0.00986325
INFO:root:[48,   200] training loss: 0.00886828
INFO:root:[48,   250] training loss: 0.00941592
INFO:root:[48,   300] training loss: 0.01340564
INFO:root:[48,   350] training loss: 0.01024129
INFO:root:[48,   400] training loss: 0.00919123
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.00286679
INFO:root:[49,   100] training loss: 0.00528249
INFO:root:[49,   150] training loss: 0.01007986
INFO:root:[49,   200] training loss: 0.00878996
INFO:root:[49,   250] training loss: 0.00932248
INFO:root:[49,   300] training loss: 0.01354204
INFO:root:[49,   350] training loss: 0.01016780
INFO:root:[49,   400] training loss: 0.00953263
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.00280250
INFO:root:[50,   100] training loss: 0.00538041
INFO:root:[50,   150] training loss: 0.00992960
INFO:root:[50,   200] training loss: 0.00904103
INFO:root:[50,   250] training loss: 0.00946431
INFO:root:[50,   300] training loss: 0.01344186
INFO:root:[50,   350] training loss: 0.01016770
INFO:root:[50,   400] training loss: 0.00937427
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 96 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.8607    0.6360    0.7315       272
           CD4+ T     0.9719    0.9989    0.9852       899
           CD8+ T     0.8366    0.9772    0.9014       351
 CD15+ neutrophil     0.9978    0.9997    0.9988      3657
   CD14+ monocyte     0.9438    0.9921    0.9674       254
          CD19+ B     0.7929    0.9752    0.8747       161
         CD56+ NK     0.9846    0.9143    0.9481       140
              NKT     0.7778    0.5463    0.6418       205
       eosinophil     0.9937    0.9968    0.9953       317

         accuracy                         0.9647      6256
        macro avg     0.9066    0.8930    0.8938      6256
     weighted avg     0.9639    0.9647    0.9623      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.731501  0.985189  0.901445           0.998771          0.96737  0.874652   0.948148  0.641834     0.995276
INFO:root:statistics used: {'mean': tensor([0.0132, 0.0048, 0.0149, 0.0123, 0.0097, 0.0121, 0.0050, 0.0077]), 'std': tensor([0.0271, 0.0020, 0.0204, 0.0125, 0.0075, 0.0052, 0.0021, 0.0031])}
INFO:root:train dataset: 132219, test dataset: 6256
INFO:root:used only channels: [1, 3, 5, 6, 7, 9, 10, 11]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.03143943
INFO:root:[1,   100] training loss: 0.03413114
INFO:root:[1,   150] training loss: 0.05834089
INFO:root:[1,   200] training loss: 0.05725810
INFO:root:[1,   250] training loss: 0.04947424
INFO:root:[1,   300] training loss: 0.05014532
INFO:root:[1,   350] training loss: 0.04614320
INFO:root:[1,   400] training loss: 0.06108510
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.02362329
INFO:root:[2,   100] training loss: 0.02336427
INFO:root:[2,   150] training loss: 0.04140827
INFO:root:[2,   200] training loss: 0.04664892
INFO:root:[2,   250] training loss: 0.04759050
INFO:root:[2,   300] training loss: 0.05382951
INFO:root:[2,   350] training loss: 0.05070382
INFO:root:[2,   400] training loss: 0.05781422
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.00921754
INFO:root:[3,   100] training loss: 0.02000787
INFO:root:[3,   150] training loss: 0.03765784
INFO:root:[3,   200] training loss: 0.04387504
INFO:root:[3,   250] training loss: 0.04247661
INFO:root:[3,   300] training loss: 0.04945618
INFO:root:[3,   350] training loss: 0.04498863
INFO:root:[3,   400] training loss: 0.04520282
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.00591018
INFO:root:[4,   100] training loss: 0.01626646
INFO:root:[4,   150] training loss: 0.02976779
INFO:root:[4,   200] training loss: 0.03446327
INFO:root:[4,   250] training loss: 0.03117171
INFO:root:[4,   300] training loss: 0.04163316
INFO:root:[4,   350] training loss: 0.03540095
INFO:root:[4,   400] training loss: 0.03348264
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.00423799
INFO:root:[5,   100] training loss: 0.01348966
INFO:root:[5,   150] training loss: 0.02235841
INFO:root:[5,   200] training loss: 0.02638093
INFO:root:[5,   250] training loss: 0.02270571
INFO:root:[5,   300] training loss: 0.03201209
INFO:root:[5,   350] training loss: 0.02601297
INFO:root:[5,   400] training loss: 0.02664334
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.00367137
INFO:root:[6,   100] training loss: 0.01099411
INFO:root:[6,   150] training loss: 0.01782384
INFO:root:[6,   200] training loss: 0.02135380
INFO:root:[6,   250] training loss: 0.01715367
INFO:root:[6,   300] training loss: 0.02244938
INFO:root:[6,   350] training loss: 0.01950612
INFO:root:[6,   400] training loss: 0.02324482
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.00359876
INFO:root:[7,   100] training loss: 0.00950187
INFO:root:[7,   150] training loss: 0.01403729
INFO:root:[7,   200] training loss: 0.01935872
INFO:root:[7,   250] training loss: 0.01370347
INFO:root:[7,   300] training loss: 0.01656760
INFO:root:[7,   350] training loss: 0.01530604
INFO:root:[7,   400] training loss: 0.02263548
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.00342211
INFO:root:[8,   100] training loss: 0.00955138
INFO:root:[8,   150] training loss: 0.02215707
INFO:root:[8,   200] training loss: 0.04203258
INFO:root:[8,   250] training loss: 0.01879331
INFO:root:[8,   300] training loss: 0.01698051
INFO:root:[8,   350] training loss: 0.01234816
INFO:root:[8,   400] training loss: 0.01251688
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.00312441
INFO:root:[9,   100] training loss: 0.00667396
INFO:root:[9,   150] training loss: 0.01354635
INFO:root:[9,   200] training loss: 0.02750308
INFO:root:[9,   250] training loss: 0.01501311
INFO:root:[9,   300] training loss: 0.01315494
INFO:root:[9,   350] training loss: 0.01218629
INFO:root:[9,   400] training loss: 0.01438218
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.00306762
INFO:root:[10,   100] training loss: 0.00608138
INFO:root:[10,   150] training loss: 0.01213130
INFO:root:[10,   200] training loss: 0.02160431
INFO:root:[10,   250] training loss: 0.01344158
INFO:root:[10,   300] training loss: 0.01236289
INFO:root:[10,   350] training loss: 0.01185231
INFO:root:[10,   400] training loss: 0.01504341
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.00301219
INFO:root:[11,   100] training loss: 0.00578381
INFO:root:[11,   150] training loss: 0.01116869
INFO:root:[11,   200] training loss: 0.01864184
INFO:root:[11,   250] training loss: 0.01249857
INFO:root:[11,   300] training loss: 0.01183100
INFO:root:[11,   350] training loss: 0.01190554
INFO:root:[11,   400] training loss: 0.01374667
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.00293378
INFO:root:[12,   100] training loss: 0.00558133
INFO:root:[12,   150] training loss: 0.01068307
INFO:root:[12,   200] training loss: 0.01625575
INFO:root:[12,   250] training loss: 0.01176371
INFO:root:[12,   300] training loss: 0.01110783
INFO:root:[12,   350] training loss: 0.01152409
INFO:root:[12,   400] training loss: 0.01280630
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.00291257
INFO:root:[13,   100] training loss: 0.00531361
INFO:root:[13,   150] training loss: 0.01005400
INFO:root:[13,   200] training loss: 0.01511541
INFO:root:[13,   250] training loss: 0.01121645
INFO:root:[13,   300] training loss: 0.01071575
INFO:root:[13,   350] training loss: 0.01123755
INFO:root:[13,   400] training loss: 0.01301520
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.00287914
INFO:root:[14,   100] training loss: 0.00524931
INFO:root:[14,   150] training loss: 0.00961304
INFO:root:[14,   200] training loss: 0.01392249
INFO:root:[14,   250] training loss: 0.01078840
INFO:root:[14,   300] training loss: 0.01035406
INFO:root:[14,   350] training loss: 0.01113763
INFO:root:[14,   400] training loss: 0.01144050
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.00281334
INFO:root:[15,   100] training loss: 0.00504573
INFO:root:[15,   150] training loss: 0.00951556
INFO:root:[15,   200] training loss: 0.01332491
INFO:root:[15,   250] training loss: 0.01087002
INFO:root:[15,   300] training loss: 0.00992286
INFO:root:[15,   350] training loss: 0.01056061
INFO:root:[15,   400] training loss: 0.00995133
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.00283881
INFO:root:[16,   100] training loss: 0.00496532
INFO:root:[16,   150] training loss: 0.00958366
INFO:root:[16,   200] training loss: 0.01322277
INFO:root:[16,   250] training loss: 0.01080744
INFO:root:[16,   300] training loss: 0.00981834
INFO:root:[16,   350] training loss: 0.01040085
INFO:root:[16,   400] training loss: 0.01028430
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.00285167
INFO:root:[17,   100] training loss: 0.00510892
INFO:root:[17,   150] training loss: 0.00941336
INFO:root:[17,   200] training loss: 0.01287392
INFO:root:[17,   250] training loss: 0.01066117
INFO:root:[17,   300] training loss: 0.00980790
INFO:root:[17,   350] training loss: 0.01041368
INFO:root:[17,   400] training loss: 0.00995703
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.00285665
INFO:root:[18,   100] training loss: 0.00501785
INFO:root:[18,   150] training loss: 0.00941774
INFO:root:[18,   200] training loss: 0.01276215
INFO:root:[18,   250] training loss: 0.01062892
INFO:root:[18,   300] training loss: 0.00976356
INFO:root:[18,   350] training loss: 0.01025314
INFO:root:[18,   400] training loss: 0.00969755
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.00281114
INFO:root:[19,   100] training loss: 0.00498290
INFO:root:[19,   150] training loss: 0.00940267
INFO:root:[19,   200] training loss: 0.01293754
INFO:root:[19,   250] training loss: 0.01048713
INFO:root:[19,   300] training loss: 0.00964804
INFO:root:[19,   350] training loss: 0.01038037
INFO:root:[19,   400] training loss: 0.01013686
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.00279485
INFO:root:[20,   100] training loss: 0.00495149
INFO:root:[20,   150] training loss: 0.00927788
INFO:root:[20,   200] training loss: 0.01280450
INFO:root:[20,   250] training loss: 0.01055659
INFO:root:[20,   300] training loss: 0.00983641
INFO:root:[20,   350] training loss: 0.01038853
INFO:root:[20,   400] training loss: 0.00973810
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.00277495
INFO:root:[21,   100] training loss: 0.00489581
INFO:root:[21,   150] training loss: 0.00925451
INFO:root:[21,   200] training loss: 0.01225610
INFO:root:[21,   250] training loss: 0.01029307
INFO:root:[21,   300] training loss: 0.00956039
INFO:root:[21,   350] training loss: 0.01033636
INFO:root:[21,   400] training loss: 0.00996534
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.00278906
INFO:root:[22,   100] training loss: 0.00489007
INFO:root:[22,   150] training loss: 0.00912363
INFO:root:[22,   200] training loss: 0.01227705
INFO:root:[22,   250] training loss: 0.01013077
INFO:root:[22,   300] training loss: 0.00960928
INFO:root:[22,   350] training loss: 0.01049070
INFO:root:[22,   400] training loss: 0.00979710
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.00277448
INFO:root:[23,   100] training loss: 0.00489692
INFO:root:[23,   150] training loss: 0.00914148
INFO:root:[23,   200] training loss: 0.01252170
INFO:root:[23,   250] training loss: 0.01019737
INFO:root:[23,   300] training loss: 0.00952229
INFO:root:[23,   350] training loss: 0.01020590
INFO:root:[23,   400] training loss: 0.01016929
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.00280915
INFO:root:[24,   100] training loss: 0.00490936
INFO:root:[24,   150] training loss: 0.00926520
INFO:root:[24,   200] training loss: 0.01231809
INFO:root:[24,   250] training loss: 0.01028447
INFO:root:[24,   300] training loss: 0.00956007
INFO:root:[24,   350] training loss: 0.01020516
INFO:root:[24,   400] training loss: 0.01036181
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.00280313
INFO:root:[25,   100] training loss: 0.00490053
INFO:root:[25,   150] training loss: 0.00929598
INFO:root:[25,   200] training loss: 0.01240509
INFO:root:[25,   250] training loss: 0.01028722
INFO:root:[25,   300] training loss: 0.00951061
INFO:root:[25,   350] training loss: 0.01048080
INFO:root:[25,   400] training loss: 0.00954892
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.00284361
INFO:root:[26,   100] training loss: 0.00490704
INFO:root:[26,   150] training loss: 0.00932117
INFO:root:[26,   200] training loss: 0.01232868
INFO:root:[26,   250] training loss: 0.01034644
INFO:root:[26,   300] training loss: 0.00944279
INFO:root:[26,   350] training loss: 0.01026294
INFO:root:[26,   400] training loss: 0.00979004
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.00282759
INFO:root:[27,   100] training loss: 0.00497671
INFO:root:[27,   150] training loss: 0.00926067
INFO:root:[27,   200] training loss: 0.01232018
INFO:root:[27,   250] training loss: 0.01016556
INFO:root:[27,   300] training loss: 0.00934331
INFO:root:[27,   350] training loss: 0.01017102
INFO:root:[27,   400] training loss: 0.01001576
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.00281320
INFO:root:[28,   100] training loss: 0.00487164
INFO:root:[28,   150] training loss: 0.00929602
INFO:root:[28,   200] training loss: 0.01231387
INFO:root:[28,   250] training loss: 0.01042829
INFO:root:[28,   300] training loss: 0.00969347
INFO:root:[28,   350] training loss: 0.01028147
INFO:root:[28,   400] training loss: 0.00990670
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.00278541
INFO:root:[29,   100] training loss: 0.00488346
INFO:root:[29,   150] training loss: 0.00902917
INFO:root:[29,   200] training loss: 0.01233133
INFO:root:[29,   250] training loss: 0.01017764
INFO:root:[29,   300] training loss: 0.00950286
INFO:root:[29,   350] training loss: 0.01047329
INFO:root:[29,   400] training loss: 0.00986311
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.00278689
INFO:root:[30,   100] training loss: 0.00488413
INFO:root:[30,   150] training loss: 0.00919443
INFO:root:[30,   200] training loss: 0.01231451
INFO:root:[30,   250] training loss: 0.01042733
INFO:root:[30,   300] training loss: 0.00961685
INFO:root:[30,   350] training loss: 0.01023407
INFO:root:[30,   400] training loss: 0.00981854
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.00283247
INFO:root:[31,   100] training loss: 0.00487096
INFO:root:[31,   150] training loss: 0.00891418
INFO:root:[31,   200] training loss: 0.01272557
INFO:root:[31,   250] training loss: 0.01027197
INFO:root:[31,   300] training loss: 0.00961707
INFO:root:[31,   350] training loss: 0.01045820
INFO:root:[31,   400] training loss: 0.01012897
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.00280056
INFO:root:[32,   100] training loss: 0.00492057
INFO:root:[32,   150] training loss: 0.00936269
INFO:root:[32,   200] training loss: 0.01242356
INFO:root:[32,   250] training loss: 0.01038172
INFO:root:[32,   300] training loss: 0.00955331
INFO:root:[32,   350] training loss: 0.01033094
INFO:root:[32,   400] training loss: 0.00988709
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.00279822
INFO:root:[33,   100] training loss: 0.00492460
INFO:root:[33,   150] training loss: 0.00938504
INFO:root:[33,   200] training loss: 0.01228355
INFO:root:[33,   250] training loss: 0.01016144
INFO:root:[33,   300] training loss: 0.00945926
INFO:root:[33,   350] training loss: 0.01034970
INFO:root:[33,   400] training loss: 0.00988404
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.00280152
INFO:root:[34,   100] training loss: 0.00485250
INFO:root:[34,   150] training loss: 0.00919929
INFO:root:[34,   200] training loss: 0.01227456
INFO:root:[34,   250] training loss: 0.01019721
INFO:root:[34,   300] training loss: 0.00967508
INFO:root:[34,   350] training loss: 0.01021631
INFO:root:[34,   400] training loss: 0.00987713
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.00281893
INFO:root:[35,   100] training loss: 0.00487192
INFO:root:[35,   150] training loss: 0.00907167
INFO:root:[35,   200] training loss: 0.01219450
INFO:root:[35,   250] training loss: 0.01027858
INFO:root:[35,   300] training loss: 0.00939385
INFO:root:[35,   350] training loss: 0.01057003
INFO:root:[35,   400] training loss: 0.00981762
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.00281038
INFO:root:[36,   100] training loss: 0.00487637
INFO:root:[36,   150] training loss: 0.00926547
INFO:root:[36,   200] training loss: 0.01215074
INFO:root:[36,   250] training loss: 0.01046731
INFO:root:[36,   300] training loss: 0.00965787
INFO:root:[36,   350] training loss: 0.01045572
INFO:root:[36,   400] training loss: 0.01001712
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.00279531
INFO:root:[37,   100] training loss: 0.00492342
INFO:root:[37,   150] training loss: 0.00930092
INFO:root:[37,   200] training loss: 0.01232441
INFO:root:[37,   250] training loss: 0.01014562
INFO:root:[37,   300] training loss: 0.00948182
INFO:root:[37,   350] training loss: 0.01029978
INFO:root:[37,   400] training loss: 0.00964612
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.00276644
INFO:root:[38,   100] training loss: 0.00484585
INFO:root:[38,   150] training loss: 0.00931068
INFO:root:[38,   200] training loss: 0.01219455
INFO:root:[38,   250] training loss: 0.01019013
INFO:root:[38,   300] training loss: 0.00957915
INFO:root:[38,   350] training loss: 0.01033601
INFO:root:[38,   400] training loss: 0.01005722
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.00279239
INFO:root:[39,   100] training loss: 0.00486114
INFO:root:[39,   150] training loss: 0.00920032
INFO:root:[39,   200] training loss: 0.01233991
INFO:root:[39,   250] training loss: 0.01023035
INFO:root:[39,   300] training loss: 0.00940324
INFO:root:[39,   350] training loss: 0.01029813
INFO:root:[39,   400] training loss: 0.00990226
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.00280537
INFO:root:[40,   100] training loss: 0.00492591
INFO:root:[40,   150] training loss: 0.00913729
INFO:root:[40,   200] training loss: 0.01225207
INFO:root:[40,   250] training loss: 0.01036150
INFO:root:[40,   300] training loss: 0.00951620
INFO:root:[40,   350] training loss: 0.01051035
INFO:root:[40,   400] training loss: 0.01017543
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.00277393
INFO:root:[41,   100] training loss: 0.00487496
INFO:root:[41,   150] training loss: 0.00917229
INFO:root:[41,   200] training loss: 0.01245075
INFO:root:[41,   250] training loss: 0.01003548
INFO:root:[41,   300] training loss: 0.00939747
INFO:root:[41,   350] training loss: 0.01041043
INFO:root:[41,   400] training loss: 0.00985516
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.00277450
INFO:root:[42,   100] training loss: 0.00483717
INFO:root:[42,   150] training loss: 0.00920796
INFO:root:[42,   200] training loss: 0.01222526
INFO:root:[42,   250] training loss: 0.01021788
INFO:root:[42,   300] training loss: 0.00954048
INFO:root:[42,   350] training loss: 0.01040857
INFO:root:[42,   400] training loss: 0.00995715
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.00283626
INFO:root:[43,   100] training loss: 0.00488455
INFO:root:[43,   150] training loss: 0.00909970
INFO:root:[43,   200] training loss: 0.01228211
INFO:root:[43,   250] training loss: 0.01024600
INFO:root:[43,   300] training loss: 0.00939004
INFO:root:[43,   350] training loss: 0.01030889
INFO:root:[43,   400] training loss: 0.01042239
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.00279648
INFO:root:[44,   100] training loss: 0.00491200
INFO:root:[44,   150] training loss: 0.00917165
INFO:root:[44,   200] training loss: 0.01226544
INFO:root:[44,   250] training loss: 0.01033821
INFO:root:[44,   300] training loss: 0.00955002
INFO:root:[44,   350] training loss: 0.01033073
INFO:root:[44,   400] training loss: 0.00979395
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.00283664
INFO:root:[45,   100] training loss: 0.00490995
INFO:root:[45,   150] training loss: 0.00923577
INFO:root:[45,   200] training loss: 0.01216989
INFO:root:[45,   250] training loss: 0.01037518
INFO:root:[45,   300] training loss: 0.00954308
INFO:root:[45,   350] training loss: 0.01035560
INFO:root:[45,   400] training loss: 0.01012160
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.00278248
INFO:root:[46,   100] training loss: 0.00489460
INFO:root:[46,   150] training loss: 0.00899791
INFO:root:[46,   200] training loss: 0.01229434
INFO:root:[46,   250] training loss: 0.01021798
INFO:root:[46,   300] training loss: 0.00954531
INFO:root:[46,   350] training loss: 0.01029344
INFO:root:[46,   400] training loss: 0.00947970
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.00280700
INFO:root:[47,   100] training loss: 0.00484303
INFO:root:[47,   150] training loss: 0.00894482
INFO:root:[47,   200] training loss: 0.01220811
INFO:root:[47,   250] training loss: 0.01006524
INFO:root:[47,   300] training loss: 0.00955538
INFO:root:[47,   350] training loss: 0.01016893
INFO:root:[47,   400] training loss: 0.00971633
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.00284010
INFO:root:[48,   100] training loss: 0.00485660
INFO:root:[48,   150] training loss: 0.00918161
INFO:root:[48,   200] training loss: 0.01237820
INFO:root:[48,   250] training loss: 0.01024904
INFO:root:[48,   300] training loss: 0.00945288
INFO:root:[48,   350] training loss: 0.01025837
INFO:root:[48,   400] training loss: 0.00971748
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.00279859
INFO:root:[49,   100] training loss: 0.00500473
INFO:root:[49,   150] training loss: 0.00903642
INFO:root:[49,   200] training loss: 0.01230528
INFO:root:[49,   250] training loss: 0.01036010
INFO:root:[49,   300] training loss: 0.00953254
INFO:root:[49,   350] training loss: 0.01023693
INFO:root:[49,   400] training loss: 0.01046352
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.00280382
INFO:root:[50,   100] training loss: 0.00494032
INFO:root:[50,   150] training loss: 0.00907541
INFO:root:[50,   200] training loss: 0.01232425
INFO:root:[50,   250] training loss: 0.01022032
INFO:root:[50,   300] training loss: 0.00968842
INFO:root:[50,   350] training loss: 0.01029664
INFO:root:[50,   400] training loss: 0.01027603
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 96 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.7387    0.6212    0.6749       264
           CD4+ T     0.9790    0.9979    0.9883       933
           CD8+ T     0.8678    0.9783    0.9197       369
 CD15+ neutrophil     0.9989    1.0000    0.9994      3634
   CD14+ monocyte     0.9714    0.9876    0.9794       241
          CD19+ B     0.8230    0.9208    0.8692       202
         CD56+ NK     0.9597    0.9370    0.9482       127
              NKT     0.7338    0.5485    0.6278       206
       eosinophil     0.9964    0.9964    0.9964       280

         accuracy                         0.9631      6256
        macro avg     0.8965    0.8875    0.8893      6256
     weighted avg     0.9608    0.9631    0.9610      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.674897  0.988323  0.919745            0.99945         0.979424  0.869159   0.948207  0.627778     0.996429
INFO:root:statistics used: {'mean': tensor([0.0132, 0.0048, 0.0149, 0.0123, 0.0097, 0.0121, 0.0050, 0.0077]), 'std': tensor([0.0271, 0.0020, 0.0203, 0.0125, 0.0075, 0.0052, 0.0021, 0.0031])}
INFO:root:train dataset: 131697, test dataset: 6256
INFO:root:used only channels: [1, 3, 5, 6, 7, 9, 10, 11]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.02854878
INFO:root:[1,   100] training loss: 0.03233018
INFO:root:[1,   150] training loss: 0.05773338
INFO:root:[1,   200] training loss: 0.06387004
INFO:root:[1,   250] training loss: 0.04984492
INFO:root:[1,   300] training loss: 0.05685559
INFO:root:[1,   350] training loss: 0.06387277
INFO:root:[1,   400] training loss: 0.06303141
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.02772212
INFO:root:[2,   100] training loss: 0.02316036
INFO:root:[2,   150] training loss: 0.04728397
INFO:root:[2,   200] training loss: 0.04674554
INFO:root:[2,   250] training loss: 0.03803588
INFO:root:[2,   300] training loss: 0.05527878
INFO:root:[2,   350] training loss: 0.05782715
INFO:root:[2,   400] training loss: 0.05754968
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.00957012
INFO:root:[3,   100] training loss: 0.02009358
INFO:root:[3,   150] training loss: 0.04134114
INFO:root:[3,   200] training loss: 0.03939819
INFO:root:[3,   250] training loss: 0.03088858
INFO:root:[3,   300] training loss: 0.05246921
INFO:root:[3,   350] training loss: 0.05192055
INFO:root:[3,   400] training loss: 0.05145161
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.00598795
INFO:root:[4,   100] training loss: 0.01715651
INFO:root:[4,   150] training loss: 0.03361494
INFO:root:[4,   200] training loss: 0.03224584
INFO:root:[4,   250] training loss: 0.02463737
INFO:root:[4,   300] training loss: 0.04968502
INFO:root:[4,   350] training loss: 0.04655832
INFO:root:[4,   400] training loss: 0.04209914
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.00477289
INFO:root:[5,   100] training loss: 0.01426699
INFO:root:[5,   150] training loss: 0.02607766
INFO:root:[5,   200] training loss: 0.02783367
INFO:root:[5,   250] training loss: 0.01919852
INFO:root:[5,   300] training loss: 0.04577394
INFO:root:[5,   350] training loss: 0.04192958
INFO:root:[5,   400] training loss: 0.03454989
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.00413556
INFO:root:[6,   100] training loss: 0.01172251
INFO:root:[6,   150] training loss: 0.02070766
INFO:root:[6,   200] training loss: 0.02447825
INFO:root:[6,   250] training loss: 0.01498126
INFO:root:[6,   300] training loss: 0.03928680
INFO:root:[6,   350] training loss: 0.03750712
INFO:root:[6,   400] training loss: 0.02864122
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.00378461
INFO:root:[7,   100] training loss: 0.00965018
INFO:root:[7,   150] training loss: 0.01664414
INFO:root:[7,   200] training loss: 0.02150613
INFO:root:[7,   250] training loss: 0.01219062
INFO:root:[7,   300] training loss: 0.02901547
INFO:root:[7,   350] training loss: 0.03191007
INFO:root:[7,   400] training loss: 0.02506566
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.00407232
INFO:root:[8,   100] training loss: 0.01044958
INFO:root:[8,   150] training loss: 0.02835131
INFO:root:[8,   200] training loss: 0.04409682
INFO:root:[8,   250] training loss: 0.01806272
INFO:root:[8,   300] training loss: 0.03346059
INFO:root:[8,   350] training loss: 0.02366362
INFO:root:[8,   400] training loss: 0.01516605
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.00355090
INFO:root:[9,   100] training loss: 0.00707092
INFO:root:[9,   150] training loss: 0.01643963
INFO:root:[9,   200] training loss: 0.02827428
INFO:root:[9,   250] training loss: 0.01291936
INFO:root:[9,   300] training loss: 0.02479437
INFO:root:[9,   350] training loss: 0.02378174
INFO:root:[9,   400] training loss: 0.01820176
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.00344913
INFO:root:[10,   100] training loss: 0.00665251
INFO:root:[10,   150] training loss: 0.01340662
INFO:root:[10,   200] training loss: 0.02236611
INFO:root:[10,   250] training loss: 0.01149971
INFO:root:[10,   300] training loss: 0.02052881
INFO:root:[10,   350] training loss: 0.02346312
INFO:root:[10,   400] training loss: 0.01882623
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.00337777
INFO:root:[11,   100] training loss: 0.00620496
INFO:root:[11,   150] training loss: 0.01239705
INFO:root:[11,   200] training loss: 0.01961778
INFO:root:[11,   250] training loss: 0.01098618
INFO:root:[11,   300] training loss: 0.01909181
INFO:root:[11,   350] training loss: 0.02322569
INFO:root:[11,   400] training loss: 0.01784609
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.00328825
INFO:root:[12,   100] training loss: 0.00587714
INFO:root:[12,   150] training loss: 0.01178558
INFO:root:[12,   200] training loss: 0.01816151
INFO:root:[12,   250] training loss: 0.01044466
INFO:root:[12,   300] training loss: 0.01777709
INFO:root:[12,   350] training loss: 0.02220844
INFO:root:[12,   400] training loss: 0.01606091
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.00320808
INFO:root:[13,   100] training loss: 0.00567010
INFO:root:[13,   150] training loss: 0.01125518
INFO:root:[13,   200] training loss: 0.01717105
INFO:root:[13,   250] training loss: 0.00973115
INFO:root:[13,   300] training loss: 0.01682140
INFO:root:[13,   350] training loss: 0.02180773
INFO:root:[13,   400] training loss: 0.01597242
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.00318177
INFO:root:[14,   100] training loss: 0.00553599
INFO:root:[14,   150] training loss: 0.01089790
INFO:root:[14,   200] training loss: 0.01590577
INFO:root:[14,   250] training loss: 0.00936559
INFO:root:[14,   300] training loss: 0.01603496
INFO:root:[14,   350] training loss: 0.02113460
INFO:root:[14,   400] training loss: 0.01487151
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.00317103
INFO:root:[15,   100] training loss: 0.00537786
INFO:root:[15,   150] training loss: 0.01062248
INFO:root:[15,   200] training loss: 0.01610690
INFO:root:[15,   250] training loss: 0.00962728
INFO:root:[15,   300] training loss: 0.01578284
INFO:root:[15,   350] training loss: 0.02054659
INFO:root:[15,   400] training loss: 0.01291140
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.00313122
INFO:root:[16,   100] training loss: 0.00541944
INFO:root:[16,   150] training loss: 0.01071800
INFO:root:[16,   200] training loss: 0.01607875
INFO:root:[16,   250] training loss: 0.00947113
INFO:root:[16,   300] training loss: 0.01547694
INFO:root:[16,   350] training loss: 0.02012232
INFO:root:[16,   400] training loss: 0.01283148
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.00319681
INFO:root:[17,   100] training loss: 0.00540886
INFO:root:[17,   150] training loss: 0.01072661
INFO:root:[17,   200] training loss: 0.01517878
INFO:root:[17,   250] training loss: 0.00943180
INFO:root:[17,   300] training loss: 0.01589266
INFO:root:[17,   350] training loss: 0.02033290
INFO:root:[17,   400] training loss: 0.01343218
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.00318350
INFO:root:[18,   100] training loss: 0.00532577
INFO:root:[18,   150] training loss: 0.01070874
INFO:root:[18,   200] training loss: 0.01608135
INFO:root:[18,   250] training loss: 0.00950366
INFO:root:[18,   300] training loss: 0.01558149
INFO:root:[18,   350] training loss: 0.01994295
INFO:root:[18,   400] training loss: 0.01282591
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.00319259
INFO:root:[19,   100] training loss: 0.00542407
INFO:root:[19,   150] training loss: 0.01057368
INFO:root:[19,   200] training loss: 0.01546015
INFO:root:[19,   250] training loss: 0.00931806
INFO:root:[19,   300] training loss: 0.01551379
INFO:root:[19,   350] training loss: 0.01987713
INFO:root:[19,   400] training loss: 0.01306496
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.00315461
INFO:root:[20,   100] training loss: 0.00523479
INFO:root:[20,   150] training loss: 0.01038360
INFO:root:[20,   200] training loss: 0.01508070
INFO:root:[20,   250] training loss: 0.00927759
INFO:root:[20,   300] training loss: 0.01516890
INFO:root:[20,   350] training loss: 0.01987057
INFO:root:[20,   400] training loss: 0.01321511
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.00315099
INFO:root:[21,   100] training loss: 0.00530053
INFO:root:[21,   150] training loss: 0.01023252
INFO:root:[21,   200] training loss: 0.01499905
INFO:root:[21,   250] training loss: 0.00917750
INFO:root:[21,   300] training loss: 0.01503733
INFO:root:[21,   350] training loss: 0.01971045
INFO:root:[21,   400] training loss: 0.01319191
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.00314854
INFO:root:[22,   100] training loss: 0.00526887
INFO:root:[22,   150] training loss: 0.01049399
INFO:root:[22,   200] training loss: 0.01514671
INFO:root:[22,   250] training loss: 0.00923770
INFO:root:[22,   300] training loss: 0.01508797
INFO:root:[22,   350] training loss: 0.01979827
INFO:root:[22,   400] training loss: 0.01272793
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.00314394
INFO:root:[23,   100] training loss: 0.00535437
INFO:root:[23,   150] training loss: 0.01053155
INFO:root:[23,   200] training loss: 0.01481338
INFO:root:[23,   250] training loss: 0.00922643
INFO:root:[23,   300] training loss: 0.01500347
INFO:root:[23,   350] training loss: 0.01984378
INFO:root:[23,   400] training loss: 0.01335837
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.00314032
INFO:root:[24,   100] training loss: 0.00529135
INFO:root:[24,   150] training loss: 0.01051377
INFO:root:[24,   200] training loss: 0.01509143
INFO:root:[24,   250] training loss: 0.00926564
INFO:root:[24,   300] training loss: 0.01494973
INFO:root:[24,   350] training loss: 0.01979445
INFO:root:[24,   400] training loss: 0.01308006
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.00317076
INFO:root:[25,   100] training loss: 0.00530253
INFO:root:[25,   150] training loss: 0.01038946
INFO:root:[25,   200] training loss: 0.01483826
INFO:root:[25,   250] training loss: 0.00900981
INFO:root:[25,   300] training loss: 0.01542382
INFO:root:[25,   350] training loss: 0.02015779
INFO:root:[25,   400] training loss: 0.01312391
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.00311360
INFO:root:[26,   100] training loss: 0.00527767
INFO:root:[26,   150] training loss: 0.01046512
INFO:root:[26,   200] training loss: 0.01478674
INFO:root:[26,   250] training loss: 0.00913364
INFO:root:[26,   300] training loss: 0.01499767
INFO:root:[26,   350] training loss: 0.01984406
INFO:root:[26,   400] training loss: 0.01294531
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.00318939
INFO:root:[27,   100] training loss: 0.00527421
INFO:root:[27,   150] training loss: 0.01058934
INFO:root:[27,   200] training loss: 0.01503690
INFO:root:[27,   250] training loss: 0.00941262
INFO:root:[27,   300] training loss: 0.01511304
INFO:root:[27,   350] training loss: 0.01981870
INFO:root:[27,   400] training loss: 0.01299130
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.00311466
INFO:root:[28,   100] training loss: 0.00526088
INFO:root:[28,   150] training loss: 0.01024211
INFO:root:[28,   200] training loss: 0.01528088
INFO:root:[28,   250] training loss: 0.00913767
INFO:root:[28,   300] training loss: 0.01504499
INFO:root:[28,   350] training loss: 0.01976797
INFO:root:[28,   400] training loss: 0.01343177
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.00312557
INFO:root:[29,   100] training loss: 0.00530691
INFO:root:[29,   150] training loss: 0.01044936
INFO:root:[29,   200] training loss: 0.01545055
INFO:root:[29,   250] training loss: 0.00912211
INFO:root:[29,   300] training loss: 0.01487438
INFO:root:[29,   350] training loss: 0.01971460
INFO:root:[29,   400] training loss: 0.01323468
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.00315375
INFO:root:[30,   100] training loss: 0.00528245
INFO:root:[30,   150] training loss: 0.01044944
INFO:root:[30,   200] training loss: 0.01506308
INFO:root:[30,   250] training loss: 0.00902520
INFO:root:[30,   300] training loss: 0.01517640
INFO:root:[30,   350] training loss: 0.01995879
INFO:root:[30,   400] training loss: 0.01342416
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.00315014
INFO:root:[31,   100] training loss: 0.00524867
INFO:root:[31,   150] training loss: 0.01040222
INFO:root:[31,   200] training loss: 0.01535676
INFO:root:[31,   250] training loss: 0.00912834
INFO:root:[31,   300] training loss: 0.01488166
INFO:root:[31,   350] training loss: 0.02004890
INFO:root:[31,   400] training loss: 0.01246829
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.00316991
INFO:root:[32,   100] training loss: 0.00522021
INFO:root:[32,   150] training loss: 0.01029239
INFO:root:[32,   200] training loss: 0.01474058
INFO:root:[32,   250] training loss: 0.00909983
INFO:root:[32,   300] training loss: 0.01488499
INFO:root:[32,   350] training loss: 0.02000436
INFO:root:[32,   400] training loss: 0.01305565
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.00317760
INFO:root:[33,   100] training loss: 0.00534703
INFO:root:[33,   150] training loss: 0.01045907
INFO:root:[33,   200] training loss: 0.01504936
INFO:root:[33,   250] training loss: 0.00885141
INFO:root:[33,   300] training loss: 0.01495818
INFO:root:[33,   350] training loss: 0.01998116
INFO:root:[33,   400] training loss: 0.01285844
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.00316620
INFO:root:[34,   100] training loss: 0.00536018
INFO:root:[34,   150] training loss: 0.01021380
INFO:root:[34,   200] training loss: 0.01505347
INFO:root:[34,   250] training loss: 0.00899416
INFO:root:[34,   300] training loss: 0.01492439
INFO:root:[34,   350] training loss: 0.01955706
INFO:root:[34,   400] training loss: 0.01289169
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.00315793
INFO:root:[35,   100] training loss: 0.00529774
INFO:root:[35,   150] training loss: 0.01045675
INFO:root:[35,   200] training loss: 0.01553355
INFO:root:[35,   250] training loss: 0.00917404
INFO:root:[35,   300] training loss: 0.01505606
INFO:root:[35,   350] training loss: 0.01999614
INFO:root:[35,   400] training loss: 0.01356334
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.00314322
INFO:root:[36,   100] training loss: 0.00524121
INFO:root:[36,   150] training loss: 0.01031016
INFO:root:[36,   200] training loss: 0.01488396
INFO:root:[36,   250] training loss: 0.00912355
INFO:root:[36,   300] training loss: 0.01494038
INFO:root:[36,   350] training loss: 0.01970776
INFO:root:[36,   400] training loss: 0.01363245
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.00315344
INFO:root:[37,   100] training loss: 0.00528752
INFO:root:[37,   150] training loss: 0.01032917
INFO:root:[37,   200] training loss: 0.01498667
INFO:root:[37,   250] training loss: 0.00910029
INFO:root:[37,   300] training loss: 0.01478282
INFO:root:[37,   350] training loss: 0.01981032
INFO:root:[37,   400] training loss: 0.01306375
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.00312932
INFO:root:[38,   100] training loss: 0.00525939
INFO:root:[38,   150] training loss: 0.01047293
INFO:root:[38,   200] training loss: 0.01496716
INFO:root:[38,   250] training loss: 0.00909559
INFO:root:[38,   300] training loss: 0.01498185
INFO:root:[38,   350] training loss: 0.02001469
INFO:root:[38,   400] training loss: 0.01329169
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.00314085
INFO:root:[39,   100] training loss: 0.00527936
INFO:root:[39,   150] training loss: 0.01030428
INFO:root:[39,   200] training loss: 0.01533891
INFO:root:[39,   250] training loss: 0.00919163
INFO:root:[39,   300] training loss: 0.01488197
INFO:root:[39,   350] training loss: 0.01971466
INFO:root:[39,   400] training loss: 0.01301856
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.00318486
INFO:root:[40,   100] training loss: 0.00527298
INFO:root:[40,   150] training loss: 0.01029943
INFO:root:[40,   200] training loss: 0.01480617
INFO:root:[40,   250] training loss: 0.00928264
INFO:root:[40,   300] training loss: 0.01509934
INFO:root:[40,   350] training loss: 0.01994747
INFO:root:[40,   400] training loss: 0.01319739
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.00314165
INFO:root:[41,   100] training loss: 0.00527052
INFO:root:[41,   150] training loss: 0.01052678
INFO:root:[41,   200] training loss: 0.01463904
INFO:root:[41,   250] training loss: 0.00896638
INFO:root:[41,   300] training loss: 0.01501362
INFO:root:[41,   350] training loss: 0.01980815
INFO:root:[41,   400] training loss: 0.01279479
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.00316395
INFO:root:[42,   100] training loss: 0.00529241
INFO:root:[42,   150] training loss: 0.01031417
INFO:root:[42,   200] training loss: 0.01463131
INFO:root:[42,   250] training loss: 0.00906150
INFO:root:[42,   300] training loss: 0.01466298
INFO:root:[42,   350] training loss: 0.01977461
INFO:root:[42,   400] training loss: 0.01338851
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.00312061
INFO:root:[43,   100] training loss: 0.00527600
INFO:root:[43,   150] training loss: 0.01032588
INFO:root:[43,   200] training loss: 0.01530943
INFO:root:[43,   250] training loss: 0.00917383
INFO:root:[43,   300] training loss: 0.01474050
INFO:root:[43,   350] training loss: 0.01964224
INFO:root:[43,   400] training loss: 0.01302518
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.00313043
INFO:root:[44,   100] training loss: 0.00529719
INFO:root:[44,   150] training loss: 0.01039864
INFO:root:[44,   200] training loss: 0.01458546
INFO:root:[44,   250] training loss: 0.00910580
INFO:root:[44,   300] training loss: 0.01507006
INFO:root:[44,   350] training loss: 0.01991413
INFO:root:[44,   400] training loss: 0.01266200
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.00314038
INFO:root:[45,   100] training loss: 0.00524343
INFO:root:[45,   150] training loss: 0.01023854
INFO:root:[45,   200] training loss: 0.01469536
INFO:root:[45,   250] training loss: 0.00915198
INFO:root:[45,   300] training loss: 0.01507664
INFO:root:[45,   350] training loss: 0.01982472
INFO:root:[45,   400] training loss: 0.01242917
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.00313733
INFO:root:[46,   100] training loss: 0.00526211
INFO:root:[46,   150] training loss: 0.01040054
INFO:root:[46,   200] training loss: 0.01545804
INFO:root:[46,   250] training loss: 0.00911981
INFO:root:[46,   300] training loss: 0.01490487
INFO:root:[46,   350] training loss: 0.01996878
INFO:root:[46,   400] training loss: 0.01364991
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.00315161
INFO:root:[47,   100] training loss: 0.00528474
INFO:root:[47,   150] training loss: 0.01035427
INFO:root:[47,   200] training loss: 0.01474434
INFO:root:[47,   250] training loss: 0.00905338
INFO:root:[47,   300] training loss: 0.01529024
INFO:root:[47,   350] training loss: 0.01985199
INFO:root:[47,   400] training loss: 0.01331190
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.00315582
INFO:root:[48,   100] training loss: 0.00527223
INFO:root:[48,   150] training loss: 0.01053809
INFO:root:[48,   200] training loss: 0.01516505
INFO:root:[48,   250] training loss: 0.00931580
INFO:root:[48,   300] training loss: 0.01500426
INFO:root:[48,   350] training loss: 0.01975802
INFO:root:[48,   400] training loss: 0.01312352
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.00317437
INFO:root:[49,   100] training loss: 0.00530543
INFO:root:[49,   150] training loss: 0.01032098
INFO:root:[49,   200] training loss: 0.01488536
INFO:root:[49,   250] training loss: 0.00903735
INFO:root:[49,   300] training loss: 0.01512517
INFO:root:[49,   350] training loss: 0.02006321
INFO:root:[49,   400] training loss: 0.01274505
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.00313739
INFO:root:[50,   100] training loss: 0.00531994
INFO:root:[50,   150] training loss: 0.01025494
INFO:root:[50,   200] training loss: 0.01496244
INFO:root:[50,   250] training loss: 0.00902304
INFO:root:[50,   300] training loss: 0.01485706
INFO:root:[50,   350] training loss: 0.01974990
INFO:root:[50,   400] training loss: 0.01292783
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 96 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.7865    0.5323    0.6349       263
           CD4+ T     0.9717    0.9978    0.9845       894
           CD8+ T     0.8809    0.9607    0.9191       331
 CD15+ neutrophil     0.9981    0.9989    0.9985      3692
   CD14+ monocyte     0.9594    0.9886    0.9738       263
          CD19+ B     0.8000    0.8736    0.8352       174
         CD56+ NK     0.9474    0.9474    0.9474       133
              NKT     0.6950    0.6985    0.6967       199
       eosinophil     0.9871    0.9967    0.9919       307

         accuracy                         0.9624      6256
        macro avg     0.8918    0.8883    0.8869      6256
     weighted avg     0.9608    0.9624    0.9604      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.634921  0.984547  0.919075           0.998511         0.973783  0.835165   0.947368  0.696742     0.991896
