INFO:root:the deviced being used is cuda:0
INFO:root:Start validation
INFO:root:statistics used: {'mean': tensor([0.0149, 0.1692]), 'std': tensor([0.0204, 0.0625])}
INFO:root:train dataset: 131886, test dataset: 6256
INFO:root:used only channels: [5, 8]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.04035578
INFO:root:[1,   100] training loss: 0.03710545
INFO:root:[1,   150] training loss: 0.05192631
INFO:root:[1,   200] training loss: 0.05527121
INFO:root:[1,   250] training loss: 0.04940734
INFO:root:[1,   300] training loss: 0.06026104
INFO:root:[1,   350] training loss: 0.05702140
INFO:root:[1,   400] training loss: 0.06537406
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.03563207
INFO:root:[2,   100] training loss: 0.03012604
INFO:root:[2,   150] training loss: 0.05006788
INFO:root:[2,   200] training loss: 0.05420423
INFO:root:[2,   250] training loss: 0.05008448
INFO:root:[2,   300] training loss: 0.05628008
INFO:root:[2,   350] training loss: 0.05383351
INFO:root:[2,   400] training loss: 0.05721895
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.02484217
INFO:root:[3,   100] training loss: 0.02874853
INFO:root:[3,   150] training loss: 0.04796839
INFO:root:[3,   200] training loss: 0.05042406
INFO:root:[3,   250] training loss: 0.04759142
INFO:root:[3,   300] training loss: 0.05449103
INFO:root:[3,   350] training loss: 0.05224154
INFO:root:[3,   400] training loss: 0.05358123
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.02129582
INFO:root:[4,   100] training loss: 0.02840061
INFO:root:[4,   150] training loss: 0.04726717
INFO:root:[4,   200] training loss: 0.05030590
INFO:root:[4,   250] training loss: 0.04666824
INFO:root:[4,   300] training loss: 0.04967131
INFO:root:[4,   350] training loss: 0.05129534
INFO:root:[4,   400] training loss: 0.05345423
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.01807527
INFO:root:[5,   100] training loss: 0.02835355
INFO:root:[5,   150] training loss: 0.04690195
INFO:root:[5,   200] training loss: 0.04762637
INFO:root:[5,   250] training loss: 0.04646402
INFO:root:[5,   300] training loss: 0.04845938
INFO:root:[5,   350] training loss: 0.05092880
INFO:root:[5,   400] training loss: 0.04728343
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.01694216
INFO:root:[6,   100] training loss: 0.02799149
INFO:root:[6,   150] training loss: 0.04344143
INFO:root:[6,   200] training loss: 0.04808418
INFO:root:[6,   250] training loss: 0.04363431
INFO:root:[6,   300] training loss: 0.04651655
INFO:root:[6,   350] training loss: 0.04646062
INFO:root:[6,   400] training loss: 0.04490771
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.01604029
INFO:root:[7,   100] training loss: 0.02845729
INFO:root:[7,   150] training loss: 0.04099917
INFO:root:[7,   200] training loss: 0.04211788
INFO:root:[7,   250] training loss: 0.03652749
INFO:root:[7,   300] training loss: 0.04285712
INFO:root:[7,   350] training loss: 0.04371431
INFO:root:[7,   400] training loss: 0.03824765
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.01723855
INFO:root:[8,   100] training loss: 0.03085980
INFO:root:[8,   150] training loss: 0.06276191
INFO:root:[8,   200] training loss: 0.06270729
INFO:root:[8,   250] training loss: 0.05322016
INFO:root:[8,   300] training loss: 0.05868645
INFO:root:[8,   350] training loss: 0.04907363
INFO:root:[8,   400] training loss: 0.02861651
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.01522566
INFO:root:[9,   100] training loss: 0.02524288
INFO:root:[9,   150] training loss: 0.04978090
INFO:root:[9,   200] training loss: 0.05338846
INFO:root:[9,   250] training loss: 0.04087149
INFO:root:[9,   300] training loss: 0.05229252
INFO:root:[9,   350] training loss: 0.04669339
INFO:root:[9,   400] training loss: 0.03289195
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.01453342
INFO:root:[10,   100] training loss: 0.02349464
INFO:root:[10,   150] training loss: 0.04709786
INFO:root:[10,   200] training loss: 0.05069873
INFO:root:[10,   250] training loss: 0.03599602
INFO:root:[10,   300] training loss: 0.04940467
INFO:root:[10,   350] training loss: 0.04578912
INFO:root:[10,   400] training loss: 0.03427341
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.01432457
INFO:root:[11,   100] training loss: 0.02267076
INFO:root:[11,   150] training loss: 0.04530883
INFO:root:[11,   200] training loss: 0.04927569
INFO:root:[11,   250] training loss: 0.03307335
INFO:root:[11,   300] training loss: 0.04739264
INFO:root:[11,   350] training loss: 0.04588555
INFO:root:[11,   400] training loss: 0.03410985
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.01417591
INFO:root:[12,   100] training loss: 0.02130740
INFO:root:[12,   150] training loss: 0.04364451
INFO:root:[12,   200] training loss: 0.04859730
INFO:root:[12,   250] training loss: 0.03035448
INFO:root:[12,   300] training loss: 0.04570600
INFO:root:[12,   350] training loss: 0.04476320
INFO:root:[12,   400] training loss: 0.03407710
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.01420559
INFO:root:[13,   100] training loss: 0.02060106
INFO:root:[13,   150] training loss: 0.04191033
INFO:root:[13,   200] training loss: 0.04816300
INFO:root:[13,   250] training loss: 0.02922058
INFO:root:[13,   300] training loss: 0.04443781
INFO:root:[13,   350] training loss: 0.04457282
INFO:root:[13,   400] training loss: 0.03395512
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.01421894
INFO:root:[14,   100] training loss: 0.01993283
INFO:root:[14,   150] training loss: 0.04092713
INFO:root:[14,   200] training loss: 0.04799718
INFO:root:[14,   250] training loss: 0.02748730
INFO:root:[14,   300] training loss: 0.04306626
INFO:root:[14,   350] training loss: 0.04376126
INFO:root:[14,   400] training loss: 0.03320915
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.01412037
INFO:root:[15,   100] training loss: 0.01943243
INFO:root:[15,   150] training loss: 0.04318901
INFO:root:[15,   200] training loss: 0.05268657
INFO:root:[15,   250] training loss: 0.02763808
INFO:root:[15,   300] training loss: 0.04467536
INFO:root:[15,   350] training loss: 0.03941030
INFO:root:[15,   400] training loss: 0.02811251
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.01403754
INFO:root:[16,   100] training loss: 0.01980017
INFO:root:[16,   150] training loss: 0.04192661
INFO:root:[16,   200] training loss: 0.05130949
INFO:root:[16,   250] training loss: 0.02749755
INFO:root:[16,   300] training loss: 0.04346804
INFO:root:[16,   350] training loss: 0.03930664
INFO:root:[16,   400] training loss: 0.02903721
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.01403166
INFO:root:[17,   100] training loss: 0.01968954
INFO:root:[17,   150] training loss: 0.04162623
INFO:root:[17,   200] training loss: 0.04994542
INFO:root:[17,   250] training loss: 0.02670861
INFO:root:[17,   300] training loss: 0.04249284
INFO:root:[17,   350] training loss: 0.03950821
INFO:root:[17,   400] training loss: 0.03006115
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.01396992
INFO:root:[18,   100] training loss: 0.01922710
INFO:root:[18,   150] training loss: 0.04133368
INFO:root:[18,   200] training loss: 0.04952471
INFO:root:[18,   250] training loss: 0.02642220
INFO:root:[18,   300] training loss: 0.04258761
INFO:root:[18,   350] training loss: 0.03916689
INFO:root:[18,   400] training loss: 0.03048064
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.01391522
INFO:root:[19,   100] training loss: 0.01907453
INFO:root:[19,   150] training loss: 0.04115448
INFO:root:[19,   200] training loss: 0.04878634
INFO:root:[19,   250] training loss: 0.02723906
INFO:root:[19,   300] training loss: 0.04201346
INFO:root:[19,   350] training loss: 0.03941044
INFO:root:[19,   400] training loss: 0.03089578
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.01406649
INFO:root:[20,   100] training loss: 0.01881595
INFO:root:[20,   150] training loss: 0.04045467
INFO:root:[20,   200] training loss: 0.04855208
INFO:root:[20,   250] training loss: 0.02641719
INFO:root:[20,   300] training loss: 0.04198509
INFO:root:[20,   350] training loss: 0.03947734
INFO:root:[20,   400] training loss: 0.03123502
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.01390833
INFO:root:[21,   100] training loss: 0.01877441
INFO:root:[21,   150] training loss: 0.04067575
INFO:root:[21,   200] training loss: 0.04815071
INFO:root:[21,   250] training loss: 0.02618894
INFO:root:[21,   300] training loss: 0.04161232
INFO:root:[21,   350] training loss: 0.03967369
INFO:root:[21,   400] training loss: 0.03081643
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.01401085
INFO:root:[22,   100] training loss: 0.01893137
INFO:root:[22,   150] training loss: 0.04045486
INFO:root:[22,   200] training loss: 0.04836608
INFO:root:[22,   250] training loss: 0.02574643
INFO:root:[22,   300] training loss: 0.04186605
INFO:root:[22,   350] training loss: 0.03875961
INFO:root:[22,   400] training loss: 0.03043574
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.01388513
INFO:root:[23,   100] training loss: 0.01895508
INFO:root:[23,   150] training loss: 0.04059875
INFO:root:[23,   200] training loss: 0.04833772
INFO:root:[23,   250] training loss: 0.02713209
INFO:root:[23,   300] training loss: 0.04163745
INFO:root:[23,   350] training loss: 0.03892887
INFO:root:[23,   400] training loss: 0.03086959
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.01391104
INFO:root:[24,   100] training loss: 0.01885403
INFO:root:[24,   150] training loss: 0.04026468
INFO:root:[24,   200] training loss: 0.04803482
INFO:root:[24,   250] training loss: 0.02682279
INFO:root:[24,   300] training loss: 0.04166679
INFO:root:[24,   350] training loss: 0.03896156
INFO:root:[24,   400] training loss: 0.03044984
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.01396397
INFO:root:[25,   100] training loss: 0.01873015
INFO:root:[25,   150] training loss: 0.04037449
INFO:root:[25,   200] training loss: 0.04811532
INFO:root:[25,   250] training loss: 0.02624240
INFO:root:[25,   300] training loss: 0.04179935
INFO:root:[25,   350] training loss: 0.03870486
INFO:root:[25,   400] training loss: 0.03053098
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.01405060
INFO:root:[26,   100] training loss: 0.01901126
INFO:root:[26,   150] training loss: 0.04035851
INFO:root:[26,   200] training loss: 0.04803839
INFO:root:[26,   250] training loss: 0.02614132
INFO:root:[26,   300] training loss: 0.04146748
INFO:root:[26,   350] training loss: 0.03893429
INFO:root:[26,   400] training loss: 0.03059150
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.01404537
INFO:root:[27,   100] training loss: 0.01901787
INFO:root:[27,   150] training loss: 0.04052734
INFO:root:[27,   200] training loss: 0.04815786
INFO:root:[27,   250] training loss: 0.02593640
INFO:root:[27,   300] training loss: 0.04140083
INFO:root:[27,   350] training loss: 0.03887517
INFO:root:[27,   400] training loss: 0.03087823
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.01383494
INFO:root:[28,   100] training loss: 0.01869091
INFO:root:[28,   150] training loss: 0.04043398
INFO:root:[28,   200] training loss: 0.04779793
INFO:root:[28,   250] training loss: 0.02616984
INFO:root:[28,   300] training loss: 0.04171452
INFO:root:[28,   350] training loss: 0.03919430
INFO:root:[28,   400] training loss: 0.03047190
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.01398509
INFO:root:[29,   100] training loss: 0.01887810
INFO:root:[29,   150] training loss: 0.04012277
INFO:root:[29,   200] training loss: 0.04809381
INFO:root:[29,   250] training loss: 0.02597686
INFO:root:[29,   300] training loss: 0.04145159
INFO:root:[29,   350] training loss: 0.03889569
INFO:root:[29,   400] training loss: 0.03081593
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.01401468
INFO:root:[30,   100] training loss: 0.01900025
INFO:root:[30,   150] training loss: 0.04028538
INFO:root:[30,   200] training loss: 0.04806181
INFO:root:[30,   250] training loss: 0.02566742
INFO:root:[30,   300] training loss: 0.04154225
INFO:root:[30,   350] training loss: 0.03906892
INFO:root:[30,   400] training loss: 0.03040035
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.01395363
INFO:root:[31,   100] training loss: 0.01879487
INFO:root:[31,   150] training loss: 0.04058815
INFO:root:[31,   200] training loss: 0.04795160
INFO:root:[31,   250] training loss: 0.02624012
INFO:root:[31,   300] training loss: 0.04119081
INFO:root:[31,   350] training loss: 0.03887132
INFO:root:[31,   400] training loss: 0.03044460
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.01390913
INFO:root:[32,   100] training loss: 0.01887135
INFO:root:[32,   150] training loss: 0.04014560
INFO:root:[32,   200] training loss: 0.04781179
INFO:root:[32,   250] training loss: 0.02652809
INFO:root:[32,   300] training loss: 0.04129464
INFO:root:[32,   350] training loss: 0.03861316
INFO:root:[32,   400] training loss: 0.03079444
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.01394281
INFO:root:[33,   100] training loss: 0.01903307
INFO:root:[33,   150] training loss: 0.04037050
INFO:root:[33,   200] training loss: 0.04786720
INFO:root:[33,   250] training loss: 0.02631693
INFO:root:[33,   300] training loss: 0.04146817
INFO:root:[33,   350] training loss: 0.03863334
INFO:root:[33,   400] training loss: 0.03073323
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.01397835
INFO:root:[34,   100] training loss: 0.01901048
INFO:root:[34,   150] training loss: 0.03996975
INFO:root:[34,   200] training loss: 0.04790698
INFO:root:[34,   250] training loss: 0.02612726
INFO:root:[34,   300] training loss: 0.04146462
INFO:root:[34,   350] training loss: 0.03869439
INFO:root:[34,   400] training loss: 0.03025349
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.01400619
INFO:root:[35,   100] training loss: 0.01900562
INFO:root:[35,   150] training loss: 0.04015662
INFO:root:[35,   200] training loss: 0.04801831
INFO:root:[35,   250] training loss: 0.02618243
INFO:root:[35,   300] training loss: 0.04139781
INFO:root:[35,   350] training loss: 0.03852374
INFO:root:[35,   400] training loss: 0.03048260
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.01396371
INFO:root:[36,   100] training loss: 0.01896947
INFO:root:[36,   150] training loss: 0.04004417
INFO:root:[36,   200] training loss: 0.04816806
INFO:root:[36,   250] training loss: 0.02571387
INFO:root:[36,   300] training loss: 0.04159282
INFO:root:[36,   350] training loss: 0.03883721
INFO:root:[36,   400] training loss: 0.03067942
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.01395878
INFO:root:[37,   100] training loss: 0.01924413
INFO:root:[37,   150] training loss: 0.04000939
INFO:root:[37,   200] training loss: 0.04779771
INFO:root:[37,   250] training loss: 0.02615597
INFO:root:[37,   300] training loss: 0.04169063
INFO:root:[37,   350] training loss: 0.03903713
INFO:root:[37,   400] training loss: 0.03062234
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.01399790
INFO:root:[38,   100] training loss: 0.01892221
INFO:root:[38,   150] training loss: 0.04031841
INFO:root:[38,   200] training loss: 0.04823344
INFO:root:[38,   250] training loss: 0.02625459
INFO:root:[38,   300] training loss: 0.04160682
INFO:root:[38,   350] training loss: 0.03917321
INFO:root:[38,   400] training loss: 0.03046164
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.01395351
INFO:root:[39,   100] training loss: 0.01899496
INFO:root:[39,   150] training loss: 0.04028329
INFO:root:[39,   200] training loss: 0.04802439
INFO:root:[39,   250] training loss: 0.02578738
INFO:root:[39,   300] training loss: 0.04167553
INFO:root:[39,   350] training loss: 0.03876184
INFO:root:[39,   400] training loss: 0.03045056
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.01392248
INFO:root:[40,   100] training loss: 0.01873322
INFO:root:[40,   150] training loss: 0.04033465
INFO:root:[40,   200] training loss: 0.04805152
INFO:root:[40,   250] training loss: 0.02586192
INFO:root:[40,   300] training loss: 0.04142035
INFO:root:[40,   350] training loss: 0.03875106
INFO:root:[40,   400] training loss: 0.03055082
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.01393554
INFO:root:[41,   100] training loss: 0.01866865
INFO:root:[41,   150] training loss: 0.04034802
INFO:root:[41,   200] training loss: 0.04803689
INFO:root:[41,   250] training loss: 0.02617202
INFO:root:[41,   300] training loss: 0.04155065
INFO:root:[41,   350] training loss: 0.03924227
INFO:root:[41,   400] training loss: 0.03076085
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.01396888
INFO:root:[42,   100] training loss: 0.01893816
INFO:root:[42,   150] training loss: 0.04018869
INFO:root:[42,   200] training loss: 0.04826166
INFO:root:[42,   250] training loss: 0.02582983
INFO:root:[42,   300] training loss: 0.04164195
INFO:root:[42,   350] training loss: 0.03883123
INFO:root:[42,   400] training loss: 0.03082622
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.01394371
INFO:root:[43,   100] training loss: 0.01902922
INFO:root:[43,   150] training loss: 0.03997858
INFO:root:[43,   200] training loss: 0.04808243
INFO:root:[43,   250] training loss: 0.02624866
INFO:root:[43,   300] training loss: 0.04167184
INFO:root:[43,   350] training loss: 0.03866042
INFO:root:[43,   400] training loss: 0.03059603
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.01389880
INFO:root:[44,   100] training loss: 0.01881467
INFO:root:[44,   150] training loss: 0.04046189
INFO:root:[44,   200] training loss: 0.04799617
INFO:root:[44,   250] training loss: 0.02575499
INFO:root:[44,   300] training loss: 0.04134315
INFO:root:[44,   350] training loss: 0.03929529
INFO:root:[44,   400] training loss: 0.03022381
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.01398764
INFO:root:[45,   100] training loss: 0.01877650
INFO:root:[45,   150] training loss: 0.04049022
INFO:root:[45,   200] training loss: 0.04822904
INFO:root:[45,   250] training loss: 0.02574895
INFO:root:[45,   300] training loss: 0.04131765
INFO:root:[45,   350] training loss: 0.03896869
INFO:root:[45,   400] training loss: 0.03097299
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.01391338
INFO:root:[46,   100] training loss: 0.01873559
INFO:root:[46,   150] training loss: 0.04044546
INFO:root:[46,   200] training loss: 0.04788318
INFO:root:[46,   250] training loss: 0.02629884
INFO:root:[46,   300] training loss: 0.04146882
INFO:root:[46,   350] training loss: 0.03901118
INFO:root:[46,   400] training loss: 0.03037674
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.01393086
INFO:root:[47,   100] training loss: 0.01904570
INFO:root:[47,   150] training loss: 0.04076342
INFO:root:[47,   200] training loss: 0.04816632
INFO:root:[47,   250] training loss: 0.02637876
INFO:root:[47,   300] training loss: 0.04141303
INFO:root:[47,   350] training loss: 0.03858446
INFO:root:[47,   400] training loss: 0.03043949
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.01394731
INFO:root:[48,   100] training loss: 0.01874402
INFO:root:[48,   150] training loss: 0.04041331
INFO:root:[48,   200] training loss: 0.04778394
INFO:root:[48,   250] training loss: 0.02627812
INFO:root:[48,   300] training loss: 0.04158750
INFO:root:[48,   350] training loss: 0.03876821
INFO:root:[48,   400] training loss: 0.03085715
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.01394034
INFO:root:[49,   100] training loss: 0.01896442
INFO:root:[49,   150] training loss: 0.04030599
INFO:root:[49,   200] training loss: 0.04809730
INFO:root:[49,   250] training loss: 0.02607396
INFO:root:[49,   300] training loss: 0.04119398
INFO:root:[49,   350] training loss: 0.03879736
INFO:root:[49,   400] training loss: 0.03042735
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.01389485
INFO:root:[50,   100] training loss: 0.01917793
INFO:root:[50,   150] training loss: 0.04059273
INFO:root:[50,   200] training loss: 0.04821332
INFO:root:[50,   250] training loss: 0.02588900
INFO:root:[50,   300] training loss: 0.04171026
INFO:root:[50,   350] training loss: 0.03830499
INFO:root:[50,   400] training loss: 0.03063150
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 76 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.1441    0.1241    0.1333       266
           CD4+ T     0.5936    0.7820    0.6749       876
           CD8+ T     0.2910    0.1108    0.1605       352
 CD15+ neutrophil     0.9720    0.9649    0.9684      3671
   CD14+ monocyte     0.4764    0.8016    0.5976       252
          CD19+ B     0.1471    0.0278    0.0467       180
         CD56+ NK     0.2047    0.3333    0.2536       132
              NKT     0.0556    0.0091    0.0156       220
       eosinophil     0.6451    0.8111    0.7186       307

         accuracy                         0.7674      6256
        macro avg     0.3922    0.4405    0.3966      6256
     weighted avg     0.7373    0.7674    0.7440      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.133333  0.674877  0.160494           0.968421         0.597633  0.046729   0.253602  0.015625     0.718615
INFO:root:statistics used: {'mean': tensor([0.0149, 0.1691]), 'std': tensor([0.0203, 0.0627])}
INFO:root:train dataset: 131886, test dataset: 6256
INFO:root:used only channels: [5, 8]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.03568974
INFO:root:[1,   100] training loss: 0.03297606
INFO:root:[1,   150] training loss: 0.04986215
INFO:root:[1,   200] training loss: 0.05308748
INFO:root:[1,   250] training loss: 0.06404615
INFO:root:[1,   300] training loss: 0.05853012
INFO:root:[1,   350] training loss: 0.05916049
INFO:root:[1,   400] training loss: 0.07052759
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.04012452
INFO:root:[2,   100] training loss: 0.02940261
INFO:root:[2,   150] training loss: 0.04837142
INFO:root:[2,   200] training loss: 0.05286116
INFO:root:[2,   250] training loss: 0.05958021
INFO:root:[2,   300] training loss: 0.05581567
INFO:root:[2,   350] training loss: 0.05590112
INFO:root:[2,   400] training loss: 0.06554987
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.02441760
INFO:root:[3,   100] training loss: 0.02681630
INFO:root:[3,   150] training loss: 0.04759173
INFO:root:[3,   200] training loss: 0.05214346
INFO:root:[3,   250] training loss: 0.05645368
INFO:root:[3,   300] training loss: 0.05456153
INFO:root:[3,   350] training loss: 0.05448717
INFO:root:[3,   400] training loss: 0.05875523
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.01983514
INFO:root:[4,   100] training loss: 0.02649347
INFO:root:[4,   150] training loss: 0.04803111
INFO:root:[4,   200] training loss: 0.05192660
INFO:root:[4,   250] training loss: 0.05385535
INFO:root:[4,   300] training loss: 0.05265285
INFO:root:[4,   350] training loss: 0.05340128
INFO:root:[4,   400] training loss: 0.05509051
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.01824217
INFO:root:[5,   100] training loss: 0.02636024
INFO:root:[5,   150] training loss: 0.04753896
INFO:root:[5,   200] training loss: 0.05105492
INFO:root:[5,   250] training loss: 0.04917339
INFO:root:[5,   300] training loss: 0.04971752
INFO:root:[5,   350] training loss: 0.04962586
INFO:root:[5,   400] training loss: 0.05075172
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.01610517
INFO:root:[6,   100] training loss: 0.02672402
INFO:root:[6,   150] training loss: 0.04676788
INFO:root:[6,   200] training loss: 0.04999829
INFO:root:[6,   250] training loss: 0.04585662
INFO:root:[6,   300] training loss: 0.04768306
INFO:root:[6,   350] training loss: 0.04995484
INFO:root:[6,   400] training loss: 0.04702636
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.01533484
INFO:root:[7,   100] training loss: 0.02727705
INFO:root:[7,   150] training loss: 0.04546725
INFO:root:[7,   200] training loss: 0.05030571
INFO:root:[7,   250] training loss: 0.04050127
INFO:root:[7,   300] training loss: 0.04530108
INFO:root:[7,   350] training loss: 0.04915487
INFO:root:[7,   400] training loss: 0.04822148
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.01635940
INFO:root:[8,   100] training loss: 0.02890501
INFO:root:[8,   150] training loss: 0.05896926
INFO:root:[8,   200] training loss: 0.05678780
INFO:root:[8,   250] training loss: 0.04324117
INFO:root:[8,   300] training loss: 0.04879865
INFO:root:[8,   350] training loss: 0.04465570
INFO:root:[8,   400] training loss: 0.03878014
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.01465389
INFO:root:[9,   100] training loss: 0.02729750
INFO:root:[9,   150] training loss: 0.05620613
INFO:root:[9,   200] training loss: 0.05500138
INFO:root:[9,   250] training loss: 0.03546009
INFO:root:[9,   300] training loss: 0.04756850
INFO:root:[9,   350] training loss: 0.04417512
INFO:root:[9,   400] training loss: 0.03968597
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.01426909
INFO:root:[10,   100] training loss: 0.02651664
INFO:root:[10,   150] training loss: 0.05452924
INFO:root:[10,   200] training loss: 0.05369320
INFO:root:[10,   250] training loss: 0.03222762
INFO:root:[10,   300] training loss: 0.04667458
INFO:root:[10,   350] training loss: 0.04407507
INFO:root:[10,   400] training loss: 0.04042280
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.01414323
INFO:root:[11,   100] training loss: 0.02563775
INFO:root:[11,   150] training loss: 0.05288343
INFO:root:[11,   200] training loss: 0.05254957
INFO:root:[11,   250] training loss: 0.02992882
INFO:root:[11,   300] training loss: 0.04575841
INFO:root:[11,   350] training loss: 0.04433018
INFO:root:[11,   400] training loss: 0.04108835
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.01405810
INFO:root:[12,   100] training loss: 0.02501658
INFO:root:[12,   150] training loss: 0.05140816
INFO:root:[12,   200] training loss: 0.05177494
INFO:root:[12,   250] training loss: 0.02869021
INFO:root:[12,   300] training loss: 0.04537841
INFO:root:[12,   350] training loss: 0.04429126
INFO:root:[12,   400] training loss: 0.04141535
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.01401641
INFO:root:[13,   100] training loss: 0.02428618
INFO:root:[13,   150] training loss: 0.05004659
INFO:root:[13,   200] training loss: 0.05094994
INFO:root:[13,   250] training loss: 0.02734680
INFO:root:[13,   300] training loss: 0.04435320
INFO:root:[13,   350] training loss: 0.04413225
INFO:root:[13,   400] training loss: 0.04198339
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.01393516
INFO:root:[14,   100] training loss: 0.02357614
INFO:root:[14,   150] training loss: 0.04854112
INFO:root:[14,   200] training loss: 0.05033360
INFO:root:[14,   250] training loss: 0.02645895
INFO:root:[14,   300] training loss: 0.04401100
INFO:root:[14,   350] training loss: 0.04379808
INFO:root:[14,   400] training loss: 0.04188873
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.01391581
INFO:root:[15,   100] training loss: 0.02322030
INFO:root:[15,   150] training loss: 0.04926227
INFO:root:[15,   200] training loss: 0.05084581
INFO:root:[15,   250] training loss: 0.02574826
INFO:root:[15,   300] training loss: 0.04345329
INFO:root:[15,   350] training loss: 0.04207384
INFO:root:[15,   400] training loss: 0.03938936
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.01391370
INFO:root:[16,   100] training loss: 0.02305393
INFO:root:[16,   150] training loss: 0.04872860
INFO:root:[16,   200] training loss: 0.05060610
INFO:root:[16,   250] training loss: 0.02536483
INFO:root:[16,   300] training loss: 0.04341186
INFO:root:[16,   350] training loss: 0.04212557
INFO:root:[16,   400] training loss: 0.03951778
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.01383108
INFO:root:[17,   100] training loss: 0.02267970
INFO:root:[17,   150] training loss: 0.04851373
INFO:root:[17,   200] training loss: 0.05063756
INFO:root:[17,   250] training loss: 0.02504510
INFO:root:[17,   300] training loss: 0.04316263
INFO:root:[17,   350] training loss: 0.04264501
INFO:root:[17,   400] training loss: 0.03978114
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.01383297
INFO:root:[18,   100] training loss: 0.02254241
INFO:root:[18,   150] training loss: 0.04834885
INFO:root:[18,   200] training loss: 0.05040631
INFO:root:[18,   250] training loss: 0.02550026
INFO:root:[18,   300] training loss: 0.04308834
INFO:root:[18,   350] training loss: 0.04203262
INFO:root:[18,   400] training loss: 0.03976044
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.01381255
INFO:root:[19,   100] training loss: 0.02265854
INFO:root:[19,   150] training loss: 0.04796696
INFO:root:[19,   200] training loss: 0.05012835
INFO:root:[19,   250] training loss: 0.02487034
INFO:root:[19,   300] training loss: 0.04304179
INFO:root:[19,   350] training loss: 0.04214599
INFO:root:[19,   400] training loss: 0.03993853
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.01379916
INFO:root:[20,   100] training loss: 0.02252449
INFO:root:[20,   150] training loss: 0.04787157
INFO:root:[20,   200] training loss: 0.05015685
INFO:root:[20,   250] training loss: 0.02501809
INFO:root:[20,   300] training loss: 0.04292559
INFO:root:[20,   350] training loss: 0.04179665
INFO:root:[20,   400] training loss: 0.03988897
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.01376277
INFO:root:[21,   100] training loss: 0.02251130
INFO:root:[21,   150] training loss: 0.04766460
INFO:root:[21,   200] training loss: 0.04999767
INFO:root:[21,   250] training loss: 0.02487657
INFO:root:[21,   300] training loss: 0.04286566
INFO:root:[21,   350] training loss: 0.04157202
INFO:root:[21,   400] training loss: 0.04007154
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.01379711
INFO:root:[22,   100] training loss: 0.02223867
INFO:root:[22,   150] training loss: 0.04770350
INFO:root:[22,   200] training loss: 0.04988387
INFO:root:[22,   250] training loss: 0.02466045
INFO:root:[22,   300] training loss: 0.04290987
INFO:root:[22,   350] training loss: 0.04165414
INFO:root:[22,   400] training loss: 0.03974827
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.01378954
INFO:root:[23,   100] training loss: 0.02228983
INFO:root:[23,   150] training loss: 0.04749877
INFO:root:[23,   200] training loss: 0.04987131
INFO:root:[23,   250] training loss: 0.02464045
INFO:root:[23,   300] training loss: 0.04288010
INFO:root:[23,   350] training loss: 0.04159037
INFO:root:[23,   400] training loss: 0.03982873
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.01378171
INFO:root:[24,   100] training loss: 0.02227248
INFO:root:[24,   150] training loss: 0.04743975
INFO:root:[24,   200] training loss: 0.05001747
INFO:root:[24,   250] training loss: 0.02508649
INFO:root:[24,   300] training loss: 0.04279183
INFO:root:[24,   350] training loss: 0.04163477
INFO:root:[24,   400] training loss: 0.04011602
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.01377700
INFO:root:[25,   100] training loss: 0.02211417
INFO:root:[25,   150] training loss: 0.04753542
INFO:root:[25,   200] training loss: 0.04988479
INFO:root:[25,   250] training loss: 0.02507604
INFO:root:[25,   300] training loss: 0.04297360
INFO:root:[25,   350] training loss: 0.04206313
INFO:root:[25,   400] training loss: 0.03983194
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.01377468
INFO:root:[26,   100] training loss: 0.02219225
INFO:root:[26,   150] training loss: 0.04761770
INFO:root:[26,   200] training loss: 0.04989814
INFO:root:[26,   250] training loss: 0.02462107
INFO:root:[26,   300] training loss: 0.04317947
INFO:root:[26,   350] training loss: 0.04178649
INFO:root:[26,   400] training loss: 0.03992470
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.01380914
INFO:root:[27,   100] training loss: 0.02215587
INFO:root:[27,   150] training loss: 0.04729025
INFO:root:[27,   200] training loss: 0.04983088
INFO:root:[27,   250] training loss: 0.02479569
INFO:root:[27,   300] training loss: 0.04282541
INFO:root:[27,   350] training loss: 0.04140424
INFO:root:[27,   400] training loss: 0.03993800
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.01372064
INFO:root:[28,   100] training loss: 0.02222202
INFO:root:[28,   150] training loss: 0.04748212
INFO:root:[28,   200] training loss: 0.04977193
INFO:root:[28,   250] training loss: 0.02444188
INFO:root:[28,   300] training loss: 0.04275059
INFO:root:[28,   350] training loss: 0.04208249
INFO:root:[28,   400] training loss: 0.04006836
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.01379117
INFO:root:[29,   100] training loss: 0.02219591
INFO:root:[29,   150] training loss: 0.04743099
INFO:root:[29,   200] training loss: 0.04977033
INFO:root:[29,   250] training loss: 0.02493736
INFO:root:[29,   300] training loss: 0.04279524
INFO:root:[29,   350] training loss: 0.04176227
INFO:root:[29,   400] training loss: 0.03973866
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.01376513
INFO:root:[30,   100] training loss: 0.02216856
INFO:root:[30,   150] training loss: 0.04759591
INFO:root:[30,   200] training loss: 0.04980761
INFO:root:[30,   250] training loss: 0.02483172
INFO:root:[30,   300] training loss: 0.04282977
INFO:root:[30,   350] training loss: 0.04161349
INFO:root:[30,   400] training loss: 0.03989183
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.01376273
INFO:root:[31,   100] training loss: 0.02216612
INFO:root:[31,   150] training loss: 0.04727960
INFO:root:[31,   200] training loss: 0.04983108
INFO:root:[31,   250] training loss: 0.02455008
INFO:root:[31,   300] training loss: 0.04286368
INFO:root:[31,   350] training loss: 0.04166722
INFO:root:[31,   400] training loss: 0.03984450
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.01381637
INFO:root:[32,   100] training loss: 0.02214459
INFO:root:[32,   150] training loss: 0.04730819
INFO:root:[32,   200] training loss: 0.04988235
INFO:root:[32,   250] training loss: 0.02514061
INFO:root:[32,   300] training loss: 0.04281115
INFO:root:[32,   350] training loss: 0.04141880
INFO:root:[32,   400] training loss: 0.03989642
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.01378509
INFO:root:[33,   100] training loss: 0.02209836
INFO:root:[33,   150] training loss: 0.04761949
INFO:root:[33,   200] training loss: 0.04981137
INFO:root:[33,   250] training loss: 0.02462382
INFO:root:[33,   300] training loss: 0.04281499
INFO:root:[33,   350] training loss: 0.04134059
INFO:root:[33,   400] training loss: 0.03975032
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.01373848
INFO:root:[34,   100] training loss: 0.02212982
INFO:root:[34,   150] training loss: 0.04727310
INFO:root:[34,   200] training loss: 0.04989003
INFO:root:[34,   250] training loss: 0.02467662
INFO:root:[34,   300] training loss: 0.04276649
INFO:root:[34,   350] training loss: 0.04188775
INFO:root:[34,   400] training loss: 0.03986550
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.01377251
INFO:root:[35,   100] training loss: 0.02222905
INFO:root:[35,   150] training loss: 0.04757093
INFO:root:[35,   200] training loss: 0.04986485
INFO:root:[35,   250] training loss: 0.02469646
INFO:root:[35,   300] training loss: 0.04309041
INFO:root:[35,   350] training loss: 0.04175998
INFO:root:[35,   400] training loss: 0.03977054
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.01380790
INFO:root:[36,   100] training loss: 0.02204447
INFO:root:[36,   150] training loss: 0.04741196
INFO:root:[36,   200] training loss: 0.04986900
INFO:root:[36,   250] training loss: 0.02486618
INFO:root:[36,   300] training loss: 0.04269468
INFO:root:[36,   350] training loss: 0.04192049
INFO:root:[36,   400] training loss: 0.03986056
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.01378082
INFO:root:[37,   100] training loss: 0.02218146
INFO:root:[37,   150] training loss: 0.04747744
INFO:root:[37,   200] training loss: 0.04982439
INFO:root:[37,   250] training loss: 0.02489800
INFO:root:[37,   300] training loss: 0.04274768
INFO:root:[37,   350] training loss: 0.04147922
INFO:root:[37,   400] training loss: 0.04002452
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.01380863
INFO:root:[38,   100] training loss: 0.02215330
INFO:root:[38,   150] training loss: 0.04734260
INFO:root:[38,   200] training loss: 0.04979744
INFO:root:[38,   250] training loss: 0.02518120
INFO:root:[38,   300] training loss: 0.04281088
INFO:root:[38,   350] training loss: 0.04147083
INFO:root:[38,   400] training loss: 0.03983459
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.01376649
INFO:root:[39,   100] training loss: 0.02215010
INFO:root:[39,   150] training loss: 0.04750795
INFO:root:[39,   200] training loss: 0.04969253
INFO:root:[39,   250] training loss: 0.02498842
INFO:root:[39,   300] training loss: 0.04278802
INFO:root:[39,   350] training loss: 0.04117583
INFO:root:[39,   400] training loss: 0.03998885
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.01379158
INFO:root:[40,   100] training loss: 0.02206916
INFO:root:[40,   150] training loss: 0.04738889
INFO:root:[40,   200] training loss: 0.04981999
INFO:root:[40,   250] training loss: 0.02496911
INFO:root:[40,   300] training loss: 0.04273273
INFO:root:[40,   350] training loss: 0.04158336
INFO:root:[40,   400] training loss: 0.03978221
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.01374415
INFO:root:[41,   100] training loss: 0.02211207
INFO:root:[41,   150] training loss: 0.04750088
INFO:root:[41,   200] training loss: 0.04981203
INFO:root:[41,   250] training loss: 0.02448320
INFO:root:[41,   300] training loss: 0.04300567
INFO:root:[41,   350] training loss: 0.04178478
INFO:root:[41,   400] training loss: 0.03983145
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.01382237
INFO:root:[42,   100] training loss: 0.02208534
INFO:root:[42,   150] training loss: 0.04750381
INFO:root:[42,   200] training loss: 0.04981929
INFO:root:[42,   250] training loss: 0.02498379
INFO:root:[42,   300] training loss: 0.04279677
INFO:root:[42,   350] training loss: 0.04169796
INFO:root:[42,   400] training loss: 0.03987297
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.01376788
INFO:root:[43,   100] training loss: 0.02226837
INFO:root:[43,   150] training loss: 0.04741056
INFO:root:[43,   200] training loss: 0.04972189
INFO:root:[43,   250] training loss: 0.02477043
INFO:root:[43,   300] training loss: 0.04276645
INFO:root:[43,   350] training loss: 0.04172922
INFO:root:[43,   400] training loss: 0.03992182
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.01377570
INFO:root:[44,   100] training loss: 0.02212366
INFO:root:[44,   150] training loss: 0.04750084
INFO:root:[44,   200] training loss: 0.04988849
INFO:root:[44,   250] training loss: 0.02502103
INFO:root:[44,   300] training loss: 0.04267969
INFO:root:[44,   350] training loss: 0.04138656
INFO:root:[44,   400] training loss: 0.03981100
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.01377621
INFO:root:[45,   100] training loss: 0.02208697
INFO:root:[45,   150] training loss: 0.04742259
INFO:root:[45,   200] training loss: 0.04977840
INFO:root:[45,   250] training loss: 0.02489579
INFO:root:[45,   300] training loss: 0.04266479
INFO:root:[45,   350] training loss: 0.04206680
INFO:root:[45,   400] training loss: 0.03981376
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.01380065
INFO:root:[46,   100] training loss: 0.02208246
INFO:root:[46,   150] training loss: 0.04746050
INFO:root:[46,   200] training loss: 0.04975874
INFO:root:[46,   250] training loss: 0.02485423
INFO:root:[46,   300] training loss: 0.04289262
INFO:root:[46,   350] training loss: 0.04160012
INFO:root:[46,   400] training loss: 0.03986638
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.01376542
INFO:root:[47,   100] training loss: 0.02224869
INFO:root:[47,   150] training loss: 0.04751832
INFO:root:[47,   200] training loss: 0.04977757
INFO:root:[47,   250] training loss: 0.02466690
INFO:root:[47,   300] training loss: 0.04263036
INFO:root:[47,   350] training loss: 0.04186285
INFO:root:[47,   400] training loss: 0.03979989
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.01375822
INFO:root:[48,   100] training loss: 0.02203010
INFO:root:[48,   150] training loss: 0.04755106
INFO:root:[48,   200] training loss: 0.04984854
INFO:root:[48,   250] training loss: 0.02470448
INFO:root:[48,   300] training loss: 0.04286017
INFO:root:[48,   350] training loss: 0.04150675
INFO:root:[48,   400] training loss: 0.03986690
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.01380846
INFO:root:[49,   100] training loss: 0.02212632
INFO:root:[49,   150] training loss: 0.04728715
INFO:root:[49,   200] training loss: 0.04985859
INFO:root:[49,   250] training loss: 0.02466839
INFO:root:[49,   300] training loss: 0.04273941
INFO:root:[49,   350] training loss: 0.04178520
INFO:root:[49,   400] training loss: 0.03971372
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.01381240
INFO:root:[50,   100] training loss: 0.02216456
INFO:root:[50,   150] training loss: 0.04731903
INFO:root:[50,   200] training loss: 0.04977636
INFO:root:[50,   250] training loss: 0.02477097
INFO:root:[50,   300] training loss: 0.04278294
INFO:root:[50,   350] training loss: 0.04122270
INFO:root:[50,   400] training loss: 0.03981609
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 80 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.2222    0.2805    0.2480       221
           CD4+ T     0.5677    0.8684    0.6866       874
           CD8+ T     0.3353    0.1506    0.2079       385
 CD15+ neutrophil     0.9660    0.9910    0.9784      3671
   CD14+ monocyte     0.6624    0.7574    0.7067       272
          CD19+ B     0.5000    0.0058    0.0115       172
         CD56+ NK     0.2459    0.1095    0.1515       137
              NKT     0.3000    0.0303    0.0550       198
       eosinophil     0.8697    0.8190    0.8436       326

         accuracy                         0.8012      6256
        macro avg     0.5188    0.4458    0.4321      6256
     weighted avg     0.7774    0.8012    0.7716      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0     0.248  0.686567  0.207885           0.978351          0.70669  0.011494   0.151515  0.055046     0.843602
INFO:root:statistics used: {'mean': tensor([0.0149, 0.1692]), 'std': tensor([0.0204, 0.0625])}
INFO:root:train dataset: 132012, test dataset: 6256
INFO:root:used only channels: [5, 8]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.04069302
INFO:root:[1,   100] training loss: 0.03303332
INFO:root:[1,   150] training loss: 0.04169783
INFO:root:[1,   200] training loss: 0.04869857
INFO:root:[1,   250] training loss: 0.05569288
INFO:root:[1,   300] training loss: 0.06815606
INFO:root:[1,   350] training loss: 0.06641371
INFO:root:[1,   400] training loss: 0.07289546
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.04773465
INFO:root:[2,   100] training loss: 0.03533721
INFO:root:[2,   150] training loss: 0.04631969
INFO:root:[2,   200] training loss: 0.04863878
INFO:root:[2,   250] training loss: 0.05384549
INFO:root:[2,   300] training loss: 0.06328549
INFO:root:[2,   350] training loss: 0.06167696
INFO:root:[2,   400] training loss: 0.06573130
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.02659675
INFO:root:[3,   100] training loss: 0.02853458
INFO:root:[3,   150] training loss: 0.04506339
INFO:root:[3,   200] training loss: 0.04898173
INFO:root:[3,   250] training loss: 0.05088135
INFO:root:[3,   300] training loss: 0.05272060
INFO:root:[3,   350] training loss: 0.05438349
INFO:root:[3,   400] training loss: 0.05489451
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.02525052
INFO:root:[4,   100] training loss: 0.02772944
INFO:root:[4,   150] training loss: 0.04265589
INFO:root:[4,   200] training loss: 0.04710307
INFO:root:[4,   250] training loss: 0.04963428
INFO:root:[4,   300] training loss: 0.05264530
INFO:root:[4,   350] training loss: 0.05148942
INFO:root:[4,   400] training loss: 0.05089995
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.02336711
INFO:root:[5,   100] training loss: 0.02788238
INFO:root:[5,   150] training loss: 0.04481637
INFO:root:[5,   200] training loss: 0.04737894
INFO:root:[5,   250] training loss: 0.04911024
INFO:root:[5,   300] training loss: 0.05138328
INFO:root:[5,   350] training loss: 0.05156815
INFO:root:[5,   400] training loss: 0.04752432
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.01958307
INFO:root:[6,   100] training loss: 0.02808068
INFO:root:[6,   150] training loss: 0.04460859
INFO:root:[6,   200] training loss: 0.04715902
INFO:root:[6,   250] training loss: 0.04839461
INFO:root:[6,   300] training loss: 0.04998268
INFO:root:[6,   350] training loss: 0.04899972
INFO:root:[6,   400] training loss: 0.04046237
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.01774358
INFO:root:[7,   100] training loss: 0.02734202
INFO:root:[7,   150] training loss: 0.04142484
INFO:root:[7,   200] training loss: 0.04531117
INFO:root:[7,   250] training loss: 0.04425965
INFO:root:[7,   300] training loss: 0.04839052
INFO:root:[7,   350] training loss: 0.04767070
INFO:root:[7,   400] training loss: 0.04273042
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.02235763
INFO:root:[8,   100] training loss: 0.03543448
INFO:root:[8,   150] training loss: 0.06555542
INFO:root:[8,   200] training loss: 0.06021458
INFO:root:[8,   250] training loss: 0.05565017
INFO:root:[8,   300] training loss: 0.05545532
INFO:root:[8,   350] training loss: 0.05137524
INFO:root:[8,   400] training loss: 0.03543277
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.01683288
INFO:root:[9,   100] training loss: 0.02905027
INFO:root:[9,   150] training loss: 0.05628162
INFO:root:[9,   200] training loss: 0.05448304
INFO:root:[9,   250] training loss: 0.04998781
INFO:root:[9,   300] training loss: 0.05191884
INFO:root:[9,   350] training loss: 0.05023348
INFO:root:[9,   400] training loss: 0.03798284
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.01604480
INFO:root:[10,   100] training loss: 0.02801872
INFO:root:[10,   150] training loss: 0.05433741
INFO:root:[10,   200] training loss: 0.05302866
INFO:root:[10,   250] training loss: 0.04685857
INFO:root:[10,   300] training loss: 0.05028170
INFO:root:[10,   350] training loss: 0.04964421
INFO:root:[10,   400] training loss: 0.03964750
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.01568913
INFO:root:[11,   100] training loss: 0.02707609
INFO:root:[11,   150] training loss: 0.05277755
INFO:root:[11,   200] training loss: 0.05194257
INFO:root:[11,   250] training loss: 0.04426542
INFO:root:[11,   300] training loss: 0.04919107
INFO:root:[11,   350] training loss: 0.04890812
INFO:root:[11,   400] training loss: 0.04061796
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.01554432
INFO:root:[12,   100] training loss: 0.02627026
INFO:root:[12,   150] training loss: 0.05116386
INFO:root:[12,   200] training loss: 0.05086294
INFO:root:[12,   250] training loss: 0.04196380
INFO:root:[12,   300] training loss: 0.04802972
INFO:root:[12,   350] training loss: 0.04854545
INFO:root:[12,   400] training loss: 0.04110261
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.01543039
INFO:root:[13,   100] training loss: 0.02542205
INFO:root:[13,   150] training loss: 0.04963587
INFO:root:[13,   200] training loss: 0.05031983
INFO:root:[13,   250] training loss: 0.04005267
INFO:root:[13,   300] training loss: 0.04684665
INFO:root:[13,   350] training loss: 0.04846520
INFO:root:[13,   400] training loss: 0.04108667
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.01527864
INFO:root:[14,   100] training loss: 0.02477710
INFO:root:[14,   150] training loss: 0.04855507
INFO:root:[14,   200] training loss: 0.04967906
INFO:root:[14,   250] training loss: 0.03783911
INFO:root:[14,   300] training loss: 0.04593237
INFO:root:[14,   350] training loss: 0.04823042
INFO:root:[14,   400] training loss: 0.04166935
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.01529686
INFO:root:[15,   100] training loss: 0.02430336
INFO:root:[15,   150] training loss: 0.04961252
INFO:root:[15,   200] training loss: 0.05117735
INFO:root:[15,   250] training loss: 0.03746130
INFO:root:[15,   300] training loss: 0.04696211
INFO:root:[15,   350] training loss: 0.04647356
INFO:root:[15,   400] training loss: 0.03666784
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.01526001
INFO:root:[16,   100] training loss: 0.02406291
INFO:root:[16,   150] training loss: 0.04876317
INFO:root:[16,   200] training loss: 0.05092523
INFO:root:[16,   250] training loss: 0.03687585
INFO:root:[16,   300] training loss: 0.04652593
INFO:root:[16,   350] training loss: 0.04628571
INFO:root:[16,   400] training loss: 0.03728799
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.01513143
INFO:root:[17,   100] training loss: 0.02390153
INFO:root:[17,   150] training loss: 0.04861334
INFO:root:[17,   200] training loss: 0.05025518
INFO:root:[17,   250] training loss: 0.03646859
INFO:root:[17,   300] training loss: 0.04596998
INFO:root:[17,   350] training loss: 0.04637072
INFO:root:[17,   400] training loss: 0.03755584
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.01511764
INFO:root:[18,   100] training loss: 0.02363649
INFO:root:[18,   150] training loss: 0.04826521
INFO:root:[18,   200] training loss: 0.05000564
INFO:root:[18,   250] training loss: 0.03670448
INFO:root:[18,   300] training loss: 0.04579768
INFO:root:[18,   350] training loss: 0.04632096
INFO:root:[18,   400] training loss: 0.03817320
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.01518012
INFO:root:[19,   100] training loss: 0.02350238
INFO:root:[19,   150] training loss: 0.04789752
INFO:root:[19,   200] training loss: 0.04957896
INFO:root:[19,   250] training loss: 0.03587155
INFO:root:[19,   300] training loss: 0.04557629
INFO:root:[19,   350] training loss: 0.04634856
INFO:root:[19,   400] training loss: 0.03845757
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.01509421
INFO:root:[20,   100] training loss: 0.02341681
INFO:root:[20,   150] training loss: 0.04767779
INFO:root:[20,   200] training loss: 0.04933333
INFO:root:[20,   250] training loss: 0.03565819
INFO:root:[20,   300] training loss: 0.04491345
INFO:root:[20,   350] training loss: 0.04643094
INFO:root:[20,   400] training loss: 0.03876613
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.01511276
INFO:root:[21,   100] training loss: 0.02327884
INFO:root:[21,   150] training loss: 0.04742316
INFO:root:[21,   200] training loss: 0.04937650
INFO:root:[21,   250] training loss: 0.03570476
INFO:root:[21,   300] training loss: 0.04497387
INFO:root:[21,   350] training loss: 0.04649981
INFO:root:[21,   400] training loss: 0.03884093
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.01506356
INFO:root:[22,   100] training loss: 0.02327748
INFO:root:[22,   150] training loss: 0.04752889
INFO:root:[22,   200] training loss: 0.04934260
INFO:root:[22,   250] training loss: 0.03546944
INFO:root:[22,   300] training loss: 0.04487343
INFO:root:[22,   350] training loss: 0.04639966
INFO:root:[22,   400] training loss: 0.03860522
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.01499647
INFO:root:[23,   100] training loss: 0.02325492
INFO:root:[23,   150] training loss: 0.04755807
INFO:root:[23,   200] training loss: 0.04932060
INFO:root:[23,   250] training loss: 0.03515942
INFO:root:[23,   300] training loss: 0.04507100
INFO:root:[23,   350] training loss: 0.04618813
INFO:root:[23,   400] training loss: 0.03831452
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.01500991
INFO:root:[24,   100] training loss: 0.02308447
INFO:root:[24,   150] training loss: 0.04722435
INFO:root:[24,   200] training loss: 0.04944596
INFO:root:[24,   250] training loss: 0.03505535
INFO:root:[24,   300] training loss: 0.04509896
INFO:root:[24,   350] training loss: 0.04615896
INFO:root:[24,   400] training loss: 0.03854944
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.01506619
INFO:root:[25,   100] training loss: 0.02308409
INFO:root:[25,   150] training loss: 0.04745586
INFO:root:[25,   200] training loss: 0.04947920
INFO:root:[25,   250] training loss: 0.03573465
INFO:root:[25,   300] training loss: 0.04494162
INFO:root:[25,   350] training loss: 0.04607777
INFO:root:[25,   400] training loss: 0.03828675
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.01502182
INFO:root:[26,   100] training loss: 0.02313749
INFO:root:[26,   150] training loss: 0.04751692
INFO:root:[26,   200] training loss: 0.04937033
INFO:root:[26,   250] training loss: 0.03514063
INFO:root:[26,   300] training loss: 0.04471529
INFO:root:[26,   350] training loss: 0.04634019
INFO:root:[26,   400] training loss: 0.03839355
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.01503870
INFO:root:[27,   100] training loss: 0.02315544
INFO:root:[27,   150] training loss: 0.04730569
INFO:root:[27,   200] training loss: 0.04966574
INFO:root:[27,   250] training loss: 0.03526234
INFO:root:[27,   300] training loss: 0.04462474
INFO:root:[27,   350] training loss: 0.04631183
INFO:root:[27,   400] training loss: 0.03870947
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.01501671
INFO:root:[28,   100] training loss: 0.02306270
INFO:root:[28,   150] training loss: 0.04732202
INFO:root:[28,   200] training loss: 0.04915775
INFO:root:[28,   250] training loss: 0.03533891
INFO:root:[28,   300] training loss: 0.04505572
INFO:root:[28,   350] training loss: 0.04630271
INFO:root:[28,   400] training loss: 0.03851334
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.01502898
INFO:root:[29,   100] training loss: 0.02318126
INFO:root:[29,   150] training loss: 0.04708387
INFO:root:[29,   200] training loss: 0.04945456
INFO:root:[29,   250] training loss: 0.03489519
INFO:root:[29,   300] training loss: 0.04539321
INFO:root:[29,   350] training loss: 0.04634022
INFO:root:[29,   400] training loss: 0.03813705
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.01505186
INFO:root:[30,   100] training loss: 0.02306987
INFO:root:[30,   150] training loss: 0.04688699
INFO:root:[30,   200] training loss: 0.04948414
INFO:root:[30,   250] training loss: 0.03520709
INFO:root:[30,   300] training loss: 0.04481357
INFO:root:[30,   350] training loss: 0.04610273
INFO:root:[30,   400] training loss: 0.03848541
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.01506366
INFO:root:[31,   100] training loss: 0.02321268
INFO:root:[31,   150] training loss: 0.04715074
INFO:root:[31,   200] training loss: 0.04964195
INFO:root:[31,   250] training loss: 0.03500567
INFO:root:[31,   300] training loss: 0.04468619
INFO:root:[31,   350] training loss: 0.04617116
INFO:root:[31,   400] training loss: 0.03862038
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.01503843
INFO:root:[32,   100] training loss: 0.02322021
INFO:root:[32,   150] training loss: 0.04719703
INFO:root:[32,   200] training loss: 0.04917638
INFO:root:[32,   250] training loss: 0.03513270
INFO:root:[32,   300] training loss: 0.04508237
INFO:root:[32,   350] training loss: 0.04623283
INFO:root:[32,   400] training loss: 0.03852159
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.01510804
INFO:root:[33,   100] training loss: 0.02318343
INFO:root:[33,   150] training loss: 0.04739545
INFO:root:[33,   200] training loss: 0.04949999
INFO:root:[33,   250] training loss: 0.03543022
INFO:root:[33,   300] training loss: 0.04501999
INFO:root:[33,   350] training loss: 0.04617125
INFO:root:[33,   400] training loss: 0.03842539
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.01500668
INFO:root:[34,   100] training loss: 0.02304334
INFO:root:[34,   150] training loss: 0.04714723
INFO:root:[34,   200] training loss: 0.04949917
INFO:root:[34,   250] training loss: 0.03525921
INFO:root:[34,   300] training loss: 0.04493952
INFO:root:[34,   350] training loss: 0.04598809
INFO:root:[34,   400] training loss: 0.03863510
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.01502420
INFO:root:[35,   100] training loss: 0.02312022
INFO:root:[35,   150] training loss: 0.04723020
INFO:root:[35,   200] training loss: 0.04924436
INFO:root:[35,   250] training loss: 0.03518930
INFO:root:[35,   300] training loss: 0.04495210
INFO:root:[35,   350] training loss: 0.04622250
INFO:root:[35,   400] training loss: 0.03862186
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.01508721
INFO:root:[36,   100] training loss: 0.02309506
INFO:root:[36,   150] training loss: 0.04724951
INFO:root:[36,   200] training loss: 0.04937036
INFO:root:[36,   250] training loss: 0.03519864
INFO:root:[36,   300] training loss: 0.04504513
INFO:root:[36,   350] training loss: 0.04607219
INFO:root:[36,   400] training loss: 0.03858351
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.01506762
INFO:root:[37,   100] training loss: 0.02305975
INFO:root:[37,   150] training loss: 0.04691168
INFO:root:[37,   200] training loss: 0.04947226
INFO:root:[37,   250] training loss: 0.03506196
INFO:root:[37,   300] training loss: 0.04499964
INFO:root:[37,   350] training loss: 0.04625809
INFO:root:[37,   400] training loss: 0.03871424
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.01505575
INFO:root:[38,   100] training loss: 0.02312188
INFO:root:[38,   150] training loss: 0.04713407
INFO:root:[38,   200] training loss: 0.04939891
INFO:root:[38,   250] training loss: 0.03502877
INFO:root:[38,   300] training loss: 0.04484428
INFO:root:[38,   350] training loss: 0.04609812
INFO:root:[38,   400] training loss: 0.03872845
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.01498082
INFO:root:[39,   100] training loss: 0.02309976
INFO:root:[39,   150] training loss: 0.04722279
INFO:root:[39,   200] training loss: 0.04917066
INFO:root:[39,   250] training loss: 0.03532644
INFO:root:[39,   300] training loss: 0.04498007
INFO:root:[39,   350] training loss: 0.04630251
INFO:root:[39,   400] training loss: 0.03861204
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.01506107
INFO:root:[40,   100] training loss: 0.02305440
INFO:root:[40,   150] training loss: 0.04741180
INFO:root:[40,   200] training loss: 0.04935526
INFO:root:[40,   250] training loss: 0.03537399
INFO:root:[40,   300] training loss: 0.04478734
INFO:root:[40,   350] training loss: 0.04618867
INFO:root:[40,   400] training loss: 0.03847722
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.01507425
INFO:root:[41,   100] training loss: 0.02320593
INFO:root:[41,   150] training loss: 0.04710615
INFO:root:[41,   200] training loss: 0.04936061
INFO:root:[41,   250] training loss: 0.03519673
INFO:root:[41,   300] training loss: 0.04486524
INFO:root:[41,   350] training loss: 0.04621633
INFO:root:[41,   400] training loss: 0.03835576
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.01501318
INFO:root:[42,   100] training loss: 0.02308910
INFO:root:[42,   150] training loss: 0.04757867
INFO:root:[42,   200] training loss: 0.04921885
INFO:root:[42,   250] training loss: 0.03510779
INFO:root:[42,   300] training loss: 0.04493179
INFO:root:[42,   350] training loss: 0.04624190
INFO:root:[42,   400] training loss: 0.03836251
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.01508896
INFO:root:[43,   100] training loss: 0.02313854
INFO:root:[43,   150] training loss: 0.04759151
INFO:root:[43,   200] training loss: 0.04922323
INFO:root:[43,   250] training loss: 0.03489234
INFO:root:[43,   300] training loss: 0.04499127
INFO:root:[43,   350] training loss: 0.04603341
INFO:root:[43,   400] training loss: 0.03835484
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.01505298
INFO:root:[44,   100] training loss: 0.02325323
INFO:root:[44,   150] training loss: 0.04735084
INFO:root:[44,   200] training loss: 0.04904294
INFO:root:[44,   250] training loss: 0.03525597
INFO:root:[44,   300] training loss: 0.04493642
INFO:root:[44,   350] training loss: 0.04617243
INFO:root:[44,   400] training loss: 0.03853356
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.01507459
INFO:root:[45,   100] training loss: 0.02303599
INFO:root:[45,   150] training loss: 0.04722366
INFO:root:[45,   200] training loss: 0.04911655
INFO:root:[45,   250] training loss: 0.03512562
INFO:root:[45,   300] training loss: 0.04504212
INFO:root:[45,   350] training loss: 0.04626861
INFO:root:[45,   400] training loss: 0.03872474
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.01504384
INFO:root:[46,   100] training loss: 0.02304499
INFO:root:[46,   150] training loss: 0.04709791
INFO:root:[46,   200] training loss: 0.04922949
INFO:root:[46,   250] training loss: 0.03509588
INFO:root:[46,   300] training loss: 0.04506447
INFO:root:[46,   350] training loss: 0.04623244
INFO:root:[46,   400] training loss: 0.03837574
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.01506251
INFO:root:[47,   100] training loss: 0.02310740
INFO:root:[47,   150] training loss: 0.04748781
INFO:root:[47,   200] training loss: 0.04923115
INFO:root:[47,   250] training loss: 0.03498074
INFO:root:[47,   300] training loss: 0.04487380
INFO:root:[47,   350] training loss: 0.04616644
INFO:root:[47,   400] training loss: 0.03852948
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.01503661
INFO:root:[48,   100] training loss: 0.02313497
INFO:root:[48,   150] training loss: 0.04756194
INFO:root:[48,   200] training loss: 0.04919294
INFO:root:[48,   250] training loss: 0.03523427
INFO:root:[48,   300] training loss: 0.04488951
INFO:root:[48,   350] training loss: 0.04614122
INFO:root:[48,   400] training loss: 0.03872428
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.01501015
INFO:root:[49,   100] training loss: 0.02325847
INFO:root:[49,   150] training loss: 0.04710088
INFO:root:[49,   200] training loss: 0.04917879
INFO:root:[49,   250] training loss: 0.03491447
INFO:root:[49,   300] training loss: 0.04498292
INFO:root:[49,   350] training loss: 0.04614057
INFO:root:[49,   400] training loss: 0.03835033
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.01497895
INFO:root:[50,   100] training loss: 0.02306181
INFO:root:[50,   150] training loss: 0.04730395
INFO:root:[50,   200] training loss: 0.04946381
INFO:root:[50,   250] training loss: 0.03535876
INFO:root:[50,   300] training loss: 0.04514192
INFO:root:[50,   350] training loss: 0.04602289
INFO:root:[50,   400] training loss: 0.03857153
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 78 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.1692    0.2463    0.2006       272
           CD4+ T     0.5943    0.8165    0.6879       899
           CD8+ T     0.3367    0.0940    0.1470       351
 CD15+ neutrophil     0.9718    0.9789    0.9753      3657
   CD14+ monocyte     0.4675    0.8504    0.6034       254
          CD19+ B     0.0000    0.0000    0.0000       161
         CD56+ NK     0.0000    0.0000    0.0000       140
              NKT     0.0333    0.0049    0.0085       205
       eosinophil     0.7565    0.8233    0.7885       317

         accuracy                         0.7820      6256
        macro avg     0.3699    0.4238    0.3790      6256
     weighted avg     0.7381    0.7820    0.7507      6256

INFO:root:    unknown   CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.200599  0.68791  0.146993           0.975344         0.603352       0.0        0.0  0.008511      0.78852
INFO:root:statistics used: {'mean': tensor([0.0149, 0.1692]), 'std': tensor([0.0204, 0.0626])}
INFO:root:train dataset: 132219, test dataset: 6256
INFO:root:used only channels: [5, 8]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.04940786
INFO:root:[1,   100] training loss: 0.03217017
INFO:root:[1,   150] training loss: 0.04180264
INFO:root:[1,   200] training loss: 0.05893910
INFO:root:[1,   250] training loss: 0.06562249
INFO:root:[1,   300] training loss: 0.06010207
INFO:root:[1,   350] training loss: 0.05795573
INFO:root:[1,   400] training loss: 0.06464431
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.03492748
INFO:root:[2,   100] training loss: 0.02701510
INFO:root:[2,   150] training loss: 0.04542207
INFO:root:[2,   200] training loss: 0.05544061
INFO:root:[2,   250] training loss: 0.06005856
INFO:root:[2,   300] training loss: 0.05487582
INFO:root:[2,   350] training loss: 0.05453246
INFO:root:[2,   400] training loss: 0.05778051
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.02221917
INFO:root:[3,   100] training loss: 0.02687861
INFO:root:[3,   150] training loss: 0.04592566
INFO:root:[3,   200] training loss: 0.05431341
INFO:root:[3,   250] training loss: 0.05608296
INFO:root:[3,   300] training loss: 0.05305949
INFO:root:[3,   350] training loss: 0.05412963
INFO:root:[3,   400] training loss: 0.05515664
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.01876006
INFO:root:[4,   100] training loss: 0.02651735
INFO:root:[4,   150] training loss: 0.04550543
INFO:root:[4,   200] training loss: 0.05227900
INFO:root:[4,   250] training loss: 0.05386696
INFO:root:[4,   300] training loss: 0.05481560
INFO:root:[4,   350] training loss: 0.05450618
INFO:root:[4,   400] training loss: 0.05353803
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.01613748
INFO:root:[5,   100] training loss: 0.02562986
INFO:root:[5,   150] training loss: 0.04237094
INFO:root:[5,   200] training loss: 0.04887359
INFO:root:[5,   250] training loss: 0.04773470
INFO:root:[5,   300] training loss: 0.05010223
INFO:root:[5,   350] training loss: 0.05249419
INFO:root:[5,   400] training loss: 0.05004474
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.01515818
INFO:root:[6,   100] training loss: 0.02647903
INFO:root:[6,   150] training loss: 0.04271063
INFO:root:[6,   200] training loss: 0.04766550
INFO:root:[6,   250] training loss: 0.04315736
INFO:root:[6,   300] training loss: 0.04757832
INFO:root:[6,   350] training loss: 0.05038077
INFO:root:[6,   400] training loss: 0.04506642
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.01504080
INFO:root:[7,   100] training loss: 0.02656312
INFO:root:[7,   150] training loss: 0.04046388
INFO:root:[7,   200] training loss: 0.04619335
INFO:root:[7,   250] training loss: 0.03501354
INFO:root:[7,   300] training loss: 0.04043836
INFO:root:[7,   350] training loss: 0.04871192
INFO:root:[7,   400] training loss: 0.04010805
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.01612710
INFO:root:[8,   100] training loss: 0.02980225
INFO:root:[8,   150] training loss: 0.06107545
INFO:root:[8,   200] training loss: 0.06602033
INFO:root:[8,   250] training loss: 0.04178811
INFO:root:[8,   300] training loss: 0.04948212
INFO:root:[8,   350] training loss: 0.04576199
INFO:root:[8,   400] training loss: 0.03237603
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.01418798
INFO:root:[9,   100] training loss: 0.02636484
INFO:root:[9,   150] training loss: 0.05420373
INFO:root:[9,   200] training loss: 0.06005133
INFO:root:[9,   250] training loss: 0.03127538
INFO:root:[9,   300] training loss: 0.04574224
INFO:root:[9,   350] training loss: 0.04464335
INFO:root:[9,   400] training loss: 0.03647359
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.01378159
INFO:root:[10,   100] training loss: 0.02436774
INFO:root:[10,   150] training loss: 0.04949173
INFO:root:[10,   200] training loss: 0.05617206
INFO:root:[10,   250] training loss: 0.02858523
INFO:root:[10,   300] training loss: 0.04351122
INFO:root:[10,   350] training loss: 0.04508030
INFO:root:[10,   400] training loss: 0.03820582
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.01361938
INFO:root:[11,   100] training loss: 0.02300972
INFO:root:[11,   150] training loss: 0.04589280
INFO:root:[11,   200] training loss: 0.05450273
INFO:root:[11,   250] training loss: 0.02745997
INFO:root:[11,   300] training loss: 0.04217130
INFO:root:[11,   350] training loss: 0.04512124
INFO:root:[11,   400] training loss: 0.03932285
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.01355510
INFO:root:[12,   100] training loss: 0.02176670
INFO:root:[12,   150] training loss: 0.04344472
INFO:root:[12,   200] training loss: 0.05260166
INFO:root:[12,   250] training loss: 0.02550627
INFO:root:[12,   300] training loss: 0.04104042
INFO:root:[12,   350] training loss: 0.04536361
INFO:root:[12,   400] training loss: 0.04035161
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.01353338
INFO:root:[13,   100] training loss: 0.02073919
INFO:root:[13,   150] training loss: 0.04089181
INFO:root:[13,   200] training loss: 0.05201842
INFO:root:[13,   250] training loss: 0.02440528
INFO:root:[13,   300] training loss: 0.04015929
INFO:root:[13,   350] training loss: 0.04484852
INFO:root:[13,   400] training loss: 0.04046855
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.01349840
INFO:root:[14,   100] training loss: 0.01974648
INFO:root:[14,   150] training loss: 0.03981400
INFO:root:[14,   200] training loss: 0.05093005
INFO:root:[14,   250] training loss: 0.02371748
INFO:root:[14,   300] training loss: 0.03907528
INFO:root:[14,   350] training loss: 0.04427093
INFO:root:[14,   400] training loss: 0.04021876
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.01346221
INFO:root:[15,   100] training loss: 0.01930601
INFO:root:[15,   150] training loss: 0.04118480
INFO:root:[15,   200] training loss: 0.05217824
INFO:root:[15,   250] training loss: 0.02466280
INFO:root:[15,   300] training loss: 0.03969974
INFO:root:[15,   350] training loss: 0.04208486
INFO:root:[15,   400] training loss: 0.03568943
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.01346618
INFO:root:[16,   100] training loss: 0.01889797
INFO:root:[16,   150] training loss: 0.04069657
INFO:root:[16,   200] training loss: 0.05159317
INFO:root:[16,   250] training loss: 0.02380354
INFO:root:[16,   300] training loss: 0.03910856
INFO:root:[16,   350] training loss: 0.04198341
INFO:root:[16,   400] training loss: 0.03644567
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.01338795
INFO:root:[17,   100] training loss: 0.01884595
INFO:root:[17,   150] training loss: 0.04004489
INFO:root:[17,   200] training loss: 0.05155623
INFO:root:[17,   250] training loss: 0.02359334
INFO:root:[17,   300] training loss: 0.03874576
INFO:root:[17,   350] training loss: 0.04209549
INFO:root:[17,   400] training loss: 0.03635541
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.01337893
INFO:root:[18,   100] training loss: 0.01811726
INFO:root:[18,   150] training loss: 0.03969755
INFO:root:[18,   200] training loss: 0.05113032
INFO:root:[18,   250] training loss: 0.02344150
INFO:root:[18,   300] training loss: 0.03836076
INFO:root:[18,   350] training loss: 0.04205450
INFO:root:[18,   400] training loss: 0.03660469
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.01339938
INFO:root:[19,   100] training loss: 0.01824404
INFO:root:[19,   150] training loss: 0.03904044
INFO:root:[19,   200] training loss: 0.05055471
INFO:root:[19,   250] training loss: 0.02330586
INFO:root:[19,   300] training loss: 0.03825670
INFO:root:[19,   350] training loss: 0.04182556
INFO:root:[19,   400] training loss: 0.03727249
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.01334167
INFO:root:[20,   100] training loss: 0.01801128
INFO:root:[20,   150] training loss: 0.03939931
INFO:root:[20,   200] training loss: 0.05043590
INFO:root:[20,   250] training loss: 0.02299743
INFO:root:[20,   300] training loss: 0.03791861
INFO:root:[20,   350] training loss: 0.04179590
INFO:root:[20,   400] training loss: 0.03759014
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.01340248
INFO:root:[21,   100] training loss: 0.01756026
INFO:root:[21,   150] training loss: 0.03920238
INFO:root:[21,   200] training loss: 0.04996029
INFO:root:[21,   250] training loss: 0.02311970
INFO:root:[21,   300] training loss: 0.03790256
INFO:root:[21,   350] training loss: 0.04196464
INFO:root:[21,   400] training loss: 0.03729369
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.01336522
INFO:root:[22,   100] training loss: 0.01767447
INFO:root:[22,   150] training loss: 0.03871318
INFO:root:[22,   200] training loss: 0.04993810
INFO:root:[22,   250] training loss: 0.02286708
INFO:root:[22,   300] training loss: 0.03790761
INFO:root:[22,   350] training loss: 0.04159664
INFO:root:[22,   400] training loss: 0.03711296
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.01335063
INFO:root:[23,   100] training loss: 0.01769340
INFO:root:[23,   150] training loss: 0.03916257
INFO:root:[23,   200] training loss: 0.05008449
INFO:root:[23,   250] training loss: 0.02325110
INFO:root:[23,   300] training loss: 0.03806242
INFO:root:[23,   350] training loss: 0.04167442
INFO:root:[23,   400] training loss: 0.03837612
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.01335615
INFO:root:[24,   100] training loss: 0.01754887
INFO:root:[24,   150] training loss: 0.03891017
INFO:root:[24,   200] training loss: 0.04998857
INFO:root:[24,   250] training loss: 0.02305006
INFO:root:[24,   300] training loss: 0.03797981
INFO:root:[24,   350] training loss: 0.04150797
INFO:root:[24,   400] training loss: 0.03707529
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.01339205
INFO:root:[25,   100] training loss: 0.01746304
INFO:root:[25,   150] training loss: 0.03900135
INFO:root:[25,   200] training loss: 0.04987445
INFO:root:[25,   250] training loss: 0.02321231
INFO:root:[25,   300] training loss: 0.03802382
INFO:root:[25,   350] training loss: 0.04165197
INFO:root:[25,   400] training loss: 0.03741622
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.01333628
INFO:root:[26,   100] training loss: 0.01775599
INFO:root:[26,   150] training loss: 0.03909741
INFO:root:[26,   200] training loss: 0.04961763
INFO:root:[26,   250] training loss: 0.02314654
INFO:root:[26,   300] training loss: 0.03790027
INFO:root:[26,   350] training loss: 0.04159795
INFO:root:[26,   400] training loss: 0.03733643
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.01335446
INFO:root:[27,   100] training loss: 0.01736912
INFO:root:[27,   150] training loss: 0.03873888
INFO:root:[27,   200] training loss: 0.05020889
INFO:root:[27,   250] training loss: 0.02298778
INFO:root:[27,   300] training loss: 0.03784569
INFO:root:[27,   350] training loss: 0.04152182
INFO:root:[27,   400] training loss: 0.03780793
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.01332954
INFO:root:[28,   100] training loss: 0.01762681
INFO:root:[28,   150] training loss: 0.03910297
INFO:root:[28,   200] training loss: 0.05002519
INFO:root:[28,   250] training loss: 0.02316309
INFO:root:[28,   300] training loss: 0.03767548
INFO:root:[28,   350] training loss: 0.04154063
INFO:root:[28,   400] training loss: 0.03760503
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.01333365
INFO:root:[29,   100] training loss: 0.01782994
INFO:root:[29,   150] training loss: 0.03902818
INFO:root:[29,   200] training loss: 0.04973256
INFO:root:[29,   250] training loss: 0.02293051
INFO:root:[29,   300] training loss: 0.03762045
INFO:root:[29,   350] training loss: 0.04165082
INFO:root:[29,   400] training loss: 0.03682626
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.01336237
INFO:root:[30,   100] training loss: 0.01780173
INFO:root:[30,   150] training loss: 0.03905381
INFO:root:[30,   200] training loss: 0.04981507
INFO:root:[30,   250] training loss: 0.02291481
INFO:root:[30,   300] training loss: 0.03764361
INFO:root:[30,   350] training loss: 0.04153224
INFO:root:[30,   400] training loss: 0.03748790
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.01332725
INFO:root:[31,   100] training loss: 0.01743182
INFO:root:[31,   150] training loss: 0.03882780
INFO:root:[31,   200] training loss: 0.04975264
INFO:root:[31,   250] training loss: 0.02281658
INFO:root:[31,   300] training loss: 0.03766515
INFO:root:[31,   350] training loss: 0.04163680
INFO:root:[31,   400] training loss: 0.03658069
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.01342202
INFO:root:[32,   100] training loss: 0.01787638
INFO:root:[32,   150] training loss: 0.03899593
INFO:root:[32,   200] training loss: 0.04981188
INFO:root:[32,   250] training loss: 0.02339659
INFO:root:[32,   300] training loss: 0.03782151
INFO:root:[32,   350] training loss: 0.04177841
INFO:root:[32,   400] training loss: 0.03787091
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.01335760
INFO:root:[33,   100] training loss: 0.01812967
INFO:root:[33,   150] training loss: 0.03901089
INFO:root:[33,   200] training loss: 0.04974619
INFO:root:[33,   250] training loss: 0.02310012
INFO:root:[33,   300] training loss: 0.03777581
INFO:root:[33,   350] training loss: 0.04149850
INFO:root:[33,   400] training loss: 0.03766411
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.01337209
INFO:root:[34,   100] training loss: 0.01742790
INFO:root:[34,   150] training loss: 0.03895174
INFO:root:[34,   200] training loss: 0.04973778
INFO:root:[34,   250] training loss: 0.02312250
INFO:root:[34,   300] training loss: 0.03751175
INFO:root:[34,   350] training loss: 0.04175864
INFO:root:[34,   400] training loss: 0.03703612
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.01336865
INFO:root:[35,   100] training loss: 0.01736526
INFO:root:[35,   150] training loss: 0.03925294
INFO:root:[35,   200] training loss: 0.04936760
INFO:root:[35,   250] training loss: 0.02290052
INFO:root:[35,   300] training loss: 0.03823132
INFO:root:[35,   350] training loss: 0.04158626
INFO:root:[35,   400] training loss: 0.03769014
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.01335584
INFO:root:[36,   100] training loss: 0.01794672
INFO:root:[36,   150] training loss: 0.03904400
INFO:root:[36,   200] training loss: 0.04996997
INFO:root:[36,   250] training loss: 0.02322492
INFO:root:[36,   300] training loss: 0.03776183
INFO:root:[36,   350] training loss: 0.04155011
INFO:root:[36,   400] training loss: 0.03703703
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.01332205
INFO:root:[37,   100] training loss: 0.01757186
INFO:root:[37,   150] training loss: 0.03891532
INFO:root:[37,   200] training loss: 0.04969976
INFO:root:[37,   250] training loss: 0.02311169
INFO:root:[37,   300] training loss: 0.03789719
INFO:root:[37,   350] training loss: 0.04161382
INFO:root:[37,   400] training loss: 0.03645350
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.01334717
INFO:root:[38,   100] training loss: 0.01749702
INFO:root:[38,   150] training loss: 0.03831584
INFO:root:[38,   200] training loss: 0.04983931
INFO:root:[38,   250] training loss: 0.02294421
INFO:root:[38,   300] training loss: 0.03787570
INFO:root:[38,   350] training loss: 0.04169334
INFO:root:[38,   400] training loss: 0.03706886
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.01340225
INFO:root:[39,   100] training loss: 0.01760082
INFO:root:[39,   150] training loss: 0.03855114
INFO:root:[39,   200] training loss: 0.04974105
INFO:root:[39,   250] training loss: 0.02279388
INFO:root:[39,   300] training loss: 0.03790914
INFO:root:[39,   350] training loss: 0.04137229
INFO:root:[39,   400] training loss: 0.03717263
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.01334846
INFO:root:[40,   100] training loss: 0.01735421
INFO:root:[40,   150] training loss: 0.03892948
INFO:root:[40,   200] training loss: 0.04994102
INFO:root:[40,   250] training loss: 0.02353076
INFO:root:[40,   300] training loss: 0.03807051
INFO:root:[40,   350] training loss: 0.04186865
INFO:root:[40,   400] training loss: 0.03710619
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.01333855
INFO:root:[41,   100] training loss: 0.01790765
INFO:root:[41,   150] training loss: 0.03893774
INFO:root:[41,   200] training loss: 0.04992573
INFO:root:[41,   250] training loss: 0.02319682
INFO:root:[41,   300] training loss: 0.03813134
INFO:root:[41,   350] training loss: 0.04147594
INFO:root:[41,   400] training loss: 0.03739629
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.01333140
INFO:root:[42,   100] training loss: 0.01755231
INFO:root:[42,   150] training loss: 0.03891834
INFO:root:[42,   200] training loss: 0.04963982
INFO:root:[42,   250] training loss: 0.02278833
INFO:root:[42,   300] training loss: 0.03790676
INFO:root:[42,   350] training loss: 0.04147917
INFO:root:[42,   400] training loss: 0.03745491
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.01339471
INFO:root:[43,   100] training loss: 0.01764497
INFO:root:[43,   150] training loss: 0.03867730
INFO:root:[43,   200] training loss: 0.04990241
INFO:root:[43,   250] training loss: 0.02299060
INFO:root:[43,   300] training loss: 0.03781372
INFO:root:[43,   350] training loss: 0.04168126
INFO:root:[43,   400] training loss: 0.03753540
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.01335117
INFO:root:[44,   100] training loss: 0.01733135
INFO:root:[44,   150] training loss: 0.03910195
INFO:root:[44,   200] training loss: 0.04987009
INFO:root:[44,   250] training loss: 0.02346365
INFO:root:[44,   300] training loss: 0.03771327
INFO:root:[44,   350] training loss: 0.04159884
INFO:root:[44,   400] training loss: 0.03714829
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.01327485
INFO:root:[45,   100] training loss: 0.01744066
INFO:root:[45,   150] training loss: 0.03886365
INFO:root:[45,   200] training loss: 0.04995682
INFO:root:[45,   250] training loss: 0.02383810
INFO:root:[45,   300] training loss: 0.03782026
INFO:root:[45,   350] training loss: 0.04164641
INFO:root:[45,   400] training loss: 0.03711722
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.01343994
INFO:root:[46,   100] training loss: 0.01743387
INFO:root:[46,   150] training loss: 0.03903538
INFO:root:[46,   200] training loss: 0.04986619
INFO:root:[46,   250] training loss: 0.02263496
INFO:root:[46,   300] training loss: 0.03810529
INFO:root:[46,   350] training loss: 0.04172580
INFO:root:[46,   400] training loss: 0.03753740
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.01336868
INFO:root:[47,   100] training loss: 0.01779886
INFO:root:[47,   150] training loss: 0.03842751
INFO:root:[47,   200] training loss: 0.04996127
INFO:root:[47,   250] training loss: 0.02278839
INFO:root:[47,   300] training loss: 0.03790169
INFO:root:[47,   350] training loss: 0.04169239
INFO:root:[47,   400] training loss: 0.03645718
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.01334026
INFO:root:[48,   100] training loss: 0.01747889
INFO:root:[48,   150] training loss: 0.03893500
INFO:root:[48,   200] training loss: 0.04984050
INFO:root:[48,   250] training loss: 0.02287014
INFO:root:[48,   300] training loss: 0.03770635
INFO:root:[48,   350] training loss: 0.04182446
INFO:root:[48,   400] training loss: 0.03681183
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.01332850
INFO:root:[49,   100] training loss: 0.01805073
INFO:root:[49,   150] training loss: 0.03869669
INFO:root:[49,   200] training loss: 0.04988372
INFO:root:[49,   250] training loss: 0.02291174
INFO:root:[49,   300] training loss: 0.03788952
INFO:root:[49,   350] training loss: 0.04132504
INFO:root:[49,   400] training loss: 0.03730079
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.01329713
INFO:root:[50,   100] training loss: 0.01778114
INFO:root:[50,   150] training loss: 0.03884637
INFO:root:[50,   200] training loss: 0.04992592
INFO:root:[50,   250] training loss: 0.02319318
INFO:root:[50,   300] training loss: 0.03796188
INFO:root:[50,   350] training loss: 0.04171500
INFO:root:[50,   400] training loss: 0.03701172
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 80 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.2718    0.3068    0.2883       264
           CD4+ T     0.5937    0.8692    0.7055       933
           CD8+ T     0.2810    0.1165    0.1648       369
 CD15+ neutrophil     0.9782    0.9887    0.9834      3634
   CD14+ monocyte     0.7162    0.8797    0.7896       241
          CD19+ B     0.5833    0.0347    0.0654       202
         CD56+ NK     0.2718    0.2205    0.2435       127
              NKT     0.2432    0.0874    0.1286       206
       eosinophil     0.8541    0.8571    0.8556       280

         accuracy                         0.8045      6256
        macro avg     0.5326    0.4845    0.4694      6256
     weighted avg     0.7830    0.8045    0.7784      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.288256  0.705524  0.164751           0.983441         0.789572  0.065421   0.243478  0.128571     0.855615
INFO:root:statistics used: {'mean': tensor([0.0149, 0.1691]), 'std': tensor([0.0203, 0.0626])}
INFO:root:train dataset: 131697, test dataset: 6256
INFO:root:used only channels: [5, 8]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.03483542
INFO:root:[1,   100] training loss: 0.03379927
INFO:root:[1,   150] training loss: 0.05084294
INFO:root:[1,   200] training loss: 0.05588484
INFO:root:[1,   250] training loss: 0.05708730
INFO:root:[1,   300] training loss: 0.05439944
INFO:root:[1,   350] training loss: 0.05649886
INFO:root:[1,   400] training loss: 0.07113091
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.03505044
INFO:root:[2,   100] training loss: 0.03043953
INFO:root:[2,   150] training loss: 0.04667793
INFO:root:[2,   200] training loss: 0.05210414
INFO:root:[2,   250] training loss: 0.05339157
INFO:root:[2,   300] training loss: 0.05316328
INFO:root:[2,   350] training loss: 0.05607726
INFO:root:[2,   400] training loss: 0.06323240
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.02536414
INFO:root:[3,   100] training loss: 0.02824808
INFO:root:[3,   150] training loss: 0.04427820
INFO:root:[3,   200] training loss: 0.04926732
INFO:root:[3,   250] training loss: 0.05067733
INFO:root:[3,   300] training loss: 0.05152858
INFO:root:[3,   350] training loss: 0.05539373
INFO:root:[3,   400] training loss: 0.05577619
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.02159300
INFO:root:[4,   100] training loss: 0.02821408
INFO:root:[4,   150] training loss: 0.04568553
INFO:root:[4,   200] training loss: 0.04460271
INFO:root:[4,   250] training loss: 0.04589835
INFO:root:[4,   300] training loss: 0.04761422
INFO:root:[4,   350] training loss: 0.05305658
INFO:root:[4,   400] training loss: 0.04982866
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.01840740
INFO:root:[5,   100] training loss: 0.02767796
INFO:root:[5,   150] training loss: 0.04601567
INFO:root:[5,   200] training loss: 0.04354735
INFO:root:[5,   250] training loss: 0.03901151
INFO:root:[5,   300] training loss: 0.04230907
INFO:root:[5,   350] training loss: 0.04987774
INFO:root:[5,   400] training loss: 0.04502576
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.01732760
INFO:root:[6,   100] training loss: 0.02765144
INFO:root:[6,   150] training loss: 0.04360899
INFO:root:[6,   200] training loss: 0.04247055
INFO:root:[6,   250] training loss: 0.03295341
INFO:root:[6,   300] training loss: 0.03758423
INFO:root:[6,   350] training loss: 0.04565015
INFO:root:[6,   400] training loss: 0.04128265
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.01627611
INFO:root:[7,   100] training loss: 0.02733577
INFO:root:[7,   150] training loss: 0.04028600
INFO:root:[7,   200] training loss: 0.03985895
INFO:root:[7,   250] training loss: 0.02680290
INFO:root:[7,   300] training loss: 0.03593735
INFO:root:[7,   350] training loss: 0.04042012
INFO:root:[7,   400] training loss: 0.03907603
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.01818027
INFO:root:[8,   100] training loss: 0.03302700
INFO:root:[8,   150] training loss: 0.06375828
INFO:root:[8,   200] training loss: 0.05960480
INFO:root:[8,   250] training loss: 0.05832656
INFO:root:[8,   300] training loss: 0.05297410
INFO:root:[8,   350] training loss: 0.04926441
INFO:root:[8,   400] training loss: 0.03193218
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.01570587
INFO:root:[9,   100] training loss: 0.02820678
INFO:root:[9,   150] training loss: 0.05538904
INFO:root:[9,   200] training loss: 0.05321934
INFO:root:[9,   250] training loss: 0.04028559
INFO:root:[9,   300] training loss: 0.04556263
INFO:root:[9,   350] training loss: 0.04495579
INFO:root:[9,   400] training loss: 0.03710305
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.01499843
INFO:root:[10,   100] training loss: 0.02693121
INFO:root:[10,   150] training loss: 0.05294581
INFO:root:[10,   200] training loss: 0.05224550
INFO:root:[10,   250] training loss: 0.03373561
INFO:root:[10,   300] training loss: 0.04280584
INFO:root:[10,   350] training loss: 0.04488599
INFO:root:[10,   400] training loss: 0.03889400
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.01462726
INFO:root:[11,   100] training loss: 0.02573882
INFO:root:[11,   150] training loss: 0.05066213
INFO:root:[11,   200] training loss: 0.05175081
INFO:root:[11,   250] training loss: 0.03092461
INFO:root:[11,   300] training loss: 0.04091345
INFO:root:[11,   350] training loss: 0.04510640
INFO:root:[11,   400] training loss: 0.04025936
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.01450526
INFO:root:[12,   100] training loss: 0.02481814
INFO:root:[12,   150] training loss: 0.04861491
INFO:root:[12,   200] training loss: 0.05149123
INFO:root:[12,   250] training loss: 0.02890576
INFO:root:[12,   300] training loss: 0.03939254
INFO:root:[12,   350] training loss: 0.04536367
INFO:root:[12,   400] training loss: 0.04123214
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.01431616
INFO:root:[13,   100] training loss: 0.02408348
INFO:root:[13,   150] training loss: 0.04714972
INFO:root:[13,   200] training loss: 0.05109559
INFO:root:[13,   250] training loss: 0.02766465
INFO:root:[13,   300] training loss: 0.03799724
INFO:root:[13,   350] training loss: 0.04540411
INFO:root:[13,   400] training loss: 0.04095354
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.01435788
INFO:root:[14,   100] training loss: 0.02333003
INFO:root:[14,   150] training loss: 0.04596106
INFO:root:[14,   200] training loss: 0.05150000
INFO:root:[14,   250] training loss: 0.02676449
INFO:root:[14,   300] training loss: 0.03667022
INFO:root:[14,   350] training loss: 0.04546907
INFO:root:[14,   400] training loss: 0.04097541
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.01431875
INFO:root:[15,   100] training loss: 0.02295692
INFO:root:[15,   150] training loss: 0.04768605
INFO:root:[15,   200] training loss: 0.05563357
INFO:root:[15,   250] training loss: 0.02826319
INFO:root:[15,   300] training loss: 0.04140261
INFO:root:[15,   350] training loss: 0.04161150
INFO:root:[15,   400] training loss: 0.03447669
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.01430300
INFO:root:[16,   100] training loss: 0.02298691
INFO:root:[16,   150] training loss: 0.04758346
INFO:root:[16,   200] training loss: 0.05382261
INFO:root:[16,   250] training loss: 0.02746709
INFO:root:[16,   300] training loss: 0.03974253
INFO:root:[16,   350] training loss: 0.04171052
INFO:root:[16,   400] training loss: 0.03556600
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.01417943
INFO:root:[17,   100] training loss: 0.02294451
INFO:root:[17,   150] training loss: 0.04734793
INFO:root:[17,   200] training loss: 0.05251164
INFO:root:[17,   250] training loss: 0.02706179
INFO:root:[17,   300] training loss: 0.03824265
INFO:root:[17,   350] training loss: 0.04215158
INFO:root:[17,   400] training loss: 0.03612491
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.01414875
INFO:root:[18,   100] training loss: 0.02278097
INFO:root:[18,   150] training loss: 0.04744464
INFO:root:[18,   200] training loss: 0.05192115
INFO:root:[18,   250] training loss: 0.02653633
INFO:root:[18,   300] training loss: 0.03771082
INFO:root:[18,   350] training loss: 0.04217638
INFO:root:[18,   400] training loss: 0.03735770
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.01416054
INFO:root:[19,   100] training loss: 0.02262166
INFO:root:[19,   150] training loss: 0.04730871
INFO:root:[19,   200] training loss: 0.05136892
INFO:root:[19,   250] training loss: 0.02670036
INFO:root:[19,   300] training loss: 0.03766058
INFO:root:[19,   350] training loss: 0.04242567
INFO:root:[19,   400] training loss: 0.03793391
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.01422823
INFO:root:[20,   100] training loss: 0.02241022
INFO:root:[20,   150] training loss: 0.04696588
INFO:root:[20,   200] training loss: 0.05106190
INFO:root:[20,   250] training loss: 0.02640893
INFO:root:[20,   300] training loss: 0.03675438
INFO:root:[20,   350] training loss: 0.04237353
INFO:root:[20,   400] training loss: 0.03681229
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.01424089
INFO:root:[21,   100] training loss: 0.02258762
INFO:root:[21,   150] training loss: 0.04702187
INFO:root:[21,   200] training loss: 0.05053936
INFO:root:[21,   250] training loss: 0.02607993
INFO:root:[21,   300] training loss: 0.03646604
INFO:root:[21,   350] training loss: 0.04249983
INFO:root:[21,   400] training loss: 0.03791149
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.01424428
INFO:root:[22,   100] training loss: 0.02233407
INFO:root:[22,   150] training loss: 0.04687728
INFO:root:[22,   200] training loss: 0.05046706
INFO:root:[22,   250] training loss: 0.02637997
INFO:root:[22,   300] training loss: 0.03637845
INFO:root:[22,   350] training loss: 0.04200165
INFO:root:[22,   400] training loss: 0.03710586
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.01418036
INFO:root:[23,   100] training loss: 0.02221327
INFO:root:[23,   150] training loss: 0.04686192
INFO:root:[23,   200] training loss: 0.05077357
INFO:root:[23,   250] training loss: 0.02624907
INFO:root:[23,   300] training loss: 0.03656346
INFO:root:[23,   350] training loss: 0.04181778
INFO:root:[23,   400] training loss: 0.03737789
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.01416087
INFO:root:[24,   100] training loss: 0.02216045
INFO:root:[24,   150] training loss: 0.04701735
INFO:root:[24,   200] training loss: 0.05036338
INFO:root:[24,   250] training loss: 0.02593068
INFO:root:[24,   300] training loss: 0.03647415
INFO:root:[24,   350] training loss: 0.04170607
INFO:root:[24,   400] training loss: 0.03771136
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.01418122
INFO:root:[25,   100] training loss: 0.02241405
INFO:root:[25,   150] training loss: 0.04714460
INFO:root:[25,   200] training loss: 0.05011987
INFO:root:[25,   250] training loss: 0.02614066
INFO:root:[25,   300] training loss: 0.03651809
INFO:root:[25,   350] training loss: 0.04189034
INFO:root:[25,   400] training loss: 0.03768848
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.01412743
INFO:root:[26,   100] training loss: 0.02234107
INFO:root:[26,   150] training loss: 0.04691244
INFO:root:[26,   200] training loss: 0.05034914
INFO:root:[26,   250] training loss: 0.02623350
INFO:root:[26,   300] training loss: 0.03658976
INFO:root:[26,   350] training loss: 0.04184281
INFO:root:[26,   400] training loss: 0.03716136
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.01423489
INFO:root:[27,   100] training loss: 0.02253091
INFO:root:[27,   150] training loss: 0.04697112
INFO:root:[27,   200] training loss: 0.05030289
INFO:root:[27,   250] training loss: 0.02618208
INFO:root:[27,   300] training loss: 0.03653573
INFO:root:[27,   350] training loss: 0.04201368
INFO:root:[27,   400] training loss: 0.03703709
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.01419520
INFO:root:[28,   100] training loss: 0.02231784
INFO:root:[28,   150] training loss: 0.04706072
INFO:root:[28,   200] training loss: 0.05016519
INFO:root:[28,   250] training loss: 0.02575629
INFO:root:[28,   300] training loss: 0.03633511
INFO:root:[28,   350] training loss: 0.04178489
INFO:root:[28,   400] training loss: 0.03795624
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.01420098
INFO:root:[29,   100] training loss: 0.02218963
INFO:root:[29,   150] training loss: 0.04680901
INFO:root:[29,   200] training loss: 0.05025033
INFO:root:[29,   250] training loss: 0.02637507
INFO:root:[29,   300] training loss: 0.03644066
INFO:root:[29,   350] training loss: 0.04179606
INFO:root:[29,   400] training loss: 0.03784959
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.01420774
INFO:root:[30,   100] training loss: 0.02238868
INFO:root:[30,   150] training loss: 0.04713940
INFO:root:[30,   200] training loss: 0.05034371
INFO:root:[30,   250] training loss: 0.02602878
INFO:root:[30,   300] training loss: 0.03641973
INFO:root:[30,   350] training loss: 0.04174734
INFO:root:[30,   400] training loss: 0.03730748
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.01413486
INFO:root:[31,   100] training loss: 0.02232892
INFO:root:[31,   150] training loss: 0.04697571
INFO:root:[31,   200] training loss: 0.05026647
INFO:root:[31,   250] training loss: 0.02612550
INFO:root:[31,   300] training loss: 0.03636238
INFO:root:[31,   350] training loss: 0.04181135
INFO:root:[31,   400] training loss: 0.03739950
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.01421978
INFO:root:[32,   100] training loss: 0.02237793
INFO:root:[32,   150] training loss: 0.04726388
INFO:root:[32,   200] training loss: 0.05031512
INFO:root:[32,   250] training loss: 0.02619069
INFO:root:[32,   300] training loss: 0.03634013
INFO:root:[32,   350] training loss: 0.04154516
INFO:root:[32,   400] training loss: 0.03711296
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.01413964
INFO:root:[33,   100] training loss: 0.02243397
INFO:root:[33,   150] training loss: 0.04678789
INFO:root:[33,   200] training loss: 0.05020234
INFO:root:[33,   250] training loss: 0.02641309
INFO:root:[33,   300] training loss: 0.03620442
INFO:root:[33,   350] training loss: 0.04198902
INFO:root:[33,   400] training loss: 0.03702526
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.01423311
INFO:root:[34,   100] training loss: 0.02222268
INFO:root:[34,   150] training loss: 0.04698672
INFO:root:[34,   200] training loss: 0.05025082
INFO:root:[34,   250] training loss: 0.02571432
INFO:root:[34,   300] training loss: 0.03657444
INFO:root:[34,   350] training loss: 0.04204533
INFO:root:[34,   400] training loss: 0.03766420
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.01413856
INFO:root:[35,   100] training loss: 0.02235910
INFO:root:[35,   150] training loss: 0.04702481
INFO:root:[35,   200] training loss: 0.05024060
INFO:root:[35,   250] training loss: 0.02601138
INFO:root:[35,   300] training loss: 0.03627587
INFO:root:[35,   350] training loss: 0.04194570
INFO:root:[35,   400] training loss: 0.03748616
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.01418704
INFO:root:[36,   100] training loss: 0.02240676
INFO:root:[36,   150] training loss: 0.04680236
INFO:root:[36,   200] training loss: 0.05021337
INFO:root:[36,   250] training loss: 0.02573963
INFO:root:[36,   300] training loss: 0.03628083
INFO:root:[36,   350] training loss: 0.04178571
INFO:root:[36,   400] training loss: 0.03713566
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.01416234
INFO:root:[37,   100] training loss: 0.02241792
INFO:root:[37,   150] training loss: 0.04714650
INFO:root:[37,   200] training loss: 0.05016052
INFO:root:[37,   250] training loss: 0.02569756
INFO:root:[37,   300] training loss: 0.03664668
INFO:root:[37,   350] training loss: 0.04184467
INFO:root:[37,   400] training loss: 0.03747050
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.01422008
INFO:root:[38,   100] training loss: 0.02239631
INFO:root:[38,   150] training loss: 0.04705197
INFO:root:[38,   200] training loss: 0.05034668
INFO:root:[38,   250] training loss: 0.02625980
INFO:root:[38,   300] training loss: 0.03621782
INFO:root:[38,   350] training loss: 0.04177563
INFO:root:[38,   400] training loss: 0.03698883
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.01422493
INFO:root:[39,   100] training loss: 0.02233836
INFO:root:[39,   150] training loss: 0.04657321
INFO:root:[39,   200] training loss: 0.05007075
INFO:root:[39,   250] training loss: 0.02618544
INFO:root:[39,   300] training loss: 0.03659237
INFO:root:[39,   350] training loss: 0.04157843
INFO:root:[39,   400] training loss: 0.03719194
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.01419026
INFO:root:[40,   100] training loss: 0.02215642
INFO:root:[40,   150] training loss: 0.04657519
INFO:root:[40,   200] training loss: 0.05033416
INFO:root:[40,   250] training loss: 0.02584186
INFO:root:[40,   300] training loss: 0.03614729
INFO:root:[40,   350] training loss: 0.04168727
INFO:root:[40,   400] training loss: 0.03772421
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.01425910
INFO:root:[41,   100] training loss: 0.02237954
INFO:root:[41,   150] training loss: 0.04687292
INFO:root:[41,   200] training loss: 0.05014689
INFO:root:[41,   250] training loss: 0.02621571
INFO:root:[41,   300] training loss: 0.03646214
INFO:root:[41,   350] training loss: 0.04196970
INFO:root:[41,   400] training loss: 0.03701213
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.01411395
INFO:root:[42,   100] training loss: 0.02241714
INFO:root:[42,   150] training loss: 0.04675078
INFO:root:[42,   200] training loss: 0.05042965
INFO:root:[42,   250] training loss: 0.02608397
INFO:root:[42,   300] training loss: 0.03633848
INFO:root:[42,   350] training loss: 0.04187107
INFO:root:[42,   400] training loss: 0.03711478
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.01423790
INFO:root:[43,   100] training loss: 0.02243175
INFO:root:[43,   150] training loss: 0.04703971
INFO:root:[43,   200] training loss: 0.05013017
INFO:root:[43,   250] training loss: 0.02603795
INFO:root:[43,   300] training loss: 0.03636198
INFO:root:[43,   350] training loss: 0.04194612
INFO:root:[43,   400] training loss: 0.03755226
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.01415827
INFO:root:[44,   100] training loss: 0.02225976
INFO:root:[44,   150] training loss: 0.04675870
INFO:root:[44,   200] training loss: 0.05012354
INFO:root:[44,   250] training loss: 0.02600156
INFO:root:[44,   300] training loss: 0.03642319
INFO:root:[44,   350] training loss: 0.04195473
INFO:root:[44,   400] training loss: 0.03730180
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.01413764
INFO:root:[45,   100] training loss: 0.02230176
INFO:root:[45,   150] training loss: 0.04678150
INFO:root:[45,   200] training loss: 0.05029744
INFO:root:[45,   250] training loss: 0.02631194
INFO:root:[45,   300] training loss: 0.03640339
INFO:root:[45,   350] training loss: 0.04175858
INFO:root:[45,   400] training loss: 0.03702116
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.01411582
INFO:root:[46,   100] training loss: 0.02252348
INFO:root:[46,   150] training loss: 0.04700931
INFO:root:[46,   200] training loss: 0.05016787
INFO:root:[46,   250] training loss: 0.02593549
INFO:root:[46,   300] training loss: 0.03644630
INFO:root:[46,   350] training loss: 0.04190882
INFO:root:[46,   400] training loss: 0.03722052
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.01423031
INFO:root:[47,   100] training loss: 0.02246462
INFO:root:[47,   150] training loss: 0.04689416
INFO:root:[47,   200] training loss: 0.05022691
INFO:root:[47,   250] training loss: 0.02602223
INFO:root:[47,   300] training loss: 0.03651615
INFO:root:[47,   350] training loss: 0.04191811
INFO:root:[47,   400] training loss: 0.03755000
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.01420654
INFO:root:[48,   100] training loss: 0.02237987
INFO:root:[48,   150] training loss: 0.04679415
INFO:root:[48,   200] training loss: 0.05013346
INFO:root:[48,   250] training loss: 0.02589206
INFO:root:[48,   300] training loss: 0.03633773
INFO:root:[48,   350] training loss: 0.04176904
INFO:root:[48,   400] training loss: 0.03718707
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.01419970
INFO:root:[49,   100] training loss: 0.02254251
INFO:root:[49,   150] training loss: 0.04686699
INFO:root:[49,   200] training loss: 0.05022504
INFO:root:[49,   250] training loss: 0.02610478
INFO:root:[49,   300] training loss: 0.03632665
INFO:root:[49,   350] training loss: 0.04175597
INFO:root:[49,   400] training loss: 0.03747564
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.01419536
INFO:root:[50,   100] training loss: 0.02214880
INFO:root:[50,   150] training loss: 0.04674108
INFO:root:[50,   200] training loss: 0.05014884
INFO:root:[50,   250] training loss: 0.02589972
INFO:root:[50,   300] training loss: 0.03663038
INFO:root:[50,   350] training loss: 0.04183836
INFO:root:[50,   400] training loss: 0.03706943
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6256 test images: 80 %
INFO:root:                   precision    recall  f1-score   support

          unknown     0.2460    0.2928    0.2674       263
           CD4+ T     0.5785    0.8781    0.6975       894
           CD8+ T     0.3421    0.0393    0.0705       331
 CD15+ neutrophil     0.9826    0.9764    0.9795      3692
   CD14+ monocyte     0.7201    0.8707    0.7883       263
          CD19+ B     0.4118    0.0402    0.0733       174
         CD56+ NK     0.1538    0.0902    0.1137       133
              NKT     0.2371    0.1156    0.1554       199
       eosinophil     0.7154    0.8599    0.7811       307

         accuracy                         0.8016      6256
        macro avg     0.4875    0.4626    0.4363      6256
     weighted avg     0.7786    0.8016    0.7736      6256

INFO:root:    unknown    CD4+ T    CD8+ T   CD15+ neutrophil   CD14+ monocyte   CD19+ B   CD56+ NK       NKT   eosinophil
0  0.267361  0.697468  0.070461           0.979486         0.788296  0.073298   0.113744  0.155405     0.781065
